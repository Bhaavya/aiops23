{"id":298,"text":"Using dash(-) as part of key name in paramters (Azure pipelines). I am creating a azure pipeline job and wanted to use all-features name as key parameter. It is valid in yaml but looks like I can't use this in condition or somehow azure parse it wrong. It is possible to make it work?\nparameters:\n  all-features: false\n  name: cargo_check\n\njobs:\n- job: ${{ parameters.name }}\n  pool:\n    vmImage: ubuntu-16.04\n  variables:\n    cliflags: ''\n  steps:\n  - template: ..\/steps\/install-rust.yml\n    parameters:\n      rustup_toolchain: ${{ parameters.rust }}\n\n  - script: echo '##vso[task.setvariable variable=cliflags]$(cliflags) --all-features'\n    enabled: ${{ parameters.all-features }}  ###### at this line I am getting error #######\n    displayName: \"Activate all available features\"\n\n\nError:\n\n\/ci\/jobs\/cargo-check.yml@templates (Line: 99, Col: 14): Unexpected symbol: 'all-features'. Located at position 12 within expression: parameters.all-features. For more help, refer to https:\/\/go.microsoft.com\/fwlink\/?linkid=842996\n\/ci\/jobs\/cargo-check.yml@templates (Line: 99, Col: 14): Unexpected value '${{ parameters.all-features }}'\n\nWorkaround would be change name from paramters.all-features to parameters.all_features but I would love to use dash(-). It will be the same parameter name as in command line interface what is my point in this case. Short answer:\nYou can use parameters['all-features'] instead of parameters.all-features and it will work.\n\nLong answer:\n\nUsing dash(-) as part of key name in paramters (Azure pipelines)\n\nSorry for any inconvenience.\nThis behavior is by designed. After a period of investigation, but could not found any workaround to solve this issue.\nJust as the document Expressions stats:\n\nVariables As part of an expression, you may access variables using one of two syntaxes:\n\nIndex syntax: variables['MyVar']\nProperty dereference syntax: variables.MyVar\n\nIn order to use property dereference syntax, the property name must:\n\nStart with a-Z or _\nBe followed by a-Z 0-9 or _\n\n\nHope this help you.\n","meta":{"link":"https:\/\/stackoverflow.com\/questions\/57586664\/using-dash-as-part-of-key-name-in-paramters-azure-pipelines"},"label":[[849,882,"problem"],[1366,1431,"resolution"]],"Comments":[]}
{"id":299,"text":"How to mention specific version while installing docker on Raspberry Pi. I am trying to install dockers on Raspberry pi which is arm based device. Initially I used to run below command in order to install docker:\ncurl -sSL get.docker.com | sh \n\nThis install the latest version of dockers but I wanted to install an older version, which command should I use. This link have some useful answer but they are not working on Raspberry pi. Please help. Thanks We can use below command to mention the version while installing the docker using curl command. I have tested this on Raspberry pi and it works fine so should also work fine on other linux based os.\nexport VERSION=18.03 && curl -sSL get.docker.com | sh\n\nRefer this answer\n","meta":{"link":"https:\/\/stackoverflow.com\/questions\/53258109\/how-to-mention-specific-version-while-installing-docker-on-raspberry-pi"},"label":[[245,328,"problem"],[653,706,"resolution"]],"Comments":[]}
{"id":300,"text":"patch uninstall not working properly developed using InstallShield. I have created a patch for my product using Installshield. When I apply the patch the product files get updated successfully. However, when trying to uninstall the patch it uninstalls or removes successfully the newly added dlls to the patch from the target m\/c, but somehow some dlls or exe which got modified due to the patch are not reverting back upon uninstallation of the patch.\nWhen I see the version, date and time of some of the dlls are the date on which patch was applied and also the version remains the new one after uinstallation of the patch. that means dlls are not getting reverted back.\nCan someone please help to understand why some of my dlls are not getting reverted back upon patch uninstallation. I have followed the patch uninstall rules properly. Hi I got the solution upon patch uninstall why some components(dlls) are not getting reverted back. that is because there is custom action in my project which was causing the issue which was not allowing the Feature in which my component(dll) to re installed upon patch uninstall.\nthe log was saying as \nMSI (s) (1C:EC) [17:14:50:054]: Feature: ALLDLLS; Installed: Local;   Request: Local;   Action: Local \nWhere it should be re installed instead of Local in request and action statement. So I have modified the custom action and the result is \nMSI (s) (1C:EC) [17:14:50:054]: Feature: ALLDLLS; Installed: Local;   Request: Reinstall;   Action: Reinstall \nwhich in turns also reverted the Components  or dlls those were not getting reverted.\n","meta":{"link":"https:\/\/stackoverflow.com\/questions\/24948387\/patch-uninstall-not-working-properly-developed-using-installshield"},"label":[[715,786,"problem"],[956,1119,"problem"],[1339,1365,"resolution"]],"Comments":[]}
{"id":301,"text":"How to deploy to production in a feature-based release model?. We are a very small team responsible for development of a intranet web system for a non-software company. We are kind of a RAD team: when an issue is closed (be it a new feature, bugfix or some kind of configuration) it is immediately pushed to production and goes live.\nThe problem is: the process are completely manual and error prone. It consists in manually copying all new\/updated files one by one to the production server. We often need to run SQL queries or clear application cache depending the kind of update. \nFeatures or bugfixes are updated two or three times a day, maybe more depending on demand. Of course this is causing a lot of problems.\nIt has to have an easier, more professional way. One solution I could think of is updating the entire application, but currently it's not possible since our trunk isn't stable and has a lot of commits not ready for production (ok, I know it's our fault) :)\nSo how can we automate and improve the process?  We're open to any tool, preferably free or not too much expensive. \nMore info on our application and tools:\n- PHP MVC (Zend Framework)\n- Ubuntu\n- SVN (moving to Git)\n- Redmine (moving to private Github) For your case, tools will only go so far. I suspect you need to have some PHP unit tests, some UI smoke and acceptance tests, maybe some performance tests and then have a tool run all of these for each patch that you want to apply to production. Then you also need automated deployment scripts and maybe a staging environment to test these scripts. This is the general area of continuous delivery and there is a whole book on this topic.\nhttp:\/\/martinfowler.com\/delivery.html\n","meta":{"link":"https:\/\/stackoverflow.com\/questions\/11312830\/how-to-deploy-to-production-in-a-feature-based-release-model"},"label":[[7,60,"problem"],[350,399,"problem"],[1292,1575,"resolution"]],"Comments":[]}
{"id":302,"text":"Create Installer for Java Application Which run as Windows Service. I have requirement to create installer for java application. The application should serve as Native Windows Service.\nI have seen following projects which can be used to execute Java Application as Windows Service.\n\nJava Service Wrapper\nYAJSW ....\n\nIssue:\nAs i have to deploy the service on more than 20 systems (Can be increased with passage of time). So i think i would be good enough to create installer and distribute the installer file.\nSo how i can create installer file for java application which run as Windows Service ? This is an example, using the built-in support for Java from Advanced Installer.\nAdvanced Installer will generate at the end an MSI that will install your application, a wrapper EXE that can also run and install as a Win32 service. Along that it contains many other options that could be handy, like an automatic updater, etc...\n","meta":{"link":"https:\/\/stackoverflow.com\/questions\/39114705\/create-installer-for-java-application-which-run-as-windows-service"},"label":[[326,378,"problem"],[522,594,"problem"],[677,826,"resolution"]],"Comments":[]}
{"id":303,"text":"NPM install task failing in Azure Devops, same code worked previously. I have yaml pipeline running a build in Azure Devops. The Npm@1 task has started failing this morning. npm install works locally with npm version 6.14.5 and it's all green lights on npm Status.\npool:\n  name: 'Azure Pipelines'\n  vmImage: ubuntu-latest\n\nstages:\n  - stage: \n    variables:\n      buildConfiguration: \"Release\"\n      buildPlatform: \"AnyCPU\"\n    jobs:\n    - job: \n      steps:      \n      - task: Npm@1\n        displayName: 'npm install'\n        inputs:\n          workingDir: Azure\/MySite\/ClientApp\n\nHere's where things start to go wrong in the logs:\n\n1156 verbose pkgid node-sass@4.14.1\n1157 verbose cwd \/home\/vsts\/work\/1\/s\/Azure\/MySite\/ClientApp\n1158 verbose Linux 5.11.0-1021-azure\n1159 verbose argv \"\/usr\/local\/bin\/node\" \"\/usr\/local\/bin\/npm\" \"install\"\n1160 verbose node v16.13.0\n1161 verbose npm  v8.1.0\n1162 error code 1\n1163 error path \/home\/vsts\/work\/1\/s\/Azure\/MySite\/ClientApp\/node_modules\/node-sass\n1164 error command failed\n1165 error command sh -c node scripts\/build.js\n1166 error Building: \/usr\/local\/bin\/node \/home\/vsts\/work\/1\/s\/Azure\/MySite\/ClientApp\/node_modules\/node-gyp\/bin\/node-gyp.js rebuild --verbose --libsass_ext= --libsass_cflags= --libsass_ldflags= --libsass_library=\n1166 error make: Entering directory '\/home\/vsts\/work\/1\/s\/Azure\/MySite\/ClientApp\/node_modules\/node-sass\/build'\n1166 error   g++ '-DNODE_GYP_MODULE_NAME=libsass' '-DUSING_UV_SHARED=1' '-DUSING_V8_SHARED=1' '-DV8_DEPRECATION_WARNINGS=1' '-DV8_DEPRECATION_WARNINGS' '-DV8_IMMINENT_DEPRECATION_WARNINGS' '-D_GLIBCXX_USE_CXX11_ABI=1' '-D_LARGEFILE_SOURCE' '-D_FILE_OFFSET_BITS=64' '-D__STDC_FORMAT_MACROS' '-DOPENSSL_NO_PINSHARED' '-DOPENSSL_THREADS' '-DLIBSASS_VERSION=\"3.5.5\"' -I\/home\/vsts\/.node-gyp\/16.13.0\/include\/node -I\/home\/vsts\/.node-gyp\/16.13.0\/src -I\/home\/vsts\/.node-gyp\/16.13.0\/deps\/openssl\/config -I\/home\/vsts\/.node-gyp\/16.13.0\/deps\/openssl\/openssl\/include -I\/home\/vsts\/.node-gyp\/16.13.0\/deps\/uv\/include -I\/home\/vsts\/.node-gyp\/16.13.0\/deps\/zlib -I\/home\/vsts\/.node-gyp\/16.13.0\/deps\/v8\/include -I..\/src\/libsass\/include  -fPIC -pthread -Wall -Wextra -Wno-unused-parameter -m64 -O3 -fno-omit-frame-pointer -std=gnu++14 -std=c++0x -fexceptions -frtti -MMD -MF .\/Release\/.deps\/Release\/obj.target\/libsass\/src\/libsass\/src\/ast.o.d.raw   -c -o Release\/obj.target\/libsass\/src\/libsass\/src\/ast.o ..\/src\/libsass\/src\/ast.cpp\n1166 error   g++ '-DNODE_GYP_MODULE_NAME=libsass' '-DUSING_UV_SHARED=1' '-DUSING_V8_SHARED=1' '-DV8_DEPRECATION_WARNINGS=1' '-DV8_DEPRECATION_WARNINGS' '-DV8_IMMINENT_DEPRECATION_WARNINGS' '-D_GLIBCXX_USE_CXX11_ABI=1' '-D_LARGEFILE_SOURCE' '-D_FILE_OFFSET_BITS=64' '-D__STDC_FORMAT_MACROS' '-DOPENSSL_NO_PINSHARED' '-DOPENSSL_THREADS' '-DLIBSASS_VERSION=\"3.5.5\"' -I\/home\/vsts\/.node-gyp\/16.13.0\/include\/node -I\/home\/vsts\/.node-gyp\/16.13.0\/src -I\/home\/vsts\/.node-gyp\/16.13.0\/deps\/openssl\/config -I\/home\/vsts\/.node-gyp\/16.13.0\/deps\/openssl\/openssl\/include -I\/home\/vsts\/.node-gyp\/16.13.0\/deps\/uv\/include -I\/home\/vsts\/.node-gyp\/16.13.0\/deps\/zlib -I\/home\/vsts\/.node-gyp\/16.13.0\/deps\/v8\/include -I..\/src\/libsass\/include  -fPIC -pthread -Wall -Wextra -Wno-unused-parameter -m64 -O3 -fno-omit-frame-pointer -std=gnu++14 -std=c++0x -fexceptions -frtti -MMD -MF .\/Release\/.deps\/Release\/obj.target\/libsass\/src\/libsass\/src\/ast_fwd_decl.o.d.raw   -c -o Release\/obj.target\/libsass\/src\/libsass\/src\/ast_fwd_decl.o ..\/src\/libsass\/src\/ast_fwd_decl.cpp\n1166 error   g++ '-DNODE_GYP_MODULE_NAME=libsass' '-DUSING_UV_SHARED=1' '-DUSING_V8_SHARED=1' '-DV8_DEPRECATION_WARNINGS=1' '-DV8_DEPRECATION_WARNINGS' '-DV8_IMMINENT_DEPRECATION_WARNINGS' '-D_GLIBCXX_USE_CXX11_ABI=1' '-D_LARGEFILE_SOURCE' '-D_FILE_OFFSET_BITS=64' '-D__STDC_FORMAT_MACROS' '-DOPENSSL_NO_PINSHARED' '-DOPENSSL_THREADS' '-DLIBSASS_VERSION=\"3.5.5\"' -I\/home\/vsts\/.node-gyp\/16.13.0\/include\/node -I\/home\/vsts\/.node-gyp\/16.13.0\/src -I\/home\/vsts\/.node-gyp\/16.13.0\/deps\/openssl\/config -I\/home\/vsts\/.node-gyp\/16.13.0\/deps\/openssl\/openssl\/include -I\/home\/vsts\/.node-gyp\/16.13.0\/deps\/uv\/include -I\/home\/vsts\/.node-gyp\/16.13.0\/deps\/zlib -I\/home\/vsts\/.node-gyp\/16.13.0\/deps\/v8\/include -I..\/src\/libsass\/include  -fPIC -pthread -Wall -Wextra -Wno-unused-parameter -m64 -O3 -fno-omit-frame-pointer -std=gnu++14 -std=c++0x -fexceptions -frtti -MMD -MF .\/Release\/.deps\/Release\/obj.target\/libsass\/src\/libsass\/src\/backtrace.o.d.raw   -c -o Release\/obj.target\/libsass\/src\/libsass\/src\/backtrace.o ..\/src\/libsass\/src\/backtrace.cpp\n1166 error   g++ '-DNODE_GYP_MODULE_NAME=libsass' '-DUSING_UV_SHARED=1' '-DUSING_V8_SHARED=1' '-DV8_DEPRECATION_WARNINGS=1' '-DV8_DEPRECATION_WARNINGS' '-DV8_IMMINENT_DEPRECATION_WARNINGS' '-D_GLIBCXX_USE_CXX11_ABI=1' '-D_LARGEFILE_SOURCE' '-D_FILE_OFFSET_BITS=64' '-D__STDC_FORMAT_MACROS' '-DOPENSSL_NO_PINSHARED' '-DOPENSSL_THREADS' '-DLIBSASS_VERSION=\"3.5.5\"' -I\/home\/vsts\/.node-gyp\/16.13.0\/include\/node -I\/home\/vsts\/.node-gyp\/16.13.0\/src -I\/home\/vsts\/.node-gyp\/16.13.0\/deps\/openssl\/config -I\/home\/vsts\/.node-gyp\/16.13.0\/deps\/openssl\/openssl\/include -I\/home\/vsts\/.node-gyp\/16.13.0\/deps\/uv\/include -I\/home\/vsts\/.node-gyp\/16.13.0\/deps\/zlib -I\/home\/vsts\/.node-gyp\/16.13.0\/deps\/v8\/include -I..\/src\/libsass\/include  -fPIC -pthread -Wall -Wextra -Wno-unused-parameter -m64 -O3 -fno-omit-frame-pointer -std=gnu++14 -std=c++0x -fexceptions -frtti -MMD -MF .\/Release\/.deps\/Release\/obj.target\/libsass\/src\/libsass\/src\/base64vlq.o.d.raw   -c -o Release\/obj.target\/libsass\/src\/libsass\/src\/base64vlq.o ..\/src\/libsass\/src\/base64vlq.cpp\n\n\n\nThere hadn't been any changes to the  package.json in several months. Going by the error message, I've narrowed the problem down to something to do with note-sass so here's a minimal package.json to reproduce the error.\n{\n  \"dependencies\": {\n    \"node-sass\": \"^4.14.1\"\n  }\n} There are two mysteries in the universe: 1) its spatial extent and 2) why does NPM fail?\n10990 error path \/home\/vsts\/work\/1\/s\/Azure\/MySite\/ClientApp\/node_modules\/node-sass\n10991 error command failed\n10992 error command sh -c node scripts\/build.js\n10993 error Building: \/usr\/local\/bin\/node \/home\/vsts\/work\/1\/s\/Azure\/MySite\/ClientApp\/node_modules\/node-gyp\/bin\/node-gyp.js rebuild --verbose --libsass_ext= --libsass_cflags= --libsass_ldflags= --libsass_library=\n10993 error make: Entering directory '\/home\/vsts\/work\/1\/s\/Azure\/MySite\/ClientApp\/node_modules\/node-sass\/build'\n\nFrom what I can see in your logs, it's matter of guessing, likeliness, and personal experience. Your pipeline is compiling node-sass, which I believe is the source of most evil in the nodejs world.\nYou likely didn't upgrade your dependencies, which is a problem in node-sass. Node-sass, in fact, is binary compiled to the nodejs version.\nAzure DevOps normally upgrades the Node runtime without telling anyone (I posted a question yesterday, about SonarCloud integration).\nNow, if you (or Azure) upgrade the Nodejs runtime, then you must find a more recent version of node-sass to use that is also compatible with your own app.\nIn your question, you did state what is the npm version you are running locally, but not the nodejs version, which is 16 on Azure.\nAs a workaround, you can instruct Azure to use a different Node version\n      - task: NodeTool@0\n        displayName: Install Node.js\n        inputs:\n          versionSpec: 'Your version e.g. 14.x'\n\n","meta":{"link":"https:\/\/stackoverflow.com\/questions\/70199030\/npm-install-task-failing-in-azure-devops-same-code-worked-previously"},"label":[[0,40,"problem"],[5606,5636,"problem"],[6658,6729,"problem"],[6857,6945,"resolution"],[7103,7149,"resolution"]],"Comments":[]}
{"id":304,"text":"Delpoy NetCore application using Azure DevOps. I create release pipeline on Azure DevOps server and i have a some problem.\nHow i can change properties in .net core configuration file (appsettings.EnvName.json).\nWhen I create application on framework I had parameters.xml where I set XPath to value, default value and property name. And on pipeline I set key-value. But on net core app this method don't work =)\nI want to use about the same approach. What would I indicate the path to the value and its value. For example:\nConnectionStrings.Db1=\"Server={DB1.Server};Database={DB1.DbName};Trusted_Connection = True;\"\nConnectionStrings.Db2=\"Server={DB2.Server};Database={DB2.DbName};Trusted_Connection = True;\"\n\nNow I have added a step to execute an arbitrary powershell script on a remote server\n$jsonFile = 'appsettings.Template.json'\n$jsonFileOut = 'appsettings.Production.json'\n\n$configValues = \n'ConnectionStrings.Db1=\"Server={DB1.Server};Database={DB1.DbName};Trusted_Connection = True;\"',\n    'ConnectionStrings.Db2=\"Server={DB2.Server};Database={DB2.DbName};Trusted_Connection = True;\"'\n\n$config = Get-Content -Path $jsonFile | ConvertFrom-Json\n\nForEach ($item in $configValues)\n{\n    $kv = $item -split \"=\"\n    Invoke-Expression $('$config.' + $kv[0] + '=\"' + $kv[1] + '\"')\n}\n\n$config | ConvertTo-Json | Out-File $jsonFileOut\n\nBut I don’t really like this solution, how can I do the same in a more beautiful way dotnet core handles this in a different way. Full framework based on app.config transformation. It means that you defined one file which later was trasnformed for given build configuration (like Debug, Release, or your own). In dotnet core you define appsettings.json for each environment. This works very well because all settings are in your compiled app. And then at runtime bases on ASPNETCORE_ENVIRONMENT environment variable a proper settings is selected. Thus you may have one package for your all environments without recompilation. To benefit from that you must define file per each enviroment, but this is not transformation. This is full file.\nFor instance file for your local development may look like this:\n{\n  \"ConnectionStrings\": {\n    \"BloggingDatabase\": \"Server=(localdb)\\\\mssqllocaldb;Database=EFGetStarted.ConsoleApp.NewDb;Trusted_Connection=True;\"\n  },\n}\n\nAnd file for your dev enviroment appsettings.dev.json like this:\n{\n  \"ConnectionStrings\": {\n    \"BloggingDatabase\": \"Server=102.10.10.12\\\\mssqllocaldb;Database=EFGetStarted.ConsoleApp.NewDb;Trusted_Connection=True;\"\n  },\n}\n\nAnd then to configure loading this file you have to have configured Startup method:\npublic Startup(IHostingEnvironment env)  \n{\n    var builder = new ConfigurationBuilder()\n            .SetBasePath(env.ContentRootPath)\n            .AddJsonFile(\"appsettings.json\", optional: true, reloadOnChange: true)\n            .AddJsonFile($\"appsettings.{env.EnvironmentName}.json\", optional: true)\n            .AddEnvironmentVariables();\n        this.Configuration = builder.Build();\n}\n\nThis will load all your appsettings file and later use proper file based on enviroment variable. \nTo set this variable you may use this command in command prompt setx ASPNETCORE_ENVIRONMENT Dev or this in Powershell [Environment]::SetEnvironmentVariable(\"ASPNETCORE_ENVIRONMENT\", \"Dev\", \"Machine\")\nI hope it help you understand how settings works on dotnet core. If you need more guidance please check this links:\n\nConfiguration in ASP.NET Core\nUse multiple environments in ASP.NET Core\n\nTo sum up you don't need to change your settings in release pipeline. You need to preapre full file per enviromnet where you are going to host your app. You can be interested in replacing some values in file based on variables in your pipeline. You can consider few options here like\n\ntoken replacement\nJSON variable substitution example\n\nThis is usefult when you don't want to keep your secrets directly in source code.\nEDIT\nIf you want to replace values in you appsettings file one of the option is token replace. For this you must first instead of values keep token in your file. For instance #{SomeVariable}# will be replaced with value of SomeVariable` from your pipeline for this confirguration of token replace task.\n\n","meta":{"link":"https:\/\/stackoverflow.com\/questions\/61474129\/delpoy-netcore-application-using-azure-devops"},"label":[[211,407,"problem"],[3491,3818,"resolution"],[3907,3995,"resolution"]],"Comments":[]}
{"id":305,"text":"Can Services in GCP's Monitoring monitor endpoints?. I installed managed Anthos on a GKE cluster. Anthos Service Mesh is working and is displaying my API. Thanks to that Services that are in Monitoring automatically detect my API. This is great as it enables me to easily set SLOs and Error Budget for my API.\nHowever I would like to be able to easily set SLOs for individual endpoints in my api. Services(in Monitoring) detect only my API and not the endpoints within my API(my API is one pod\/container + sidecar). I tried to add endpoints to Services in Monitoring but it looks like it is only possible to add Kubernetes Objects there.\nIs there a way to use Services in Monitoring with endpoints? Is the only way to do so to break endpoints to separate microservices? You can monitor your endpoints using Cloud Endpoints with OpenAPI, which allows you to monitor the health of APIs you own by using the logs and metrics Cloud Endpoints maintains for you automatically. When users make requests to your API, Endpoints logs information about the requests and responses and also tracks three of the four golden signals of monitoring: latency, traffic, and errors. These usage and performance metrics help you monitor your API.\nThe following URL Configuring Cloud Endpoints  has the configuration process for Cloud Endpoints. Use this URL Monitoring your API as a reference on the monitoring process for your API, and this last URL for the Cloud Endpoint’s overview.\n","meta":{"link":"https:\/\/stackoverflow.com\/questions\/71562237\/can-services-in-gcps-monitoring-monitor-endpoints"},"label":[[397,475,"problem"],[770,835,"resolution"]],"Comments":[]}
{"id":306,"text":"Get-AzResourceGroup : 'this.Client.SubscriptionId' cannot be null. Having a bit of an issue with Azure and Powershell. I'm just checking to see if a resource group exists and I keep hitting this error. The next step after this is to create the resource group if it does not exist, but that is also throwing the same error. Was hoping someone might be able to suggest some workarounds or fixes.\nPlease note, I have access to the subscriptions, I can see them and sucessfully set the default subscription as can be seen in the script below.\nFull Error Returned\n'this.Client.SubscriptionId' cannot be null.\nAt **********************\\envir\\create-env.ps1:21 char:1\n+ Get-AzResourceGroup -Name $resourceGroup -ErrorVariable $doesNotExist ...\n+ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n    + CategoryInfo          : CloseError: (:) [Get-AzResourceGroup], ValidationException\n    + FullyQualifiedErrorId : Microsoft.Azure.Commands.ResourceManager.Cmdlets.Implementation.GetAzureResourceGroupCmdlet\n\nI've just installed the Az module, version details below.\nVersion    Name                                Repository\n-------    ----                                ----------\n1.6.0      Az                                  PSGallery\n\nI'm just running a pretty straight forward script (below), I thought it might be something to do with a default subscription not been set, but setting that has made no difference.\n$passwd = ConvertTo-SecureString $servicePrincipalKey -AsPlainText -Force\n$pscredential = New-Object \nSystem.Management.Automation.PSCredential($servicePrincipalUserName, $passwd)\nConnect-AzAccount -ServicePrincipal -Credential $pscredential -TenantId $tenantId\nSelect-AzureSubscription -Default -SubscriptionName $subscriptioName\nGet-AzResourceGroup -Name $resourceGroup -ErrorVariable $doesNotExist \n\nI then get the error listed above. I found the problem, the Service Principal I had created did not have sufficient access to the subscription as suggested by @4c74356b41, I ended up giving it a role of Contributor and that resolved the problem.\n","meta":{"link":"https:\/\/stackoverflow.com\/questions\/55538160\/get-azresourcegroup-this-client-subscriptionid-cannot-be-null"},"label":[[559,603,"problem"],[1892,1978,"problem"],[2019,2050,"resolution"]],"Comments":[]}
{"id":307,"text":"AppHarbor: NancyFx app got Internal Server Error. I've recently deployed to http:\/\/domainshop.apphb.com\/ a simple ASP.NET application using the following NuGet packages:\nNancy.Hosting.Aspnet\nDapper\nMySql.Data\n\nThe application uses MySQL. I've activated the addon and labeled the instance using the same name written in my Web.config. (I'v also deplyoed my db to that instance).\nNancyFx is configured in system.webServer\/handler and not in system.web\/httpHandlers.\nI've also set system.web\/customErrors to Off.\nWhat am I missing for get 500 - Internal Server Error?\nThe interesting thing is that static files are served correctly (http:\/\/domainshop.apphb.com\/assets\/style.css).\nRegards,\nGiacomo Are your views set with a build action of Content?\nI have had this before where my view files are set to None and so are not deployed by AppHarbor.\n","meta":{"link":"https:\/\/stackoverflow.com\/questions\/10554254\/appharbor-nancyfx-app-got-internal-server-error"},"label":[[0,48,"problem"],[694,743,"resolution"],[777,827,"problem"]],"Comments":[]}
{"id":308,"text":"Using app_offline with TFS2015 RM. I am in the course of setting up continuous deployment to IIS using TFS 2015 RM and wanted to take an application offline before deployment. Upon little research have found good posts on achieving it through “app_offline.htm” file with following steps.\n\nCreating an “app_offline.htm” file in the \"root\" of the website but with a different name (app_offline.htm_) to avoid debugging or related problems.\nBefore deployments, rename the file back to “app_offline” and place it in the destination web app.\nPost deployment, we delete the file from destination web app.\n\nCurrently, I have included the “Windows Machine File Copy” task to copy build binaries along with “app_offline.htm_” to destination web app and “Delete Files” task to delete it post deployment.\nNow, All I want to understand is how to rename the file back to app_offline.htm before deployment.\nHelp on this would be greatly appreciated! There are many ways to do it:\n\nUsing PowerShell (e.g. Rename-Item) script through PowerShell on Target Machines (rename file that on remove machine) or PowerShell (rename file on build agent machine) step\/task\nDeploy web app through WinRM-IIS Web App Deployment step\/task with Take App Offline option. (Install IIS Web App Deployment Using WinRM extension)\nRe-copy all files (include app_offline.html) to target machine through Windows Machine File Copy with Clean Target option\n\n","meta":{"link":"https:\/\/stackoverflow.com\/questions\/42836080\/using-app-offline-with-tfs2015-rm"},"label":[[827,891,"problem"],[967,1414,"resolution"]],"Comments":[]}
{"id":309,"text":"problems installing Zend Framework 2 in 1 and 1 shared hosting. I'm trying to deploy on a 1&1 shared linux hosting the Zend Skeleton Application that I obtained from github (https:\/\/github.com\/zendframework\/ZendSkeletonApplication) via a SSH connection.\nI have those files already in the server, but when doing the installation's last step that's explained in github (php composer.phar install) I get the following error: \nX-Powered-By: PHP\/4.4.9\nContent-type: text\/html\n\n<br \/>\n<b>Parse error<\/b>:  syntax error, unexpected '<' in <b>\/homepages\/45\/d*******\/htdocs\/zend_test\/ZendSkeletonApplication\/composer.phar<\/b> on line <b>75<\/b><br \/>\n\nI can see that the PHP version that shows there is 4.4.9, but I've set up the server to have php 5 (phpinfo() shows 5.4.7). I've also included an .htaccess file in the root of the installation's folder with the AddType x-mapp-php5 .php instruction, as explained here, but that doesn't seem to do anything.\nEDIT:\nFiles structure as requested below:\n[root]\n    [vendor]\n      [ZF2]\n         [bin]\n         [demos]\n         [library]\n         [resources]\n         [vendor]\n         composer.json\n         LICENSE.txt\n         README.md\n      .gitignore\n      README.md` So instead of following the directions in github I downloaded the skeleton app from there and manually added the ZF2 library in the vendors folder and all worked fine\n","meta":{"link":"https:\/\/stackoverflow.com\/questions\/12663755\/problems-installing-zend-framework-2-in-1-and-1-shared-hosting"},"label":[[479,528,"problem"],[1212,1355,"resolution"]],"Comments":[]}
{"id":310,"text":"ExpressJS problem with rendering subdomain. I am writing store and i have a problem with rendering.\nI want to have subdomain \/category and it's working just find for route like tis\nconst CategoryOne = ((req, res) =>{\n  res.render('shop\/category');\n});\nrouter.get('\/category', CategoryOne);\nThis is working perfect, but when i go on subdomain category\/shoes i want to be redirect on \/category with parametr shoes\nconst Category = ((req, res) =>{\n  const categoryPass = req.params.category;\n  res.render('shop\/category', {\n    category: categoryPass \n  });\n});\nrouter.get('\/category\/:category', Category);\n\nand it's not working, should i redirect? When i do it\nres.redirect('\/category')\n\nThen i dont have category parametr\nEDIT:\nWhat i have done so far:\nconst CategoryOne = ((req, res) =>{\nconst passedCategory = req.session.categorypassed;\nreq.session.categorypassed = undefined;\nconsole.log(passedCategory);\nProduct.find((err, response) => {\nres.render('shop\/category', {\n  title: 'Test', products: response, category: passedCategory\n});\n});\n\n});\n\nconst Category = ((req, res) =>{\n  req.session.categorypassed = req.params.category;\n  res.redirect('\/category');\n});\n\nThe problem is, when i refresh page i dont have this paramets, is there any way to save it? A better way to handle this is to use a central public directory for your assets and request them with an absolute path, such as \/assets\/images\/logo.png, instead of a relative path, such as assets\/images\/logo.png.\nYou can read more about relative and absolute paths here\n","meta":{"link":"https:\/\/stackoverflow.com\/questions\/53966010\/expressjs-problem-with-rendering-subdomain"},"label":[[1183,1228,"problem"],[1292,1378,"resolution"]],"Comments":[]}
{"id":311,"text":"deploy all Qt dependencies when building. I've created a CMakeLists.txt for creating a simple Qt application (actually it has only one file main.cpp showing an empty main window):\ncmake_minimum_required (VERSION 3.7.0)\n\nproject(guitest)\n\nset(CMAKE_ARCHIVE_OUTPUT_DIRECTORY ${CMAKE_BINARY_DIR}\/lib)\nset(CMAKE_LIBRARY_OUTPUT_DIRECTORY ${CMAKE_BINARY_DIR}\/lib)\nset(CMAKE_RUNTIME_OUTPUT_DIRECTORY ${CMAKE_BINARY_DIR}\/bin)\nset(CMAKE_INCLUDE_CURRENT_DIR ON)\nset(CMAKE_AUTOMOC ON)\n\nfind_package(Qt5Widgets REQUIRED)\ninclude_directories (${CMAKE_SOURCE_DIR}\/src ${CMAKE_CURRENT_SOURCE_DIR})\nfile (GLOB_RECURSE WSIMGUI_SRC *.cpp)\n\nadd_executable(${PROJECT_NAME} ${WSIMGUI_SRC})\ntarget_link_libraries(${PROJECT_NAME} Qt5::Widgets)\nadd_custom_command(TARGET ${PROJECT_NAME} POST_BUILD COMMAND ${CMAKE_COMMAND} -E copy_if_different $<TARGET_FILE:Qt5::Widgets> $<TARGET_FILE_DIR:${PROJECT_NAME}>)\n\nProject builds and the add_custom_command copies the Qt5Widgets.dll file to output directory. But when I try to run the program I get the following error:\n\nThis application failed to start because it could not find or load the Qt platform plugin \"windows\" in \"\".\n\nI've been looking for this problem and I've noticed that I must also copy the platform folder from my Qt installation.\nHow can I do this inside my CMakeLists.txt file? Qt provides a utility to deploy dependencies on Windows: windeployqt.\nGiven your Qt installation is located in <qt_install_prefix>, you should use the following commands:\nset PATH=%PATH%;<qt_install_prefix>\/bin\nwindeployqt --dir \/path\/to\/deployment\/dir \/path\/to\/qt\/application.exe\n\nA flexible way to use windeployqt with CMake is to first declare an imported target:\nfind_package(Qt5 ...)\n\nif(Qt5_FOUND AND WIN32 AND TARGET Qt5::qmake AND NOT TARGET Qt5::windeployqt)\n    get_target_property(_qt5_qmake_location Qt5::qmake IMPORTED_LOCATION)\n\n    execute_process(\n        COMMAND \"${_qt5_qmake_location}\" -query QT_INSTALL_PREFIX\n        RESULT_VARIABLE return_code\n        OUTPUT_VARIABLE qt5_install_prefix\n        OUTPUT_STRIP_TRAILING_WHITESPACE\n    )\n\n    set(imported_location \"${qt5_install_prefix}\/bin\/windeployqt.exe\")\n\n    if(EXISTS ${imported_location})\n        add_executable(Qt5::windeployqt IMPORTED)\n\n        set_target_properties(Qt5::windeployqt PROPERTIES\n            IMPORTED_LOCATION ${imported_location}\n        )\n    endif()\nendif()\n\nYou can then use it as follows:\nadd_executable(foo ...)\n\nif(TARGET Qt5::windeployqt)\n    # execute windeployqt in a tmp directory after build\n    add_custom_command(TARGET foo\n        POST_BUILD\n        COMMAND ${CMAKE_COMMAND} -E remove_directory \"${CMAKE_CURRENT_BINARY_DIR}\/windeployqt\"\n        COMMAND set PATH=%PATH%$<SEMICOLON>${qt5_install_prefix}\/bin\n        COMMAND Qt5::windeployqt --dir \"${CMAKE_CURRENT_BINARY_DIR}\/windeployqt\" \"$<TARGET_FILE_DIR:foo>\/$<TARGET_FILE_NAME:foo>\"\n    )\n\n    # copy deployment directory during installation\n    install(\n        DIRECTORY\n        \"${CMAKE_CURRENT_BINARY_DIR}\/windeployqt\/\"\n        DESTINATION ${FOO_INSTALL_RUNTIME_DESTINATION}\n    )\nendif()\n\nThis can (should?) be embedded in a cmake function.\n\nNote that using GLOB to collect source files is discouraged in CMake's doc:\n\nWe do not recommend using GLOB to collect a list of source files from your source tree. If no CMakeLists.txt file changes when a source is added or removed then the generated build system cannot know when to ask CMake to regenerate.\n\n\nIf you're using Visual C++, you may also want to install visual redistributable by adding the following code at the end of your CMakeLists.txt file:\nset(CMAKE_INSTALL_SYSTEM_RUNTIME_LIBS_SKIP TRUE)\n\ninclude(InstallRequiredSystemLibraries)\n\ninstall(\n    PROGRAMS ${CMAKE_INSTALL_SYSTEM_RUNTIME_LIBS}\n    DESTINATION ${FOO_INSTALL_RUNTIME_DESTINATION}\n)\n\nYou can take a look to one of my project, named Softbloks, for a complete and working integration of windeployqt and InstallRequiredSystemLibraries.\n","meta":{"link":"https:\/\/stackoverflow.com\/questions\/41193584\/deploy-all-qt-dependencies-when-building"},"label":[[1041,1146,"problem"],[1488,1597,"resolution"]],"Comments":[]}
{"id":312,"text":"How can I monitor the executed sql statements on a SQL Server 2005. In a project of mine the SQL statements that are executed against a SQL Server are failing for some unknown reason. Some of the code is already used in production so debugging it is not an easy task. Therefore I need a way to see in the database itself what SQL statements are used, as the statements are generated at runtime by the project and could be flawed when certain conditions are met. \nI therefore considered the possibility to monitor the incoming statements and check myself if I see any flaws. \nThe database is running on a SQL Server 2005, and I use SQL server management studio express as primary tool to manipulate the database. So my question is, what is the best way to do this? Seeing how you use the Management Studio Express, I will assume you don't have access to the MSSQL 2005 client tools. If you do, install those, because it includes the SQL profiler which does exactly what you want (and more!). For more info about that one, see msdn.\nI found this a while ago, because I was thinking about the exact same thing. I have access to the client tools myself, so I don't really need to yet, but that access is not unlimited (it's through my current job). If you try it out, let me know if it works ;-)\n","meta":{"link":"https:\/\/stackoverflow.com\/questions\/47376\/how-can-i-monitor-the-executed-sql-statements-on-a-sql-server-2005"},"label":[[93,182,"problem"],[932,944,"resolution"]],"Comments":[]}
{"id":313,"text":"Repository deployment and Composer : what workflow?. As a PHP developer I find myself working with Composer a lot. In the past it was on personal projects and such so I didn't have much problems with it, but now with Laravel 4 it's on project that require deploying and I'm in kind of a struggle to adapt my workflow.\nAll my projects are git repositories, thus per convention and because it's still quite buggy, like most developers I put the vendor directory in my .gitignore.\nNow the problem is : I also use Git to deploy to the server, and by all logic the vendor directory is not uploaded as it's not tracked by the repository.\nSo my question is towards people that have worked with Composer and Git for longer than me : what is the best workflow to keep the server in sync ? How  to track the vendor folder without really tracking it ?\nI tried uploading it every time I update with Composer but some of my vendor folders are quite big and I can't manually upload 30Mb of files every time something updates. \nI don't really know, how do you guys work it out ? I tried not ignoring the vendor folder but Git just messes it up, half are recognized as cloned repos and are just ignored anyway, etc.\nUPDATE : Note that I'm on a shared host so I don't have access to the server's terminal. The best way is to run composer install on the server after updating to the latest code. You should also make sure you commit your composer.lock file, which the server will then use to install (you should not run composer update on the server).\n","meta":{"link":"https:\/\/stackoverflow.com\/questions\/14498560\/repository-deployment-and-composer-what-workflow"},"label":[[499,630,"problem"],[1307,1376,"resolution"],[1394,1438,"resolution"]],"Comments":[]}
{"id":314,"text":"MySQLdb install problem. I need to install MySQLdb.\nI write:\n$ tar xfz MySQL-python-1.2.1.tar.gz\n$ cd MySQL-python-1.2.1    \n$ python setup.py build    #it is ok\n$ su root setup.py install    #return list of errors\n\nerror list:\n\nsetup.py: line 3: import: command not\nfound\nsetup.py: line 4: import: command not\nfound-\nsetup.py: line 5: from: command not\nfound-\nsetup.py: line 7: syntax error\nnearunexpected token '('-\nsetup.py: line 7: 'if not\nhasattr(sys, \"hexversion\") or\nsys.hexversion < 0x02030000:'\n\nWhat's wrong? You forgot an important word in your example: \"python\"\n$ su root python setup.py install\n\n","meta":{"link":"https:\/\/stackoverflow.com\/questions\/3108239\/mysqldb-install-problem"},"label":[[248,272,"problem"],[291,317,"problem"],[336,360,"problem"],[379,417,"problem"],[436,503,"problem"],[519,573,"problem"],[576,607,"resolution"]],"Comments":[]}
{"id":315,"text":"HspellPy installation. I am trying to install HspellPy package (using google colab) :\npip install HspellPy\n(https:\/\/pypi.org\/project\/HspellPy\/)\nI get :\nCollecting HspellPy\nUsing cached https:\/\/files.pythonhosted.org\/packages\/94\/fb\/d56f7809bae4a8376bc79974559a99c7deca3b3fe039acb4a4d67abd7f39\/HspellPy-0.1.4.tar.gz\nBuilding wheels for collected packages: HspellPy\nBuilding wheel for HspellPy (setup.py) ... error\nERROR: Failed building wheel for HspellPy\nRunning setup.py clean for HspellPy\nFailed to build HspellPy\nInstalling collected packages: HspellPy\nRunning setup.py install for HspellPy ... error\nERROR: Command errored out with exit status 1: \/usr\/bin\/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'\/tmp\/pip-install-_msu45vm\/HspellPy\/setup.py'\"'\"'; __file__='\"'\"'\/tmp\/pip-install-_msu45vm\/HspellPy\/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record \/tmp\/pip-record-lyfnm7ww\/install-record.txt --single-version-externally-managed --compile Check the logs for full command output. \n\nI looked up some fixes to \"ERROR: Failed building wheel for HspellPy\" but onthing helped, and since that package is not widely used i can't find anything helpful\nThank you For me the main part of the error is \"fatal error: hspell.h: No such file or directory\" which means that Hspell is not installed (HspellPy is just a Python wrapper for Hspell which is written in C). Install Hspell; on Debian\/Ubuntu try\nsudo apt install -y hspell\n\nOr compile from sources.\nThen retry pip install HspellPy\n","meta":{"link":"https:\/\/stackoverflow.com\/questions\/65846332\/hspellpy-installation"},"label":[[1360,1408,"problem"],[1427,1450,"problem"],[1521,1535,"resolution"]],"Comments":[]}
{"id":316,"text":"Cloud Service Project (ccproj) wont compile with unsafe code - VSTS. I have a Cloud Service project that I use to deploy my WebRole through VSTS, on a Hosted Agent.\nThe build definition is created with the Azure Cloud Services template, and has, amongst others, these steps:\n\nBuild solution **\\*.sln (step #1)\nBuild solution **\\*.ccproj (step #2)\n\nI've added \n<AllowUnsafeBlocks>true<\/AllowUnsafeBlocks>\n\nTo the .csproj file of the class library that uses unsafe code (for the release and debug configurations). I am using the release configuration when deploying.\nThe Build solution **\\*.sln step passes, but the Build solution **\\*.ccproj step fails.\nBy inspecting the logs, I can see that the Build solution **\\*.sln step is started with the \/unsafe+ parameter, however the second build step is not.\nMoreover, the MSBuild arguments for step #1 are empty, but for step #2, they are:\n\n\/t:Publish \/p:TargetProfile=$(targetProfile) \/p:DebugType=None\n  \/p:SkipInvalidConfigurations=true \/p:OutputPath=bin\\\n  \/p:PublishDir=\"$(build.artifactstagingdirectory)\\\"\n\nHow can I add this parameter to the ccproj build? The <AllowUnsafeBlocks>true<\/AllowUnsafeBlocks> was added to the Release|AnyCPU condition:\n  <PropertyGroup Condition=\" '$(Configuration)|$(Platform)' == 'Release|AnyCPU' \">\n    <DebugSymbols>true<\/DebugSymbols>\n    <DebugType>pdbonly<\/DebugType>\n    <Optimize>true<\/Optimize>\n    <OutputPath>bin\\<\/OutputPath>\n    <DefineConstants>TRACE<\/DefineConstants>\n    <ErrorReport>prompt<\/ErrorReport>\n    <WarningLevel>4<\/WarningLevel>\n    <AllowUnsafeBlocks>true<\/AllowUnsafeBlocks>\n  <\/PropertyGroup>\n\nHowever, when building the project with \"any cpu\" platform (which is default, with a space between 'any' and 'cpu') the condition does not get hit, contrary to when building a .sln project. I have to explicitly set the platform to \"anycpu\" without a space, and everything works. It wasnt opmitizing the code either, because of this.\n\n","meta":{"link":"https:\/\/stackoverflow.com\/questions\/49322183\/cloud-service-project-ccproj-wont-compile-with-unsafe-code-vsts"},"label":[[614,651,"problem"],[1614,1663,"problem"],[1721,1751,"problem"],[1804,1860,"resolution"]],"Comments":[]}
{"id":317,"text":"Error: XML transformation error in Azure Pipelines. I've spent the day trying to get to the bottom of an XML transformation error in Azure DevOps and I can't seem to get a handle on the problem. Locally the config transform works fine and the config transforms for the other environments that I'm deploying to are working in Azure DevOps.\nThe error that appears in the logs when the IIS Web App Deployment task is terminated is:\n2021-02-26T15:29:49.9263250Z ##[error]Error: XML transformation error while transforming C:\\...\\Web.config using C:\\...\\Web.Live.config.\n\nObviously I've cut out the long file paths that Azure DevOps applies.\nFurther up in the IIS Web App Deployment task logs the following error appears but is not highlighted as being significant. Aster this error is logged it does appear to apply transforms, before giving the aforementioned error which terminates the process:\n2021-02-26T15:29:49.9178971Z System.NullReferenceException: Object reference not set to an instance of an object.\n2021-02-26T15:29:49.9180071Z    at Microsoft.Web.XmlTransform.XmlTransformationLogger.ConvertUriToFileName(XmlDocument xmlDocument)\n2021-02-26T15:29:49.9180525Z    at Microsoft.Web.XmlTransform.XmlTransformationLogger.LogWarning(XmlNode referenceNode, String message, Object[] messageArgs)\n2021-02-26T15:29:49.9180911Z    at Microsoft.Web.XmlTransform.Transform.ApplyOnAllTargetNodes()\n\nI've been through the XML file and can't find any malformed XML, I've also run it through an XML validator. I've previewed the transform locally in Visual Studio 2019 Professional (possible using Slow Cheetah) and it's fine there too.\nCan anyone give a pointer as to what might cause this transformation error in the pipelines? I found my answer, it was posted online on a blog by this top lad or lady.\nhttps:\/\/tatvog.wordpress.com\/2017\/06\/06\/visual-studio-team-services-web-config-transform-error\/\nEdit:\nIn case the link goes dead, the solution was as below.\nI tracked that the culprit was the following transform:\n<system.web>\n    <compilation xdt:Transform=\"RemoveAttributes(debug)\" \/>\n<\/system.web>\n\nI got it to work by changing the transform to this:\n<system.web>\n   <compilation targetFramework=\"4.5\" xdt:Transform=\"Replace\">\n   <\/compilation>\n<\/system.web>\n\n","meta":{"link":"https:\/\/stackoverflow.com\/questions\/66389123\/error-xml-transformation-error-in-azure-pipelines"},"label":[[0,50,"problem"],[1969,2096,"problem"],[2118,2257,"resolution"]],"Comments":[]}
{"id":318,"text":"Azure DevOp Pipelines authentication to AKS with Azure AD RBAC configured?. We have configured our Azure Kubernetes Clusters to use Azure Active Directory RBAC. This means when using kubectl we need to first authenticate as an AD user (usually done through manually completing device code authentication via the web browser). We have configured this almost exactly as per the MSDN article Integrate Azure Active Directory with Azure Kubernetes Service.\nThe issue is that this authentication is now also required for Kubernetes build\/release tasks in Azure DevOp Pipelines, for example when we run kubectl apply:\n2019-01-02T08:48:21.2070286Z ##[section]Starting: kubectl apply\n2019-01-02T08:48:21.2074936Z ==============================================================================\n2019-01-02T08:48:21.2075160Z Task         : Deploy to Kubernetes\n2019-01-02T08:48:21.2075398Z Description  : Deploy, configure, update your Kubernetes cluster in Azure Container Service by running kubectl commands.\n2019-01-02T08:48:21.2075625Z Version      : 1.1.17\n2019-01-02T08:48:21.2075792Z Author       : Microsoft Corporation\n2019-01-02T08:48:21.2076009Z Help         : [More Information](https:\/\/go.microsoft.com\/fwlink\/?linkid=851275)\n2019-01-02T08:48:21.2076245Z ==============================================================================\n2019-01-02T08:48:25.7971481Z Found tool in cache: kubectl 1.7.0 x64\n2019-01-02T08:48:25.7980222Z Prepending PATH environment variable with directory: C:\\agents\\HephaestusForge\\_work\\_tool\\kubectl\\1.7.0\\x64\n2019-01-02T08:48:25.8666111Z [command]C:\\agents\\HephaestusForge\\_work\\_tool\\kubectl\\1.7.0\\x64\\kubectl.exe apply -f C:\\agents\\HephaestusForge\\_work\\r8\\a\\_MyProject\\kubernetes\\deploy.yaml -o json\n2019-01-02T08:48:26.3518703Z To sign in, use a web browser to open the page https:\/\/microsoft.com\/devicelogin and enter the code CUYYYYYVV to authenticate.\n\nWhat is a workaround for this?  Is it possible to have Azure DevOps authenticate itself as a server client instead of an AD client? You can use the admin profile which doesn't require interactive login but unfortunately bypasses any RBAC controls you may have setup.\nVote here: https:\/\/feedback.azure.com\/forums\/914020-azure-kubernetes-service-aks\/suggestions\/35146387-support-non-interactive-login-for-aad-integrated-c\n","meta":{"link":"https:\/\/stackoverflow.com\/questions\/54004007\/azure-devop-pipelines-authentication-to-aks-with-azure-ad-rbac-configured"},"label":[[1764,1793,"problem"],[2032,2054,"resolution"]],"Comments":[]}
{"id":319,"text":"Error: More than one package matched with specified pattern: D:\\a\\r1\\a\\**\\*.zip. Please restrain the search pattern. I have created a build pipeline which works correctly and publishes artifacts at the end.\nI also created a release pipeline to deploy the REST API to Azure web app\nThe release pipeline task has this information:\n$(System.DefaultWorkingDirectory)\/**\/*.zip\n\nHowever I get this error:\nError: More than one package matched with specified pattern: D:\\a\\r1\\a\\**\\*.zip. Please restrain the search pattern.\n\nWhen I see the drop folder, I can indeed see that there are folders by date and there are several .zip files\n\nThe webapi is in the drop root, but also in the shown folder.\nI havent been able to find how to clean the entire drop folder each time, or how to avoid this error.\nUpdate 1:\nIn the build pipeline I can see when publishin this:\n##[section]Starting: Publish Artifact: webapidrops\n==============================================================================\nTask         : Publish Build Artifacts\nDescription  : Publish build artifacts to Azure Pipelines\/TFS or a file share\nVersion      : 1.142.2\nAuthor       : Microsoft Corporation\nHelp         : [More Information](https:\/\/go.microsoft.com\/fwlink\/?LinkID=708390)\n==============================================================================\n##[section]Async Command Start: Upload Artifact\nUploading 31 files\nUploading 'webapidrops\/2019_04\/04_06_58\/LuloWebApi.zip' (16%)\nUploading 'webapidrops\/LuloWebApi.zip' (16%)\nUploading 'webapidrops\/LuloWebApi.zip' (33%)\nUploading 'webapidrops\/LuloWebApi.zip' (50%)\nUploading 'webapidrops\/LuloWebApi.zip' (66%)\nUploading 'webapidrops\/LuloWebApi.zip' (83%)\nUploading 'webapidrops\/LuloWebApi.zip' (100%)\nFile upload succeed.\nUpload 'D:\\a\\1\\a' to file container: '#\/1483345\/webapidrops'\nAssociated artifact 387 with build 125\n##[section]Async Command End: Upload Artifact\n##[section]Finishing: Publish Artifact: webapidrops The zip files are came from your build pipeline. if you need only the LuloWebApi.zip so configure your build pipeline publish artifacts task to take only this.\nIf you need also the second zip for your release so you can specify the LuloWebApi.zip in your release:\n$(System.DefaultWorkingDirectory)\/**\/LuloWebApi.zip\n\nOr, you mentioned that the zip also exist in the date folder so specify this folder:\n$(System.DefaultWorkingDirectory)\/**\/**\/*.zip\n\n","meta":{"link":"https:\/\/stackoverflow.com\/questions\/55495885\/error-more-than-one-package-matched-with-specified-pattern-d-a-r1-a-zip"},"label":[[399,515,"problem"],[1993,2100,"resolution"]],"Comments":[]}
{"id":320,"text":"Deployment to heroku fails with: couldn't find file 'turbolinks'. UPD: my bad, i have 2 local branches master and heroku. I was pushing from heroku branch (in which i was making my changes) by git push heroku master, so my local master branch was pushed instead of local heroku to remote master.\nIf someone is experiencing same problem, keep your mind that heroku works only with master branch. Make your pushes right: git push <remote> <local-branch>:<remote-branch>\nNew problem: Now i have faced another problem. While assets:precompile completes (the public\/assets folder is populated) without errors, server does not serve assets, so i have no styles nor scripts. Every request to any asset returns 404. I tried to make new server with different app (very simple, with bootstrap included), same result.\nOriginal question: I've deleted every mention of turbolinks from the project (in application.js, Gemfile and also from layout), but nothing helps. Deployment to heroku keeps on failing on assets:precompile.\n   I, [2014-01-27T16:55:13.789592 #1579]  INFO -- : Writing \/tmp\/build_fedb5742-a4ef-49f7-a837-3b092b48052c\/public\/assets\/active_admin-d5eac0453c093d372f4a0c6ce08b29fb.js\n   rake aborted!\n   couldn't find file 'turbolinks'\n   (in \/tmp\/build_fedb5742-a4ef-49f7-a837-3b092b48052c\/app\/assets\/javascripts\/application.js:15)\n\napplication.js is empty.\nMy Gemfile:\nsource 'https:\/\/rubygems.org'\n\n# Bundle edge Rails instead: gem 'rails', github: 'rails\/rails'\ngem 'rails', '4.0.2'\n\n# Use postgresql as the database for Active Record\ngem 'pg'\n\ngem 'activeadmin', github: 'gregbell\/active_admin'\n\ngem 'rails-i18n', '~> 4.0.0'\n\ngem 'unicorn'\n\n# Use SCSS for stylesheets\ngem 'sass-rails', '~> 4.0.0'\n\n# Use Uglifier as compressor for JavaScript assets\ngem 'uglifier', '>= 1.3.0'\n\n# Use CoffeeScript for .js.coffee assets and views\ngem 'coffee-rails', '~> 4.0.0'\n\n# See https:\/\/github.com\/sstephenson\/execjs#readme for more supported runtimes\n# gem 'therubyracer', platforms: :ruby\n\n# Use jquery as the JavaScript library\ngem 'jquery-rails'\n\ngroup :development do\n  gem 'foreman'\nend I've solved it.\nJust set config.serve_static_assets to true in environments\/production.rb\nBtw, strange option. Isn't heroku powered by apache or nginx?\n","meta":{"link":"https:\/\/stackoverflow.com\/questions\/21389133\/deployment-to-heroku-fails-with-couldnt-find-file-turbolinks"},"label":[[0,65,"problem"],[220,295,"problem"],[419,467,"resolution"],[605,666,"problem"],[2107,2175,"resolution"]],"Comments":[]}
{"id":321,"text":"Static files not being served on AWS Lightsail. After 2 days of trying multiple tutorials & reading StackOverflow, I'm calling for help!\nThe setting:\nThe development version is running smoothly on the AWS Lightsail server. It's during the production deployment that I'm running in continuous problems with the static files. The application runs on the appointed subdomain but it's missing all the JS\/CSS\/images\/...\nI have followed the official docs but to no avail.\n1\/ https:\/\/docs.bitnami.com\/aws\/infrastructure\/django\/get-started\/deploy-django-project\/\n2\/ https:\/\/docs.bitnami.com\/aws\/infrastructure\/django\/get-started\/deploy-django-project\/\nMy Folder Tree with relevant files:\nProject\n    - conf\n       - httpd-app.conf\n       - httpd-prefix.conf\n    - Django2Tutorial\n      - settings.py\n      - wsgi.py\n    - Solar\n        - static\n    - static (after running collectstatic function in terminal,-it includes the admin, Solar statics)\n\nMy settings:\nSTATIC_ROOT = os.path.join(BASE_DIR,'static')\nSTATIC_URL = '\/static\/'\nDEBUG = False\nALLOWED_HOSTS = ['54.169.172.***']\n\nwsgi.py file\nimport os\nimport sys\nsys.path.append('\/opt\/bitnami\/apps\/django\/django_projects\/Project')\nos.environ.setdefault(\"PYTHON_EGG_CACHE\", \"\/opt\/bitnami\/apps\/django\/django_projects\/Project\/egg_cache\")\nos.environ.setdefault('DJANGO_SETTINGS_MODULE', 'DjangoTutorial2.settings')\nfrom django.core.wsgi import get_wsgi_application\napplication = get_wsgi_application()\n\nMy conf files :\n1.  httpd-app.conf file\n<IfDefine !IS_DJANGOSTACK_LOADED> \nDefine IS_DJANGOSTACK_LOADED\nWSGIDaemonProcess wsgi-djangostack   processes=2 threads=15    display-name=%{GROUP}\n<\/IfDefine> \n\n<Directory \"\/opt\/bitnami\/apps\/django\/django_projects\/Project\/DjangoTutorial2\">\n\n    Options +MultiViews\n    AllowOverride All\n    <IfVersion >= 2.3>\n        Require all granted\n    <\/IfVersion>\n    \n    WSGIProcessGroup wsgi-djangostack\n\n    WSGIApplicationGroup %{GLOBAL}\n                    \n<\/Directory>\n\nAlias \/static \"\/opt\/bitnami\/apps\/django\/django_projects\/Project\/static\"\n<Directory \"\/opt\/bitnami\/apps\/django\/django_projects\/Project\/static\">\nRequire all granted  \n<\/Directory>\n\nWSGIScriptAlias \/  '\/opt\/bitnami\/apps\/django\/django_projects\/Project\/DjangoTutorial2\/wsgi.py'\n\n2. httpd-prefix.conf file\n# Include file\nRewriteEngine On\nRewriteCond \"%{HTTP_HOST}\" ^ec2-([0-9]{1,3})-([0-9]{1,3})-([0-9]{1,3})-([0-9]{1,3})\\..*\\.amazonaws.com(:[0-9]*)?$\nRewriteRule \"^\/?(.*)\" \"%{REQUEST_SCHEME}:\/\/%1.%2.%3.%4%5\/$1\" [L,R=302,NE]\nInclude \"\/opt\/bitnami\/apps\/django\/django_projects\/Project\/conf\/httpd-app.conf\"\n\nOther adjustments made: (\/opt\/bitnami\/apache2\/conf\/bitnami)\n1\/ bitnami-apps-prefix.conf  file\nInclude \"\/opt\/bitnami\/apps\/django\/django_projects\/Project\/conf\/httpd-prefix.conf\"\n\n2\/ bitnami.conf file\nVirtualHost _default_:80>\n    WSGIScriptAlias \/ \/opt\/bitnami\/apps\/django\/django_projects\/Project\/DjangoTutorial2\/wsgi.py\n<Directory \/opt\/bitnami\/apps\/django\/django_projects\/Project>\n    AllowOverride all\nRequire all granted\nOptions FollowSymlinks\n    <\/Directory>\nDocumentRoot \/opt\/bitnami\/apps\/django\/django_projects\/Project\n<\/VirtualHost>\n\n\nthe bitnami-apps-vhosts.conf  file is empty? Can this be?\n\nChecked as well:\n\nrestarted Apache on multiple occasions\nplayed around with the static_url & static_roots\n\nCan anyone advise how to proceed? It's been extremely frustrating 2 days haha.\nNote, maybe this can help :\ndouble-checked with the findstatic function, it redirects me to the Solar\/static folder. I thought since I ran collect static, I should point to the Project level static folder in the apache conf & not the Solar level static folder. You need to add Alias \/static\/ \/opt\/bitnami\/apps\/django\/django_projects\/Project\/static\/ to your virtualhost config so the server knows to map \/static\/ requests to that folder.\n","meta":{"link":"https:\/\/stackoverflow.com\/questions\/63187103\/static-files-not-being-served-on-aws-lightsail"},"label":[[0,46,"problem"],[3612,3714,"resolution"]],"Comments":[]}
{"id":322,"text":"How do you get approvals and checks for a given service connection via the Azure DevOps Rest API?. Having looked through the Azure DevOps REST API documentation, and a few failed attempts at guessing the endpoint, there doesn't appear to be any mention of how to view or create 'Approvals and checks' associated with a given service connection:\nhttps:\/\/docs.microsoft.com\/en-us\/rest\/api\/azure\/devops\/serviceendpoint\/endpoints?view=azure-devops-rest-6.1\n\nAre there any ideas on how to do this, or where the Rest API documentation for approvals\/checks for service connections are?\nFor background information, when creating a service connection via the REST API we are aiming to assign a check to the service connection so that it uses a given YAML template, as the service connections themselves are already being created as part of an automated flow. You can use an unrecorded REST API:\nPOST https:\/\/dev.azure.com\/{organization}\/{project}\/_apis\/pipelines\/checks\/configurations?api-version=5.2-preview.1\n\nHere is an example of its request body:\n{\n    \"type\": {\n        \"name\": \"ExtendsCheck\"\n    },\n    \"settings\": {\n        \"extendsChecks\": [\n            {\n                \"repositoryType\": \"git\",\n                \"repositoryName\": \"{project}\/{repository}\",\n                \"repositoryRef\": \"refs\/heads\/master\",\n                \"templatePath\": \"templates.yml\"\n            }\n        ]\n    },\n    \"resource\": {\n        \"type\": \"endpoint\",\n        \"id\": \"{service connection id}\",\n        \"name\": \"{service connection name}\"\n    }\n}\n\nTo get the service connection id, you can use the REST API Endpoints - Get Service Endpoints or Endpoints - Get Service Endpoints By Names.\n","meta":{"link":"https:\/\/stackoverflow.com\/questions\/64370254\/how-do-you-get-approvals-and-checks-for-a-given-service-connection-via-the-azure"},"label":[[0,97,"problem"],[857,884,"resolution"]],"Comments":[]}
{"id":323,"text":"Devops npm task with custom command (build) not working. I am trying to automate the build process (Azure Devops) for my Vue.js application by making use of \"npm\" task.\nTo Install the node packages, I have used npm task with built in \"install\" command. \nFor build process, I have deployed another npm task but with custom command (build). This custom build command runs successfully with the following warning \n\n\"npm WARN build 'npm build' called with no arguments. Did you mean to\n  'npm run-script build'?\"\n\nI believe it is not doing the build at all as when I go the Copy Publish Artifact, it says  \n\nTotal files copied: 0. [warning]Directory 'D:\\a\\3\\a\\drop' is empty.\n  Nothing will be added to build artifact 'drop'.\n\nI have tried 'npm run-script build' command but get the error \n\n\"NPM failed with return code: 1\"\n\nThere are some stack overflow threads (Here) where people mentioned the build as an internal command of install. If that's really the case, why I can't see the dist folder created by the install command or I am doing something wrong with my custom command npm task?\n\nNPM Install Task\n\nNPM Install Task with custom Build Command\n\nnpm install Task log\n\nnpm build Task log\n\nCopy and Publish Artifact Task\n\nCopy and Publish Task log\n\nThe script section in package.json file\n\"scripts\": {\n    \"serve\": \"vue-cli-service serve\",\n    \"build\": \"vue-cli-service build\",\n    \"lint\": \"vue-cli-service lint\"\n  }, For \"npm build\" task, the custom command (In question above, tried \"build\" and \"npm run-script build\") should be \"run-script build\". The build has successfully created the dist folder.\n\n","meta":{"link":"https:\/\/stackoverflow.com\/questions\/55119865\/devops-npm-task-with-custom-command-build-not-working"},"label":[[412,508,"problem"],[1420,1460,"resolution"],[1523,1551,"resolution"]],"Comments":[]}
{"id":325,"text":"How to integrate AzSK in VSTS CI-Build. Am working on Secure DevOps Kit for Azure(AzSK) using VSTS CI&CD. For working AzSK in VSTS there were two tasks named \"AzSK ARM Templete Checker\" and \"Secure DevOps Kit(AzSK) CICD Extensions for Azure\" which are available from Market place. But, here am unable to access \"Secure DevOps Kit(AzSK) CICD Extensions for Azure\" task after adding both to my organisation. Is there any extension\/install additional tasks to add them for accessing it?\n \n\nPlease suggest me to \"How to add it to my CI-Build Definition\" Based on the screenshot, you have installed the extension. \nSecure DevOps Kit(AzSK) CICD Extensions for Azure is just the name of the extension. And the real task is AzSK ARM Template Checker.\nSo, you just need to add the AzSK ARM Template Checker task under test hub.\n\nUPDATE:\nWell, please note that another AzSK_SVT task (AzSK Security Verification Tests) is available in 'Release' pipeline tasks only.\nSo to use it you need to create a release pipeline,... then add the AzSK_SVT task from Test category. Please see Security Verification Tests (SVTs) for details. \n\n","meta":{"link":"https:\/\/stackoverflow.com\/questions\/52269401\/how-to-integrate-azsk-in-vsts-ci-build"},"label":[[294,368,"problem"],[610,742,"problem"],[764,818,"resolution"]],"Comments":[]}
{"id":326,"text":"How solve\"no python application found check your startup logs\" error for Django + uWSGI + nginx stack. I user Django 1.10 with uWSGI and nginx on ubuntu 16.04 and deploy my app with ansible. My project have not default structure, but quite common ( thank Two scoopce for this :).\nI use split dev and production settings and config folder instead 'name' project folder. It's looks like this:\n|-- config\n|   |-- __init__.py\n|   |-- settings\n|   |   |-- __init__.py\n|   |   |-- base.py\n|   |   `-- dev.py\n|   |-- urls.py\n|   |-- wsgi_dev.py\n|   `-- wsgi_production.py\n|-- manage.py\n`-- requirements.txt\n\nMy production.py genarate from ansible with security encrypt and locate in config\/settings.\nWith this config i get \"no python application found check your startup logs\". Uwsgi don't see my application.\n( {{  }} it's jinja2 syntax for ansible )\n\/etc\/uwsgi\/sites\/{{ project_name }}\n[uwsgi]\nchdir = {{ django_root }}\nhome = \/home\/{{ project_user }}\/venvs\/{{ project_name }}\nmodule = config.wsgi_production:application\n\nmaster = true\nprocesses = 5\n\nsocket = \/run\/uwsgi\/{{ project_name }}.sock\nchown-socket = {{ project_user }}:www-data\nchmod-socket = 660\nvacuum = true After several weeks i can find problem in my wsgi.py. It common solution use os.environ['ENV'] for DJANGO_SETTINGS_MODULE, but with deffrent users and permissions its dosen't work.\nIf you use in your wsgi.py file something like this:\nos.environ[\"DJANGO_SETTINGS_MODULE\"] = \"config.settings.\" + os.environ[\"ENV\"]\nAnd have problem with no python application found - split your wsgi file. I can catch that os.environ[\"ENV\"] return empty string. I add it for my all user, use source and etc. But uwsgi in emperior mode don't see it.\nYou sould use wsgi_dev.py and wsgi_production.py where you can write somethink like this os.environ[\"DJANGO_SETTINGS_MODULE\"] = \"config.settings.production\". It's not so elegant but solve this problems fine.\nFor use splitting wsgi you can write something like this in wsgi.py\nimport os\n\nfrom django.core.wsgi import get_wsgi_application\n\nif os.environ.get('DEV') is True:\n   os.environ.setdefault(\"DJANGO_SETTINGS_MODULE\", \"config.settings.dev\")\nelse:\n   os.environ.setdefault(\"DJANGO_SETTINGS_MODULE\", \n   \"config.settings.production\")\n\napplication = get_wsgi_application()\n\n","meta":{"link":"https:\/\/stackoverflow.com\/questions\/42185596\/how-solveno-python-application-found-check-your-startup-logs-error-for-django"},"label":[[10,101,"problem"],[1530,1550,"resolution"],[1695,1851,"resolution"]],"Comments":[]}
{"id":327,"text":"Disable VSTS build trigger when creating tag. I have a YAML build definition in VSTS working fine for some times.\nI recently started to tag my repo from my release definition using this task.\nIt is working pretty fine, except that each time this task add a new lightweight tag to my repo, it also trigger my CI on that tag. The branch associated to the build is refs\/tags\/mytag.\nI tried to prevent that behavior by adding trigger branch filter in my YAML definition:\ntrigger:\n  branches:\n    include:\n    - master\n    - release\n    exclude:\n    - refs\/tags\/*\n\nBut my build is still being triggered.\nAny idea about how to prevent triggering a build definition when creating a new tag?\n\nNote\nI also tried by creating a Tag myself from the VSTS portal (which in that case is NOT a lightweight tag). The build is still being triggered, but in a different way because it is failing with error message Expected a Commit, but objectId f768714f0bac926164dea5b77e696da7a73db426 resolved to a Tag.\nAlso the version string is not properly computed in that case and result in a simple int instead of my formatted version string. For now, there has no options to specify tags include\/exclude to trigger the CI build.\nAnd there has the user vice Trigger build when pushing tag to git which suggest the feature to specify tags in CI build, you can vote and follow up.\nThe workaround for now is override YAML Continuous Integration trigger from Triggers Tab:\nIn your YAML build definition -> Triggers Tab -> Override YAML Continuous Integration trigger from here -> Include the branches you want to trigger the branch (master and release branches for your situation) -> Save build definition.\n\nNow only commits are pushed to master or release branch will trigger the CI build, and tags creation will not trigger CI build.\n","meta":{"link":"https:\/\/stackoverflow.com\/questions\/51090583\/disable-vsts-build-trigger-when-creating-tag"},"label":[[231,322,"problem"],[1379,1441,"resolution"]],"Comments":[]}
{"id":329,"text":"Installing new version of rails. When I installed rails at an earlier period from the command line, I did it from my home directory. My question is, when I install a newer version, should I create a new project and install the new version in that directory? If I install the new version from the home directory and then create a new project directory and create a new rails project with the version specified, will it cause problems? You can install the new version right in your home directory, this is perfectly acceptable and even common.  The Rails executable (the file run by rails new myCoolThing) will default to the newest version of rails when you create a new project.  You can override this if you like by passing the version as an argument rails <version> new myOldSchoolThing when you create a new project.\n","meta":{"link":"https:\/\/stackoverflow.com\/questions\/44168544\/installing-new-version-of-rails"},"label":[[181,256,"problem"],[433,494,"resolution"]],"Comments":[]}
{"id":330,"text":"Error response from daemon: manifest for abhishek8054\/token-app:latest not found: manifest unknown: manifest unknown. I had made my own Docker image that is a simple react app and pushed it onto the docker hub.\nNow I am trying to pull my image in system then it shows me an error\n\nError response from daemon: manifest for abhishek8054\/token-app:latest not found: manifest unknown: manifest unknown\".\n\nI am doing something wrong.\nMy Dockerfile code is:\nFROM node:16-alpine\nWORKDIR \/app\/\nCOPY package*.json .\nRUN npm install\nCOPY . .\nEXPOSE 3000\nCMD [\"npm\",\"start\"]\n\nAnd I made the image from the following command:\ndocker image build -t abhishek8054\/token-app:latest .\n\nAnd pushed my image with the following command:\ndocker push abhishek8054\/token-app:latest\n\nAnd pulled it again with the following command:\ndocker pull abhishek\/8054\/token-app\n\nAnd it gives me the error above. Try using the below command to pull the docker image. The issue which you are facing is that u have pushed the image with the name abhishek8054\/token-app:latest so if you need to pull the same image you will need to pull using the same image name and tag.\ndocker pull abhishek8054\/token-app:latest\n\nIts not mandatory to have latest tag by default if you have not mentioned any tag docker pulls the latest image from the container registry\n","meta":{"link":"https:\/\/stackoverflow.com\/questions\/73441435\/error-response-from-daemon-manifest-for-abhishek8054-token-applatest-not-found"},"label":[[0,116,"problem"],[1094,1132,"resolution"]],"Comments":[]}
{"id":331,"text":"Any way to fail build in TeamCity by result of cppcheck analysis. I use cppcheck for static code analysis in my project and TeamCity for continuous integration. It would be nice if build server didn't build the project when cppcheck finds some errors or warnings. Is there any way to make build fail by result of cppcheck analysis? I'm a cppcheck dev. The cppcheck command line flag --error-exitcode might help.\n","meta":{"link":"https:\/\/stackoverflow.com\/questions\/18660673\/any-way-to-fail-build-in-teamcity-by-result-of-cppcheck-analysis"},"label":[[263,331,"problem"],[352,400,"resolution"]],"Comments":[]}
{"id":332,"text":"argocd app create in CI pipeline (GitHub Actions, Tekton, ...) throws \"PermissionDenied desc = permission denied: applications, create, default\/myapp\". From our Tekton pipeline we want to use ArgoCD CLI to do a argocd app create and argocd app sync dynamically based on the app that is build. We created a new user as described in the docs by adding a accounts.tekton: apiKey to the argocd-cm ConfigMap:\nkubectl patch configmap argocd-cm -n argocd -p '{\"data\": {\"accounts.tekton\": \"apiKey\"}}'\n\nThen we created a token for the tekton user with:\nargocd account generate-token --account tekton\n\nWith this token as the password and the username tekton we did the argocd login like\nargocd login $(kubectl get service argocd-server -n argocd --output=jsonpath='{.status.loadBalancer.ingress[0].hostname}') --username=tekton --password=\"$TOKEN\";\n\nNow from within our Tekton pipeline (but we guess that would be the same for every other CI, given the usage of a non-admin user) we get the following error if we run argocd app create:\n$ argocd app create microservice-api-spring-boot --repo https:\/\/gitlab.com\/jonashackt\/microservice-api-spring-boot-config.git --path deployment --dest-server https:\/\/kubernetes.default.svc --dest-namespace default --revision argocd --sync-policy auto\nerror rpc error: code = PermissionDenied desc = permission denied: applications, create, default\/microservice-api-spring-boot, sub: tekton, iat: 2022-02-03T16:36:48Z The problem is mentioned in Argo's useraccounts docs:\n\nWhen you create local users, each of those users will need additional\nRBAC rules set up, otherwise they will fall back to the default policy\nspecified by policy.default field of the argocd-rbac-cm ConfigMap.\n\nBut these additional RBAC rules could be setup the simplest using ArgoCD Projects. And with such a AppProject you don't even need to create a user like tekton in the ConfigMap argocd-cm. ArgoCD projects have the ability to define Project roles:\n\nProjects include a feature called roles that enable automated access to a project's applications. These can be used to give a CI pipeline a restricted set of permissions. For example, a CI system may only be able to sync a single app (but not change its source or destination).\n\nThere are 2 solutions how to configure the AppProject, role & permissions incl. role token:\n\nusing argocd CLI\nusing a manifest YAML file\n\n1.) Use argocd CLI to create AppProject, role & permissions incl. role token\nSo let's get our hands dirty and create a ArgoCD AppProject using the argocd CLI called apps2deploy:\nargocd proj create apps2deploy -d https:\/\/kubernetes.default.svc,default --src \"*\"\n\nWe create it with the --src \"*\" as a wildcard for any git repository (as described here).\nNow we create a Project role called create-sync via:\nargocd proj role create apps2deploy create-sync --description \"project role to create and sync apps from a CI\/CD pipeline\"\n\nYou can check the new role has been created with argocd proj role list apps2deploy.\nThen we need to create a token for the new Project role create-sync, which can be created via:\nargocd proj role create-token apps2deploy create-sync\n\nThis token needs to be used for the argocd login command inside our Tekton \/ CI pipeline. There's also a --token-only parameter for the command, so we can create an environment variable via\nARGOCD_AUTH_TOKEN=$(argocd proj role create-token apps2deploy create-sync --token-only)\n\nThe ARGOCD_AUTH_TOKEN will be automatically used by argo login.\nNow we need to give permissions to the role, so it will be able to create and sync our application in ArgoCD from within Tekton or any other CI pipeline. As described in the docs we therefore add policies to our roles using the argocd CLI:\nargocd proj role add-policy apps2deploy create-sync --action get --permission allow --object \"*\"\nargocd proj role add-policy apps2deploy create-sync --action create --permission allow --object \"*\"\nargocd proj role add-policy apps2deploy create-sync --action sync --permission allow --object \"*\"\nargocd proj role add-policy apps2deploy create-sync --action update --permission allow --object \"*\"\nargocd proj role add-policy apps2deploy create-sync --action delete --permission allow --object \"*\"\n\nHave a look on the role policies with argocd proj role get apps2deploy create-sync, which should look somehow like this:\n$ argocd proj role get apps2deploy create-sync\nRole Name:     create-sync\nDescription:   project role to create and sync apps from a CI\/CD pipeline\nPolicies:\np, proj:apps2deploy:create-sync, projects, get, apps2deploy, allow\np, proj:apps2deploy:create-sync, applications, get, apps2deploy\/*, allow\np, proj:apps2deploy:create-sync, applications, create, apps2deploy\/*, allow\np, proj:apps2deploy:create-sync, applications, update, apps2deploy\/*, allow\np, proj:apps2deploy:create-sync, applications, delete, apps2deploy\/*, allow\np, proj:apps2deploy:create-sync, applications, sync, apps2deploy\/*, allow\nJWT Tokens:\nID          ISSUED-AT                                EXPIRES-AT\n1644166189  2022-02-06T17:49:49+01:00 (2 hours ago)  <none>\n\nFinally we should have setup everything to do a successful argocd app create. All we need to do is to add the --project apps2deploy parameter:\nargocd app create microservice-api-spring-boot --repo https:\/\/gitlab.com\/jonashackt\/microservice-api-spring-boot-config.git --path deployment --project apps2deploy --dest-server https:\/\/kubernetes.default.svc --dest-namespace default --revision argocd --sync-policy auto\n\n2.) Use manifest YAML to create AppProject, role & permissions incl. role token\nAs all those CLI based steps in solution 1.) are quite many, we could also using a manifest YAML file. Here's an example argocd-appproject-apps2deploy.yml which configures exactly the same as in solution a):\napiVersion: argoproj.io\/v1alpha1\nkind: AppProject\nmetadata:\n  name: apps2deploy\n  namespace: argocd\nspec:\n  destinations:\n    - namespace: default\n      server: https:\/\/kubernetes.default.svc\n  sourceRepos:\n    - '*'\n  roles:\n    - description: project role to create and sync apps from a CI\/CD pipeline\n      name: create-sync\n      policies:\n      - p, proj:apps2deploy:create-sync, applications, get, apps2deploy\/*, allow\n      - p, proj:apps2deploy:create-sync, applications, create, apps2deploy\/*, allow\n      - p, proj:apps2deploy:create-sync, applications, update, apps2deploy\/*, allow\n      - p, proj:apps2deploy:create-sync, applications, delete, apps2deploy\/*, allow\n      - p, proj:apps2deploy:create-sync, applications, sync, apps2deploy\/*, allow\n\nThere are only 2 steps left to be able to do a successful argocd app create from within Tekton (or other CI pipeline). We need to apply the manifest with\nkubectl apply -f argocd-appproject-apps2deploy.yml\n\nAnd we need to need to create a role token, ideally assigning it directly to the ARGOCD_AUTH_TOKEN for the argocd login command (which also needs to be done afterwards):\nARGOCD_AUTH_TOKEN=$(argocd proj role create-token apps2deploy create-sync --token-only)\n\nThe same argocd app create command as mentioned in solution 1.) should work now:\nargocd app create microservice-api-spring-boot --repo https:\/\/gitlab.com\/jonashackt\/microservice-api-spring-boot-config.git --path deployment --project apps2deploy --dest-server https:\/\/kubernetes.default.svc --dest-namespace default --revision argocd --sync-policy auto\n\n","meta":{"link":"https:\/\/stackoverflow.com\/questions\/71052421\/argocd-app-create-in-ci-pipeline-github-actions-tekton-throws-permissio"},"label":[[0,150,"problem"],[1498,1705,"problem"],[2374,2446,"resolution"],[5489,5564,"resolution"]],"Comments":[]}
{"id":333,"text":"Modal not showing in vue.js. So I have an issue, I have made a map application with various features in vanilla js. I am now trying to translate all this code to vue.js and it's a slow process. I have a modal I want to popup when clicking the button \"Add step\", but it doesn't. I have read documentation and viewed examples but I can't figure it out. \nAs I mentioned I've tried various examples I've found here on stackoverflow and elsewhere.\nI understand the solution might be very simple indeed, but I'm stuck and this modal wont show. Any tips or tricks? Things to think about when using vue?\nHere is a fiddle of my entire project in it's wholesome, https:\/\/jsfiddle.net\/8Lmjrhgs\/ .\nMany thanks for your time people!\nThis is how my vanilla.js modal looked like\nlet btn = document.getElementById(\"myBtn\");\nlet span = document.getElementsByClassName(\"close\")[0];\n\nbtn.onclick = function() {\n    modal.style.display = \"block\";\n};\nspan.onclick = function() {\n    modal.style.display = \"none\";\n};\nwindow.onclick = function(event) {\n    if (event.target == modal) {\n        modal.style.display = \"none\";\n    }\n};``` Note: data() { return { ...\ndata()  {\nreturn {\nshowModal: false,\n    isEditing: false,\n    user: {\n        mapTitle: 'Add a title to your map',\n        mapDescription: 'Add a description to your map',\n    },\n    steps: [{title: \"\", description: \"\"}],\n    step: {title: \"\", description: \"\"}\n}\n\n},\n\nDon't put your v-show on the transition-group, put it on the immediate child, the same actually goes for click, I would also not toggle the value for \"Close modal\", since you dont for \"Open modal\" at least id make those handle that the same way.\n<transition name=\"modal\" :modalData='customData' @close='showModal = !showModal'>\n        <div class=\"modal-mask\" v-show=\"showModal\">\n\nI believe the main issue here was the way you've created the component, which in this specific case is redundant since all the code is there anyway (but your real code might be actually importing it, so I cant say for sure)\nRemoving your script tags got the modal out.\nhttps:\/\/jsfiddle.net\/1kv24wyr\/1\/\n","meta":{"link":"https:\/\/stackoverflow.com\/questions\/55966047\/modal-not-showing-in-vue-js"},"label":[[194,276,"problem"],[2015,2040,"resolution"]],"Comments":[]}
{"id":334,"text":".Net Framework 4 installation-failed. I want to install .Net framework 4 on Windows 7 32 bits but i got this error message:\n\n.NET Framework 4 has not been installed because HRESULT 0xc8000222\n\nDo you know this message ? It seems you have Windows Update issue while installing, Follow these steps to fix the problem:\n\nin Command line tool type net stop wuauserv (and click enter of course)\nOpen Run command and type %windir% (and click enter), a window will be opened\nRename the file SoftwareDistribution to SDold\nin Command line tool type net start wuauserv (and click enter of course)\n\nAfter you do that the installation will be done successfully.\n","meta":{"link":"https:\/\/stackoverflow.com\/questions\/27938412\/net-framework-4-installation-failed"},"label":[[125,191,"problem"],[238,275,"problem"],[317,585,"resolution"]],"Comments":[]}
{"id":335,"text":"Python\/Flask web development. I using flask web development to create a mini-web with python 2.7.\nimport settings\n\nfrom flask import Flask, render_template\n\napp = Flask(__name__)\napp.config.from_object(settings)\n\n@app.route(\"\/\")\ndef hello():\n    return \"Hello World!\"\n\n@app.route(\"\/login\")\ndef login_template():\n    return render_template(\"login.html\") \n\nif __name__==\"__main__\":\n    app.run()\n\nThe @app.route(\"\/\") is working perfectly fine but i got an error in @app.route(\"\/login\") because im tryng to render_template(\"login.html\").\nThe file is in a directory \/templates\/login.html in the same working directory\nI got this error:\nhttps:\/\/i.gyazo.com\/761c169e0d55de45e3dd6c7af346c48c.png\nhttps:\/\/i.gyazo.com\/571c079b44c6216612c16798d57d200a.png The usage of the url_for includes a provider parameter, but your code doesn't use that. \nPlease change accordingly \n@app.route('\/login\/')\n@app.route('\/login\/<provider>')\ndef login_template(provider=None):\n    # do something \n\n","meta":{"link":"https:\/\/stackoverflow.com\/questions\/37228080\/python-flask-web-development"},"label":[[446,483,"problem"],[746,834,"problem"],[842,950,"resolution"]],"Comments":[]}
{"id":337,"text":"InnoSetup: dll can't be deleted at uninstall. I'm using the VCL Styles provided in this link to skin my installer\/uninstaller: https:\/\/code.google.com\/p\/vcl-styles-plugins\/wiki\/VCLStylesInnoSetup\nBut when I uninstall the program, the dll that contains the functions is not deleted.\nHow I can delete it?.\nI've thinked about this alternative: Copy the dll to a temporary folder and load that temporary dll that Windows Cleaner should delete in the future, but that causes me another problem that I talk about that in this post: https:\/\/stackoverflow.com\/questions\/26863987\/innosetup-pascalscript-filecopy-doesnt-copy\nBut that's another problem, what I would like to know here is how I can delete this dll file.\nThis is the full [Code] section that I'm using, notice the DeinitializeUninstall method where I try to delete the file:\n\/\/ Import the LoadVCLStyle function from VclStylesInno.DLL\nprocedure LoadVCLStyle(VClStyleFile: String); external 'LoadVCLStyleA@files:unins000.dll stdcall setuponly';\nprocedure LoadVCLStyle_UnInstall(VClStyleFile: String); external 'LoadVCLStyleA@{app}\\unins000.dll stdcall uninstallonly';\n\n\/\/ Import the UnLoadVCLStyles function from VclStylesInno.DLL\nprocedure UnLoadVCLStyles; external 'UnLoadVCLStyles@files:unins000.dll stdcall setuponly';\nprocedure UnLoadVCLStyles_UnInstall; external 'UnLoadVCLStyles@{app}\\unins000.dll stdcall uninstallonly';\n\nfunction InitializeSetup(): Boolean;\nbegin\n    ExtractTemporaryFile('unins000.vsf');\n    LoadVCLStyle(ExpandConstant('{tmp}\\unins000.vsf'));\n    Result := True;\nend;\n\nprocedure DeinitializeSetup();\nbegin\n    UnLoadVCLStyles;\nend;\n\nfunction InitializeUninstall: Boolean;\nbegin\n    Result := True;\n    LoadVCLStyle_UnInstall(ExpandConstant('{app}\\unins000.vsf'));\nend;\n\nprocedure DeinitializeUninstall();\nbegin\n    UnLoadVCLStyles_UnInstall;\n    DeleteFile(ExpandConstant('{app}\\unins000.dll'));\nend; You need to unload the library before deleting. Use the UnloadDLL function for that (the help contains an example just for this case). Missing that leads the DeleteFile function to fail in your code. For your uninstaller write this instead:\nprocedure LoadVCLStyle_UnInstall(VClStyleFile: String); external 'LoadVCLStyleA@{app}\\unins000.dll stdcall uninstallonly';\nprocedure UnLoadVCLStyles_UnInstall; external 'UnLoadVCLStyles@{app}\\unins000.dll stdcall uninstallonly';\n\nfunction InitializeUninstall: Boolean;\nbegin\n  Result := True;\n  LoadVCLStyle_UnInstall(ExpandConstant('{app}\\unins000.vsf'));\nend;\n\nprocedure DeinitializeUninstall();\nbegin\n  UnLoadVCLStyles_UnInstall;\n  UnloadDLL(ExpandConstant('{app}\\unins000.dll'));\n  DeleteFile(ExpandConstant('{app}\\unins000.dll'));\nend;\n\n","meta":{"link":"https:\/\/stackoverflow.com\/questions\/26864098\/innosetup-dll-cant-be-deleted-at-uninstall"},"label":[[0,45,"problem"],[1893,1927,"resolution"]],"Comments":[]}
{"id":338,"text":"Configuring the SSL in Windows Azure. I'm new to Windows azure. I've browsed the web but stuck at the moment. Here is my problem.\nI've deployed a web role and uploaded a certificate. I also configured the domain name. \nIn the control panel certificate name is *.mydomain.com. My website responds to mysubdomain.mydomain.com. In the properties of my azure project, in certificates tab I added a certificate with name Mydomain and copy pased the Thumbprint of my certificate from the control panel.\nThen in the Endpoints  tab I added an endpoint \n\n   Name: Endpoint2\n   Type: Input\n   Public port: 8080\n   Certificate: Mydomain (the one I recently added)\n\nThen I published the project via Visual Studio. \nBut it doesn't open via https. What I'm missing? I've solved the Issue. I had to change the public port from 8080 to 443 and everything worked fine.\n","meta":{"link":"https:\/\/stackoverflow.com\/questions\/11374733\/configuring-the-ssl-in-windows-azure"},"label":[[707,732,"problem"],[784,823,"resolution"]],"Comments":[]}
{"id":339,"text":"Azure-DevOps Pipeline fails because of testhost.dll. I'm having some troubles with my azure devops testing-pipeline.\nI'm using xunit tests and every test-project runs correctly and I can view the test results, but the pipeline fails anyway because of the following error\n##[error]Testhost process exited with error: A fatal error was encountered. The library 'hostpolicy.dll' required to execute the application was not found in 'C:\\Program Files\\dotnet'.\n##[error]. Please check the diagnostic logs for more information.\n\nLike suggested here, I added an additional filter for testhost.dll so my yaml-snippet looks like this:\n- task: VSTest@2\n  inputs:\n    platform: '$(buildPlatform)'\n    configuration: '$(buildConfiguration)'\n    testSelector: 'testAssemblies'\n    testAssemblyVer2: |\n      **\\*test.dll\n      !**\\*TestAdapter.dll\n      !**\\obj\\**\n      !**\\testhost.dll\n      !*testhost.dll\n    searchFolder: '$(System.DefaultWorkingDirectory)'\n    diagnosticsEnabled: true\n    codeCoverageEnabled: true\n\nHowever, the error still occurs and my Build is marked as failed.\nHow can I fix this? This GitHub issue should hopefully help.\nThings to try:\n\nremove references from one test assembly to another\nupdate Microsoft.NET.Test.SDK to 16.6.1\nadd an application.runtimeconfig.json file\n\nHope this helps!\n","meta":{"link":"https:\/\/stackoverflow.com\/questions\/62090915\/azure-devops-pipeline-fails-because-of-testhost-dll"},"label":[[280,314,"problem"],[347,425,"problem"],[1152,1286,"resolution"]],"Comments":[]}
{"id":340,"text":"AzureDevops- .Net project analysis with sonarQube error. I have setup my Sonar server on my AKS (Azure Kubernetes cluster) and using Sonar docker image version (image: sonarqube:7.1).\nI have my CI\/CD pipeline in Azure Devops for .net application and I am using AzureDevops SonarScanner ( sonar) .\nWhen I am running my pipeline I am getting this error:\nDescription : Run scanner and upload the results to the SonarQube server.\nVersion : 4.11.0\nAuthor : sonarsource\nHelp : Version: 4.11.0. This task is not needed for Maven and Gradle projects since the scanner should be run as part of the build.\n\/usr\/bin\/dotnet \/home\/vsts\/work\/_tasks\/SonarQubePrepare_15b84ca1-b62f-4a2a-a403-89b77a063157\/4.11.0\/dotnet-sonar-scanner-msbuild\/SonarScanner.MSBuild.dll end\nSonarScanner for MSBuild 4.10\nUsing the .NET Core version of the Scanner for MSBuild\nPost-processing started.\n##[error]The SonarQube MSBuild integration failed: SonarQube was unable to collect the required information about your projects.\nPossible causes:\n\nThe project has not been built - the project must be built in between the begin and end steps\nAn unsupported version of MSBuild has been used to build the project. Currently MSBuild 14.0.25420.1 and higher are supported.\nThe begin, build and end steps have not all been launched from the same folder\nNone of the analyzed projects have a valid ProjectGuid and you have not used a solution (.sln)\nThe SonarQube MSBuild integration failed: SonarQube was unable to collect the required information about your projects.\n\nMy Pipeline looks like this :\n- task: SonarQubePrepare@4\n  inputs:\n    SonarQube: 'SonarQube-Dev'\n    scannerMode: 'MSBuild'\n    projectKey: 'Test'\n    projectName: 'Test'\n    extraProperties: |\n      # Additional properties that will be passed to the scanner, \n      # Put one key=value per line, example:\n      # sonar.exclusions=**\/*.bin\n      sonar.projectBaseDir=\/src\n      sonar.sources=src  \n\n  \n- task: MSBuild@1\n  inputs:\n    solution: '\/*.sln'\n    restoreNugetPackages: true\n    clean: true   \n    \n- task: SonarQubeAnalyze@4 I tried many things in between and look like when I used .NetCLIBuild insted of MSBuild step then my sonar analysis started working .\nNow my pipeline looks like this.\n- task: DotNetCoreCLI@2\n      inputs:\n        command: 'restore'\n        projects: '**\/*.csproj'\n        workingDirectory: '\/src'\n    - task: SonarQubePrepare@4\n      inputs:\n        SonarQube: 'SonarQube-Dev'\n        scannerMode: 'MSBuild'\n        projectKey: 'Wiired.Card.PauseManagement'\n        projectName: 'Wiired.Card.PauseManagement'\n           \n    - task: DotNetCoreCLI@2\n      inputs:\n        command: 'build'\n        projects: '**\/*.sln'    \n    - task: SonarQubeAnalyze@4 \n\nThe problem was my CI\/CD pipeline is created to build the docker image and and deploy on AKS cluster. When I added sonar analysis my code was actually not getting build as build part was happening in my docker build step. I tried adding MSBuild before my SonarAnalysis step but that did not work and then I tried with DotNetCoreCLI@2 build and my pipeline is working as expected.\n","meta":{"link":"https:\/\/stackoverflow.com\/questions\/63789402\/azuredevops-net-project-analysis-with-sonarqube-error"},"label":[[873,992,"problem"],[2115,2151,"resolution"],[2733,2937,"problem"]],"Comments":[]}
{"id":341,"text":"What happens to {{ STATIC_URL }} in deployment. Simple question:\nWhat happens to {{ STATIC_URL }} variable in deployment?. For example, if I'm using a filter like this:\n@register.filter\ndef new_filter(g):\n    from myapp import settings\n    STATIC_URL = settings.STATIC_URL\n    return STATIC_URL + 'dir\/' + g\n\nAm I going to have problems or Django will still point to the same STATIC_URL as it is in localhost?.\nI have read the documentation but I'm still not sure about this.\nRegards Your filter will always point to whatever is defined in settings.STATIC_URL - end of story.\nIf you change the settings for production, your filter will now point to your new STATIC_URL. If you don't, it wont. \nIf your STATIC_URL is a relative URL, your URLs will automatically resolve to the host.\n\n\/static\/ on development might be looked up as localhost\/static\/\n\/static\/ on production example.com the browser will attempt example.com\/static\/ \n\nIf it's absolute, then well production or development the browser will read the same resource until you change STATIC_URL.\n","meta":{"link":"https:\/\/stackoverflow.com\/questions\/10789339\/what-happens-to-static-url-in-deployment"},"label":[[0,47,"problem"],[484,559,"resolution"]],"Comments":[]}
{"id":342,"text":"Deployed Rails app still seeing nginx splash page. I've just set up my first Digital Ocean VPS and managed to deploy my Rails app by following this guide: https:\/\/gorails.com\/deploy\/ubuntu\/14.04\nUnfortunatley I am still seeing the nginx splash page and hoep you guys can help me fix this?\nIn the nginx error log I am seeing this:\n\n[ 2016-04-20 14:48:15.4541 31875\/7f70762f57c0 age\/Ust\/UstRouterMain.cpp:342 ]: Passenger UstRouter online, PID 31875\n  2016\/04\/21 14:58:58 [emerg] 1721#0: \"listen\" directive is not allowed here in \/etc\/nginx\/sites-enabled\/default.save:27\n\nMy nginx conf looks like:\nserver {\n        listen 80 default_server;\n        server_name dev.myapp.co.uk;\n        passenger_enabled on;\n        rails_env production;\n        root \/home\/deploy\/myapp\/current\/public;\n\n        # redirect server error pages to the static page \/50x.html\n        error_page   500 502 503 504  \/50x.html;\n        location = \/50x.html {\n            root   html;\n        }\n}\n\nI obviously have changed \"myapp\" to my correct domain and app name as appropriate.\nAny ideas? The issue seems like you are having two nginx configuration files inside \/etc\/nginx\/sites-enabled\/ and both of them have the same listen 80 default_server; line.\n","meta":{"link":"https:\/\/stackoverflow.com\/questions\/36808705\/deployed-rails-app-still-seeing-nginx-splash-page"},"label":[[0,49,"problem"],[486,524,"problem"],[1093,1225,"problem"]],"Comments":[]}
{"id":343,"text":"Cannot see bugs in DevOps KanBan when specifying subarea. I have a DevOps account with a project in it. I am using the Agile process. I have work items such as bugs, features, etc. Please notice that I can actually see bugs in my KanBan board as I did the configuration to show bugs in there.\nProblem\nProblem is that if my bugs have the root area MyArea, they are shown in the board, but if I set a subarea MyArea\/Subarea, the bug disappears from the board.\nHow do I solve this? That`s may depend on your team settings. As example, you can enable sub-areas in your root area:\n\nOr add every every area to the team areas that you want to see on your board.\n","meta":{"link":"https:\/\/stackoverflow.com\/questions\/66505377\/cannot-see-bugs-in-devops-kanban-when-specifying-subarea"},"label":[[0,57,"problem"],[540,575,"resolution"],[580,654,"resolution"]],"Comments":[]}
{"id":344,"text":"devops VSTest@2: ##[error]Could not find testhost. We are using devops to build our .net 4.7.2 application. As part of that, we are running the unit tests which are using the nunit framework and test runner.\nIt has been running fine for about 18 months, but has just stopped working in the last day :(\nIt's using the standard template for running the tests and looks like:\n      - task: VSTest@2\n        displayName: \"Running tests\"\n        inputs:\n          testSelector: 'testAssemblies'\n          testAssemblyVer2: |\n              **\\*test*.dll\n              !**\\*TestAdapter.dll\n              !**\\obj\\**\n          searchFolder: '$(System.DefaultWorkingDirectory)'\n\nHowever, now it is failing the step with the following logs:\n\nNUnit Adapter 4.2.0.0: Test execution started\nRunning all tests in D:\\a\\1\\s\\Configuration.Tests\\bin\\Release\\Microsoft.VisualStudio.QualityTools.UnitTestFramework.dll\nNUnit3TestExecutor discovered 0 of 0 NUnit test cases using Current Discovery mode, Explicit run\nRunning all tests in D:\\a\\1\\s\\Configuration.Tests\\bin\\Release\\testcentric.engine.metadata.dll\nNUnit3TestExecutor discovered 0 of 0 NUnit test cases using Current Discovery mode, Explicit run\nRunning all tests in D:\\a\\1\\s\\Api.Tests\\bin\\Release\\testcentric.engine.metadata.dll\nNUnit3TestExecutor discovered 0 of 0 NUnit test cases using Current Discovery mode, Explicit run\nRunning all tests in D:\\a\\1\\s\\CommunicationTests\\bin\\Release\\testcentric.engine.metadata.dll\nNUnit3TestExecutor discovered 0 of 0 NUnit test cases using Current Discovery mode, Explicit run\nRunning all tests in D:\\a\\1\\s\\Domain.Tests\\bin\\Release\\testcentric.engine.metadata.dll\nNUnit3TestExecutor discovered 0 of 0 NUnit test cases using Current Discovery mode, Explicit run\nRunning all tests in D:\\a\\1\\s\\packages\\NUnit3TestAdapter.4.2.1\\build\\net35\\testcentric.engine.metadata.dll\nNUnit3TestExecutor discovered 0 of 0 NUnit test cases using Current Discovery mode, Explicit run\nNUnit Adapter 4.2.0.0: Test execution complete\nNo test is available in D:\\a\\1\\s\\Configuration.Tests\\bin\\Release\\Microsoft.VisualStudio.QualityTools.UnitTestFramework.dll D:\\a\\1\\s\\Configuration.Tests\\bin\\Release\\testcentric.engine.metadata.dll D:\\a\\1\\s\\Api.Tests\\bin\\Release\\testcentric.engine.metadata.dll D:\\a\\1\\s\\CommunicationTests\\bin\\Release\\testcentric.engine.metadata.dll D:\\a\\1\\s\\Domain.Tests\\bin\\Release\\testcentric.engine.metadata.dll D:\\a\\1\\s\\packages\\NUnit3TestAdapter.4.2.1\\build\\net35\\testcentric.engine.metadata.dll. Make sure that test discoverer & executors are registered and platform & framework version settings are appropriate and try again.\n##[error]Could not find testhost\nResults File: D:\\a_temp\\TestResults\\VssAdministrator_WIN-FVJ4KUK6IFI_2022-08-18_12_38_44.trx\n##[error]Test Run Aborted.\nTotal tests: Unknown\nPassed: 110\nTotal time: 16.7203 Seconds\nVstest.console.exe exited with code 1.\n**************** Completed test execution *********************\nTest results files: D:\\a_temp\\TestResults\\VssAdministrator_WIN-FVJ4KUK6IFI_2022-08-18_12_38_44.trx\nCreated test run: 1080\nPublishing test results: 112\nPublishing test results to test run '1080'.\nTestResults To Publish 112, Test run id:1080\nTest results publishing 112, remaining: 0. Test run id: 1080\nPublished test results: 112\nPublishing Attachments: 1\nExecution Result Code 1 is non zero, checking for failed results\nCompleted TestExecution Model...\n##[warning]Vstest failed with error. Check logs for failures. There might be failed tests.\n##[error]Error: The process 'D:\\a_tasks\\VSTest_ef087383-ee5e-42c7-9a53-\nab56c98420f9\\2.205.0\\Modules\\DTAExecutionHost.exe' failed with exit code 1\n##[error]Vstest failed with error. Check logs for failures. There might be failed tests.\nFinishing: Running tests\n\nLooking through this log, it seems that the nunit tests have run successfully, but it might be trying to run mstests?  It is frustrating when devops gets an update and it breaks working pipelines. We have the similar situation.\nThe unit tests are run with xUnit.\n\n\/TestAdapterPath:\"D:\\a\\1\\s\" Starting test execution, please wait... A\ntotal of 36 test files matched the specified pattern.\n2.4828\n##[error]Could not find testhost\nData collector 'Code Coverage' message: No code coverage data\navailable. Profiler was not initialized..\n2.0273\n##[error]Could not find testhost\nData collector 'Code Coverage' message: No code coverage data\navailable. Profiler was not initialized..\n2.3746\n##[error]Could not find testhost\nData collector 'Code Coverage' message: No code coverage data\navailable. Profiler was not initialized..\n1.992\n##[error]Could not find testhost\nData collector 'Code Coverage' message: No code coverage data\navailable. Profiler was not initialized..\n4.8409\n##[error]Could not find testhost\nData collector 'Code Coverage' message: No code coverage data\navailable. Profiler was not initialized..\n2.1874\n##[error]Could not find testhost\n\nI compared the output of the successful run and the failed run and found the different version of the test platform. If you don't specify a version, the default version will be the latest and probably a preview one. So I add something in YAML to specify a workable version.\n- task: VisualStudioTestPlatformInstaller@1\n  inputs:\n    versionSelector: 'SpecificVersion'\n    testPlatformVersion: '17.2.0'\n- task: VSTest@2\n  inputs:\n    platform: '$(buildPlatform)'\n    configuration: '$(buildConfiguration)'\n    codeCoverageEnabled: True\n    vsTestVersion: 'toolsInstaller'\n\nhttps:\/\/docs.microsoft.com\/en-us\/azure\/devops\/pipelines\/tasks\/test\/vstest?view=azure-devops\nhttps:\/\/docs.microsoft.com\/en-us\/azure\/devops\/pipelines\/tasks\/tool\/vstest-platform-tool-installer?view=azure-devops\n","meta":{"link":"https:\/\/stackoverflow.com\/questions\/73404846\/devops-vstest2-errorcould-not-find-testhost"},"label":[[26,50,"problem"],[4892,5092,"problem"],[5098,5150,"resolution"]],"Comments":[]}
{"id":345,"text":"How to install CRM 2016 Required Components without internet access. I am installing Microsoft Dynamics CRM 2016 (Full Server Role) on an isolated Windows Server 2012 R2 which doesn't have internet access. This is a constraint I have to deal with so it is not even possible for me to 'temporay enable internet'. MSSQL 2012 is on a different box.\nI've been able to manually download, copy the files, and install all prerequisites but two:\n\nMicrosoft Application Error Reporting\nSQL Native Client\n\n\nWhere\/how do I install these two remaining prerequisites?\nStrange thing is I downloaded native client (and other SQL prereqs) from  Microsoft® SQL Server® 2012 Feature Pack page and native client show in program files.\nI've also turned-on \"Windows Error Reporting\" thinking it might solve the \"Microsoft Application Error Reporting\" prerequisite.\nI've looked at the Software requirements and no specific server role is specified. Do I need to install the \"Application Server Role\" ??? This is the only thing I can think of for the error reporting.\nAny help is welcome! You MUST install SQL 2008 Native Client!\nSo the answer was finally obvious. Even if the item indicates 'minimum version required' and even if CRM 2016 requires SQL 2012, installing SQL 2012 Native Client does not work! You must install SQL Native client 2008 (you may select only the native client from the feature pack). I installed 2008 R2 SP3 and the requirement was met.\n\nFinally once all SQL requirements are met, you may click \"Install\" to install Application Error reporting. This will install properly without requiring any downloads.\n\n","meta":{"link":"https:\/\/stackoverflow.com\/questions\/37221390\/how-to-install-crm-2016-required-components-without-internet-access"},"label":[[7,68,"problem"],[346,494,"problem"],[1074,1105,"resolution"],[1236,1283,"problem"],[1494,1547,"resolution"]],"Comments":[]}
{"id":346,"text":"Error in building LibreOffice while running .\/autogen.sh in cygwin shell. I was following this link to build LibreOffice : https:\/\/blog.documentfoundation.org\/blog\/2019\/07\/09\/start-developing-libreoffice-download-the-source-code-and-build-on-windows\/\nWhen I was on the 8th step, I ran .\/autogen.sh in the Cygwin shell, it gave the following error :\n$src_path must not contain spaces, but it is '$src_path'. (It is the 135th line in the code of autogen.sh)\nTo remove the error, I removed the spaces in the username which was present at this location - C:\\cygwin64\\home<username>. It somehow created two usernames inside home, one with spaces and without spaces.\nAnyways I continued further because all the required files were present in the username without spaces. I ran .\/autogen.sh in the Cygwin shell after going to the right directory (i.e username without spaces)  but then the following error came up :\n\"cannot find al.exe as \/al.exe\" (It showed that error is in line 296 of autogen.sh)\nI thought I did something wrong and tried to undo the things.\nSo I again introduced the spaces in the username and it merged the two usernames(It merged the username with spaces and without spaces).\nThen I downloaded the raw file of .\/autogen.sh from https:\/\/github.com\/LibreOffice\/core\/blob\/master\/autogen.sh\nand commented out the 135th line which said \"die \"$src_path must not contain spaces, but it is '$src_path'.\" if ($src_path =~ \/ \/)\"\nThen I again ran .\/autogen.sh and it gave the following error :\n.\/autogen.sh: line 1: $':\\r': command not found\naclocal-1.16: error: non-option arguments are not accepted: 'Khare\/libreoffice\/m4'.\naclocal-1.16: Try '\/usr\/bin\/aclocal-1.16 --help' for more information.\nFailed to run aclocal at .\/autogen.sh line 196.\n\nI am stuck. Someone please help. I removed spaces from Cygwin user name following this:\nRename\/change cygwin username\nAnd it solved the problem for me.\n","meta":{"link":"https:\/\/stackoverflow.com\/questions\/62700322\/error-in-building-libreoffice-while-running-autogen-sh-in-cygwin-shell"},"label":[[349,406,"problem"],[1786,1868,"resolution"]],"Comments":[]}
{"id":347,"text":"Can no longer ignore certificate errors during publishing in VS2012. I've been using the MSDeploy publishing service from Visual Studio to publish my projects.  The publishing URL is https:\/\/machinename.domain.com:8172\/MsDeploy.axd.  It worked fine in VS2010 -- I was able to check a checkbox that said ignore certificate errors.  That box is no longer shown in VS 2012.  I found a post indicating that I'm supposed to get prompted about whether to ignore certificate errors -- but I'm not and I can no longer publish my project.  Any ideas? Open your publish profile from \/Properties\/PublishProfiles\/<Profile>.pubxml and add the following line within the PropertyGroup element:\n<AllowUntrustedCertificate>True<\/AllowUntrustedCertificate>\n\n","meta":{"link":"https:\/\/stackoverflow.com\/questions\/14692368\/can-no-longer-ignore-certificate-errors-during-publishing-in-vs2012"},"label":[[0,67,"problem"],[542,738,"resolution"]],"Comments":[]}
{"id":348,"text":"Laravel app deploy on linux server using nginx got 404 error. This is my last step for my class final project. I follow the steps from the Digital-Ocean {{ https:\/\/www.digitalocean.com\/community\/tutorials\/how-to-deploy-a-laravel-application-with-nginx-on-ubuntu-16-04 }} but I got the 404 error when I call the URL.\nThe following code is the nginx configuration. As I am a new to laravel, i also don't know how to deploy it. This is my first time deployment using the server also. \n    # Default server configuration\n    #\n    server {\n            listen 80;\n            listen [::]:80;\n # SSL configuration\n        #\n         #listen 443 ssl http2;\n        # listen [::]:443 ssl http2;\n\n        access_log            \/var\/log\/nginx\/jenkins.access.log;\n        error_log            \/var\/log\/nginx\/jenkins.error.log;\n        #\n        # Note: You should disable gzip for SSL traffic.\n        # See: https:\/\/bugs.debian.org\/773332\n # Read up on ssl_ciphers to ensure a secure configuration.\n        # See: https:\/\/bugs.debian.org\/765782\n        #\n        # Self signed certs generated by the ssl-cert package\n        # Don't use them in a production server!\n        #\n        # include snippets\/ssl-marikhu.com.conf;\n        #include snippest\/ssl-params.conf;\n\n        root \/var\/www\/laravel\/smartroom\/public;\n\n        # Add index.php to the list if you are using PHP\n        index index.php index.html index.htm index.ngix-debian.html;\n\n        server_name marikhu.com www.marikhu.com;\n\n        location \/ {\n                # First attempt to serve request as file, then\n                # as directory, then fall back to displaying a 404.\n  try_files $uri $uri\/\/index.php?query_string;\n                include \/etc\/nginx\/proxy_params;\n                proxy_pass          http:\/\/localhost:8080;\n                proxy_read_timeout  90s;\n                # Fix potential \"It appears that your reverse proxy set up is b$\n                proxy_redirect      http:\/\/localhost:8080 https:\/\/marikhu.com;\n        }\n\n        # pass the PHP scripts to FastCGI server listening on 127.0.0.1:9000\n        #\n      location ~ \\.php$ {\n                include snippets\/fastcgi-php.conf;\n        #\n        #       # With php7.0-cgi alone:\n        #       fastcgi_pass 127.0.0.1:9000;\n        #       # With php7.0-fpm:\n                fastcgi_pass unix:\/run\/php\/php7.0-fpm.sock;\n        }\n\n        # deny access to .htaccess files, if Apache's document root\n  # concurs with nginx's one\n        #\n        location ~ \/\\.ht {\n                deny all;\n        }\n\n    listen 443 ssl; # managed by Certbot\nssl_certificate \/etc\/letsencrypt\/live\/marikhu.com\/fullchain.pem; # managed by C$\nssl_certificate_key \/etc\/letsencrypt\/live\/marikhu.com\/privkey.pem; # managed by$\n    include \/etc\/letsencrypt\/options-ssl-nginx.conf; # managed by Certbot\n}\n\n\n# Virtual Host configuration for example.com\n#\n# You can move that to a different file under sites-available\/ and symlink that\n# to sites-enabled\/ to enable it.\n#\n#server {\n#       listen 80;\n#       listen [::]:80;\n#\n#       server_name moodymountains.marikhu.com;\n#\n#       root \/var\/www\/html;\n#       index index.php;\n#\n}\n\n\n# Virtual Host configuration for example.com\n#\n# You can move that to a different file under sites-available\/ and symlink that\n# to sites-enabled\/ to enable it.\n#\n#server {\n#       listen 80;\n#       listen [::]:80;\n#\n#       server_name moodymountains.marikhu.com;\n#\n#       root \/var\/www\/html;\n#       index index.php;\n#\n}\n\n\n# Virtual Host configuration for example.com\n#\n# You can move that to a different file under sites-available\/ and symlink that\n# to sites-enabled\/ to enable it.\n#\n#server {\n#       listen 80;\n#       listen [::]:80;\n#\n#       server_name moodymountains.marikhu.com;\n#\n#       root \/var\/www\/html;\n#       index index.php;\n#}\n#       location \/ {\n#               try_files $uri $uri\/ =404;\n#       }\n#} Replace this block\nlocation \/ {\n                # First attempt to serve request as file, then\n                # as directory, then fall back to displaying a 404.\n  try_files $uri $uri\/\/index.php?query_string;\n                include \/etc\/nginx\/proxy_params;\n                proxy_pass          http:\/\/localhost:8080;\n                proxy_read_timeout  90s;\n                # Fix potential \"It appears that your reverse proxy set up is b$\n                proxy_redirect      http:\/\/localhost:8080 https:\/\/marikhu.com;\n        }\n\nwith this:\nlocation \/ {\n    try_files $uri $uri\/ \/index.php?$query_string;\n}\n\nsave the file, restart your nginx server and try to visit again.\n","meta":{"link":"https:\/\/stackoverflow.com\/questions\/47732264\/laravel-app-deploy-on-linux-server-using-nginx-got-404-error"},"label":[[0,61,"problem"],[3877,4549,"resolution"]],"Comments":[]}
{"id":350,"text":"Android Studio installation rule?. I have just downloaded the last build (v0.8.14)\nI got a .RAR file. Extracted it and got a folder with some other files.\nI'm on W7 64bit so i can't run the \"studio.exe\" file, i had to run \"studio64.exe\".\nBut the IDE just starts itself... No errors, nothing, it just starts, it does not install... I mean, I'm not sent to a wizard process or something...\nDoes this has anything to do with the new politics that the SDK cannot be inside the installation path anymore and stuff?\nAlso, is this normal? (because of any other new politics) 0.8.14 is not the newest one. In Canary Channel is already 1.0 RC2, and if you really like installers it is also there:\nhttp:\/\/tools.android.com\/download\/studio\/canary\/1-0rc2\n","meta":{"link":"https:\/\/stackoverflow.com\/questions\/27206013\/android-studio-installation-rule"},"label":[[242,371,"problem"],[659,742,"resolution"]],"Comments":[]}
{"id":352,"text":"OutOfMemoryError while deploying a war from Jenkins to Tomcat. I deploy my application from Jenkins my using the following shell command:\nrm -rf E:\/FooTomcat\/apache-tomcat-7.0.55\/webapps\/foo-web;\nrm -rf E:\/FooTomcat\/apache-tomcat-7.0.55\/webapps\/foo-web.war;\nsleep 2;\ncp foo-web\/target\/foo-web-0.0.1-SNAPSHOT.war E:\/FooTomcat\/apache-tomcat-7.0.55\/webapps\/foo-web.war\n\nOften when the periodic build is started the Tomcat runs out of memory giving the following error.\nFeb 13, 2015 3:59:58 PM org.apache.catalina.startup.HostConfig deployWAR\nINFO: Deploying web application archive E:\\FooTomcat\\apache-tomcat-7.0.55\\webapps\\foo-web.war\nFeb 13, 2015 4:00:06 PM org.apache.catalina.startup.HostConfig deployWARs\nSEVERE: Error waiting for multi-thread deployment of WAR files to complete\njava.util.concurrent.ExecutionException: java.lang.OutOfMemoryError: PermGen space\n    at java.util.concurrent.FutureTask.report(FutureTask.java:122)\n    at java.util.concurrent.FutureTask.get(FutureTask.java:188)\n    at org.apache.catalina.startup.HostConfig.deployWARs(HostConfig.java:818)\n    at org.apache.catalina.startup.HostConfig.deployApps(HostConfig.java:488)\n    at org.apache.catalina.startup.HostConfig.check(HostConfig.java:1658)\n\nI have a windows environment and executing the shell through cygwin.\nIs someting wrong with the shell command, or is it due to some memory leak? I introduced the sleep to give some time for the application to undeploy. I have never tried by increasing the sleep duration.\nThanks. As the exception indicates your problem comes from filling up the PermGen space. It is a part of memory where Java stores the metadata about your classes (let's say the code of your loaded classes). When you deploy a new web-app you add new classes and increase the load of PermGen. What is worse, when you re-deploy the same app in 90% of the cases, the previous versions of the used classes stays in the memmory so you do not release the 'old' memory from PermGen but just add to it (the 90% estimation comes from using DataBase drivers, frameworks which uses ThreadLocal, Scheduler threads ... in reality it happens almost all the time). \nThe default permgen is too small (like 64M or something as ridiculous). Start tomcat with higher values, you do it by passing to tomcat the JVM options, for example:\nJAVA_OPTS = -XX:MaxPermSize=512m -Xmx4024m\n(first set the perm size to 512MB second the heap size to 4G, which is fine for modern system).\nYou set this variable before you start tomcat, or (I prefer it that way, you can modify your \/bin\/catalina script to always have them set up there, that way you will not 'forget' them if you start your tomcat again).\nFrom tomact 7, when you undeploy app, the tomcat logs show warnings of apps that stays in memmory (and fill up your PermGen), you may want to check them and try to fix some problems (updating frameworks, shuting down properly your threads pool ...)\n","meta":{"link":"https:\/\/stackoverflow.com\/questions\/28536190\/outofmemoryerror-while-deploying-a-war-from-jenkins-to-tomcat"},"label":[[0,62,"problem"],[715,864,"problem"],[2153,2181,"problem"],[2221,2300,"resolution"]],"Comments":[]}
{"id":353,"text":"Cant install a package in R. I cant seem to install this package.\npackage ‘zoo’ successfully unpacked and MD5 sums checked\nWarning: unable to move temporary installation ‘C:\\Users\\Xiang\\Documents\\R\\win-library\\3.2\\file248444b815fb\\zoo’ to ‘C:\\Users\\Xiang\\Documents\\R\\win-library\\3.2\\zoo’\nDoes anyone have a solution to solve this problem?\nThank you. There are a couple of potential solutions at this link: Windows 7, update.packages problem: \"unable to move temporary installation\"?\nCould be an antivirus problem, or possibly lacking administrator privileges. The link recommends installing R in a different folder, or downloading the zip file for the package and installing it by hand. \n","meta":{"link":"https:\/\/stackoverflow.com\/questions\/30515619\/cant-install-a-package-in-r"},"label":[[0,27,"problem"],[132,169,"problem"],[483,558,"problem"],[580,685,"resolution"]],"Comments":[]}
{"id":354,"text":"Prevent downtime using lftp mirror. I'm using lftp to deploy a website via Travis CI. There is a build process before the deployment, for that reason a build directory is present and pushed to the root of the ftp server.\nlftp $FTP_URL -e \"glob -d mirror build . --reverse --delete-first --parallel=10 && exit\"\n\nIt works quite well, but I dislike to have a downtime \/ temporary PHP parse errors because of missing files on my website. What is the best way to work arround that issue?\n\nMy first approach was an option to set a temporary directory, but the lftp man page says there is only a options for temporary files. I still tried the option but it didn't help.\n\nMy second approach was to use \"mirror build temp\" to use a temporary folder and then replace the root with it. The problem here is, that I cannot exclude the temp folder while deleting the old files and folders like rm -rf *. Finally,\nI wrote a tiny Grunt plugin that can handle lftp operations quite well and helps to process files and directories.\nThis is a zero downtown pattern - each placeholder is going to be automatically replaced:\ncommand:\n[\n    'mirror {SOURCE} {TARGET}-new-{TIMESTAMP} --reverse --delete-first',\n    'mv {TARGET} {TARGET}-old-{TIMESTAMP}',\n    'mv {TARGET}-new-{TIMESTAMP} {TARGET}',\n    'rm -rf {TARGET}-old-{TIMESTAMP}',\n    'exit'\n]\n\nThis would be the Vanilla lftp command - remember to manually replace each placeholder:\nlftp $FTP_URL -e \"mirror {SOURCE} {TARGET}-new-{TIMESTAMP} --reverse --delete-first; mv {TARGET} {TARGET}-old-{TIMESTAMP}; mv {TARGET}-new-{TIMESTAMP} {TARGET}; rm -rf {TARGET}-old-{TIMESTAMP}; exit\"\n\n","meta":{"link":"https:\/\/stackoverflow.com\/questions\/40814642\/prevent-downtime-using-lftp-mirror"},"label":[[356,432,"problem"],[1113,1327,"resolution"],[1417,1616,"resolution"]],"Comments":[]}
{"id":355,"text":"nuget.build.tasks.pack - build error author is required. Because of this question I tried to change my Azure Devops build process to use DotNet isntead of Nuget\nThe task is now\n- task: DotNetCoreCLI@2\n  inputs:\n    command: pack\n    packagesToPack: '**\/SBD.*.csproj'\n    versioningScheme: byPrereleaseNumber\n    majorVersion: '$(Major)'\n    minorVersion: '$(Minor)'\n    patchVersion: '$(Patch)\n\nAnd the error is\nerror MSB4057: The target \"pack\" does not exist in the project\n\nI found this question but it is not for DevOps\nThe answer mentions I need to install the Nuget.Build.Tasks Pack package from Nuget\nSo I did that\nNow the log in devops states\nC:\\Users\\VssAdministrator\\.nuget\\packages\\nuget.build.tasks.pack\\5.6.0\\build\\NuGet.Build.Tasks.Pack.targets(198,5): \nerror : Authors is required. [D:\\a\\1\\s\\Common.WinForms\\SBD.Common.WinForms.csproj] The failing project was net472. I should not use DotNetCoreCli to pack it \n","meta":{"link":"https:\/\/stackoverflow.com\/questions\/62275776\/nuget-build-tasks-pack-build-error-author-is-required"},"label":[[0,55,"problem"],[854,880,"problem"],[884,924,"resolution"]],"Comments":[]}
{"id":356,"text":"`GetJREPath()` when installing IntelliJ via command line OSX. I was trying to be able to install Intellj hands free by running this:\nwget https:\/\/download.jetbrains.com\/idea\/ideaIC-2017.2.3.dmg\nhdiutil mount -nobrowse ideaIC-2017.2.3.dmg\nmkdir \/Applications\/IntelliJ\\ IDEA\\ CE.app\ncp -r \/Volumes\/IntelliJ\\ IDEA\\ CE\/IntelliJ\\ IDEA\\ CE.app\/ \/Applications\/IntelliJ\\ IDEA\\ CE.app\/\nhdiutil unmount \/Volumes\/IntelliJ\\ IDEA\\ CE\/ \nrm ideaIC-2017.2.3.dmg\n\nI notice when installing on OSX and the Intellij.app file gets coppied from the mount in \/Volumes\/IntelliJ into a dir in my \/Applications folder from the command line with cp, I have the error:\nError: could not find libjava.dylib\nFailed to GetJREPath()\n\nWhen running the executable file in \/Applications\/IntelliJ IDEA CE.app\/Contents\/MacOS.\nSpecifically, I got:\nLSOpenURLsWithRole() failed with error -10810 for the file \/Applications\/IntelliJ IDEA CE.app.\nwhen I tried to:\nopen \/Applications\/IntelliJ\\ IDEA\\ CE.app\/\nBut none of this goes wrong when I try manually \"drag-and-drop\" the app from the Finder window. What's going on here?\nInfo:\nOutput when I \"drag-and-drop\":\n.\/idea \n2017-08-31 18:14:27.258 idea[5098:151348] allVms required 1.8*,1.8+\n2017-08-31 18:14:27.260 idea[5098:151359] Value of IDEA_VM_OPTIONS is (null)\n2017-08-31 18:14:27.260 idea[5098:151359] fullFileName is: \/Applications\/IntelliJ IDEA CE.app\/Contents\/bin\/idea.vmoptions\n2017-08-31 18:14:27.260 idea[5098:151359] fullFileName exists: \/Applications\/IntelliJ IDEA CE.app\/Contents\/bin\/idea.vmoptions\n2017-08-31 18:14:27.260 idea[5098:151359] Processing VMOptions file at \/Applications\/IntelliJ IDEA CE.app\/Contents\/bin\/idea.vmoptions\n2017-08-31 18:14:27.260 idea[5098:151359] Done\n\/Applications\/IntelliJ IDEA CE.app\/Contents\/bin\/idea.properties: 'java.endorsed.dirs = ' already defined in system properties: \/Applications\/IntelliJ IDEA CE.app\/Contents\/jdk\/Contents\/Home\/jre\/lib\/endorsed\n\nOutput when I mkdir then cp:\n.\/idea \n2017-08-31 18:15:56.295 idea[5134:152286] allVms required 1.8*,1.8+\n2017-08-31 18:15:56.297 idea[5134:152301] Value of IDEA_VM_OPTIONS is (null)\n2017-08-31 18:15:56.297 idea[5134:152301] fullFileName is: \/Applications\/IntelliJ IDEA CE.app\/Contents\/bin\/idea.vmoptions\n2017-08-31 18:15:56.297 idea[5134:152301] fullFileName exists: \/Applications\/IntelliJ IDEA CE.app\/Contents\/bin\/idea.vmoptions\n2017-08-31 18:15:56.297 idea[5134:152301] Processing VMOptions file at \/Applications\/IntelliJ IDEA CE.app\/Contents\/bin\/idea.vmoptions\n2017-08-31 18:15:56.298 idea[5134:152301] Done\nError: could not find libjava.dylib\nFailed to GetJREPath()\n2017-08-31 18:15:56.298 idea[5134:152301] JNI_CreateJavaVM (\/Applications\/IntelliJ IDEA CE.app\/Contents\/jdk) failed: 4294967295 You should use the following command instead:\ncp -R \/Volumes\/IntelliJ\\ IDEA\\ CE\/IntelliJ\\ IDEA\\ CE.app\/ \/Applications\/IntelliJ\\ IDEA\\ CE.app\/\n\nNotice -R option instead of -r.\nAccording to this man page, -r works differently on macOS:\n\nCOMPATIBILITY\nHistoric versions of the cp utility had a -r option.  This implementation supports that option; however, its use is strongly discouraged, as it does not correctly copy special files, symbolic links, or\nfifo's.\n\n","meta":{"link":"https:\/\/stackoverflow.com\/questions\/45991832\/getjrepath-when-installing-intellij-via-command-line-osx"},"label":[[456,640,"problem"],[648,699,"problem"],[809,963,"problem"],[2752,2847,"resolution"]],"Comments":[]}
{"id":357,"text":"\"Invalid Server URL\" error after replacing .jar files while installing GraphDB. I was trying to install GraphDB to complement default Sesame. Following the instructions for 'Easy Install' in http:\/\/owlim.ontotext.com\/display\/OWLIMv54\/OWLIM-SE+Installation, I copied the .war files from GraphDB distribution to my Tomcat directory and replaced the pre-existing .war files that came with Sesame. However, I saw no 'OWLIM-SE' repositories in the Sesame drop down after restarting the server.\nWhile troubleshooting, I replaced the .jar files in C:\\Program Files\\Apache Software Foundation\\Tomcat 7.0\\webapps\\openrdf-sesame\\WEB-INF\\lib with those from the most recent Sesame version to see if it makes any difference (I had 2.8.3).\nDifference it did make. I now get only the 'Change Server' page. Upon entering the URL of my server as http:\/\/my.ip.address:8080\/openrdf-sesame, I get an \"Invalid Server url\" error. I had changed the server url from localhost to ip address so that I can access it externally. How can I resolve two issues:\n\nGet Sesame working again\nInstall GraphDB\n\nFollowing is the log:\nJul 08, 2015 4:41:24 PM org.openrdf.workbench.proxy.ServerValidator canConnect\nWARNING: java.io.FileNotFoundException: http:\/\/my.ip.address.here:8080\/openrdf-sesame\/protocol\njava.io.FileNotFoundException: http:\/\/my.ip.address.here\/openrdf-sesame\/protocol\n    at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1834)\n    at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1439)\n    at java.net.URL.openStream(URL.java:1038)\n    at org.openrdf.workbench.proxy.ServerValidator.canConnect(ServerValidator.java:121)\n    at org.openrdf.workbench.proxy.ServerValidator.isValidServer(ServerValidator.java:76)\n    at org.openrdf.workbench.proxy.WorkbenchGateway.findWorkbenchServlet(WorkbenchGateway.java:251)\n    at org.openrdf.workbench.proxy.WorkbenchGateway.service(WorkbenchGateway.java:121)\n    at org.openrdf.workbench.base.BaseServlet.service(BaseServlet.java:141)\n    at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:303)\n    at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)\n    at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:52)\n    at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241)\n    at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)\n    at org.openrdf.workbench.proxy.CookieCacheControlFilter.doFilter(CookieCacheControlFilter.java:63)\n    at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241)\n    at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)\n    at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:220)\n    at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:122)\n    at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:501)\n    at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:171)\n    at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:103)\n    at org.apache.catalina.valves.AccessLogValve.invoke(AccessLogValve.java:950)\n    at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:116)\n    at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:408)\n    at org.apache.coyote.http11.AbstractHttp11Processor.process(AbstractHttp11Processor.java:1070)\n    at org.apache.coyote.AbstractProtocol$AbstractConnectionHandler.process(AbstractProtocol.java:611)\n    at org.apache.tomcat.util.net.JIoEndpoint$SocketProcessor.run(JIoEndpoint.java:314)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n    at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)\n    at java.lang.Thread.run(Thread.java:745) Resolved! Here's what worked:\nI first installed GraphDB. Then, I replaced the jar files of GraphDB with those of Sesame 2.7.16. I copied the same set of files to the 'libs' folder of both Sesame-openrdf and Sesame-workbench. Though some jar files are duplicate, I did not remove one version for the other.\n\nFor anyone reading this, please note this is a risky workaround at best. Implementing CORS is a better option. More information is available at http:\/\/answers.ontotext.com\/questions\/1984\/previous-functional-ajax-code-with-jsonp-not-working-with-graphdb-lite.\n","meta":{"link":"https:\/\/stackoverflow.com\/questions\/31300044\/invalid-server-url-error-after-replacing-jar-files-while-installing-graphdb"},"label":[[0,78,"problem"],[4146,4340,"resolution"],[4496,4513,"resolution"]],"Comments":[]}
{"id":358,"text":"Azure devops page conflict. 2 developers are updating the same file in the same project. the last update gives an error. Because the first updated codes are not found in the file that the 2nd developer is trying to update. Migrate error.\nThis way, the first updated user's codes are deleted. How can I solve this problem?\nAzure devops Repos style.css\na{}b{}\n\n\nDeveloper's style.css (First updater)\na{}b{}c{}d{} \n\n2.Developer's style.css\na{}b{}e{}\n\nupdated Azure devops repos style.css\na{}b{}e{}\n\nc{} and d{} disappeared Your team needs to select and adhere to a version control workflow that meets your needs and that is supported by Git (the distributed source control you have apparently chosen). I suggest starting here: Azure Repos Git tutorial. \nYou might even question whether Git is the appropriate source control choice given your needs. See Choosing the right version control for your project\n","meta":{"link":"https:\/\/stackoverflow.com\/questions\/55359198\/azure-devops-page-conflict"},"label":[[28,291,"problem"],[539,637,"resolution"]],"Comments":[]}
{"id":359,"text":"Continue Azure Pipeline on failed task. I have a task that runs Cypress:\n-ErrorAction SilentlyContinue\ncd $(System.DefaultWorkingDirectory)\/_ClientWeb-Build-CI\/ShellArtifact\/\nnpx cypress run\n\nAnd I've set the ErrorActionPreference to continue. But when my Cypress fails: \n##[error]PowerShell exited with code '1'.\n\nThe next task is canceled and the release failed. How do I continue the release even Cypress failed, and is it possible to give a boolean a true\/false value based on the Cypress task result? If you want to continue the release even the Cypress task failed, just add to the Cypress task this line:\ncontinueOnError: true \n\n","meta":{"link":"https:\/\/stackoverflow.com\/questions\/56925979\/continue-azure-pipeline-on-failed-task"},"label":[[248,270,"problem"],[315,364,"problem"],[577,633,"resolution"]],"Comments":[]}
{"id":360,"text":"Deploying Django on IIS and ngrok. I am trying to deploy Django on local host and \"tunnel\" using ngrok. The ngrok works but the IIS (Internet Information Manager) gives 500 Error <handler> scriptProcessor could not be found in <fastCGI> application configuration. Reference into fastcgi shows that this feature is deprecated but what is the replacement for serving Django using local server and ngrok. I also pip installed pyngrok. Can you suggest a clear solution? FastCGI was deprecated in Django 6+ years ago, their docs say WSGI is the preferred alternative, and they provide a tutorial for types of WSGI deployments to get you started.\nBut you wouldn't use ngrok in this case, you'd serve it up with something like nginx or apache using a wsgi mod (also shown in their tutorial). Where you'd use ngrok is in development with Django's built-in dev server, and that's the full example provided in pyngrok's documentation.\nUsually I'd provide actual sample code here, but what you're asking about are full end-to-end solutions, which is why I'm providing links. Without the full context and examples of what you've built, it's hard to tell you where it's going wrong—hard to provide specific solutions without specific examples of the problem. But these tutorials tutorials are for exactly what you're doing, so hopefully they can help you debug your own solution.\n","meta":{"link":"https:\/\/stackoverflow.com\/questions\/68438825\/deploying-django-on-iis-and-ngrok"},"label":[[128,262,"problem"],[466,511,"problem"],[528,561,"resolution"],[688,752,"resolution"],[785,858,"resolution"]],"Comments":[]}
{"id":361,"text":"Cannot deploy project in MyEclipse for Spring 10. I have MyEclipse for Spring 10 on Mac OS X 10.7.3. I use Tomcat 6.0.32 installed via Macports 2.0.4, instead of MyEclipse integrated sandbox.\nWhen I tried to deploy my project, it kept giving me \nSenCal could not be redeployed because it could not be completely removed in the undeployment phase...........\n\nI tried stop tomcat then deploy. I tried remove libraries from build path then add them back again, as most people suggested. I even tried reinstall tomcat6. None of these solved my problem.\nCan anyone help me fix this?\nThanks,\nMilo Stupid mistake. The permission on Webapps folder was wrong. Problem solved.\n","meta":{"link":"https:\/\/stackoverflow.com\/questions\/9504242\/cannot-deploy-project-in-myeclipse-for-spring-10"},"label":[[246,345,"problem"],[607,650,"problem"]],"Comments":[]}
{"id":362,"text":"Error: Error: Failed to deploy web package to App Service. Bad Request (CODE: 400). I am using the Devops release pipeline to deploy new releases to my function apps.\nHowever a few days ago, the release pipeline stopped working and I continue to receive the following error message on deployments:\nError: Error: Failed to deploy web package to App Service. Bad Request (CODE: 400)\nAfter digging in the kudu stack trace, I find the following:\nError occurred, type: error, text: No space left on device\nHowever, when I look at my quota usage on the app service plan that hosts my function apps, I have not even exceeded 1% of my quota. I am on a basic app service plan that provides 10gb of storage and my function apps combined have used 50mib.\nI am reaching out to see if anyone else has\/is currently experiencing this issue and what they did to resolve it.\nMany Thanks!\nAdam If anyone encounters this issue in the future this may resolve your problem.\nI believe the problem with deployments may have been caused by a corrupt function app in the app service plan. I noticed that the function app I recently added had been categorised as a container although I never specified it as such. Anyway, deleting that function app and recreating it resolved the issue. All deployments to that app service plan are now working fine!\n","meta":{"link":"https:\/\/stackoverflow.com\/questions\/65098456\/error-error-failed-to-deploy-web-package-to-app-service-bad-request-code-40"},"label":[[0,82,"problem"],[477,500,"problem"],[1018,1062,"problem"],[1196,1240,"resolution"]],"Comments":[]}
{"id":363,"text":"Issue when trying to deploy a Struts2 Spring 3 web application to Weblogic 10.3.5 from eclipse. When I try to deploy a simple (Hello World) application from eclipse to Weblogic 10.3.5, I get the following error: \n<Jun 11, 2013 9:28:43 PM EDT> <Error> <org.apache.struts2.dispatcher.Dispatcher> <BEA-000000> <Dispatcher initialization failed\nUnable to load configuration. - bean - zip:C:\/Oracle\/FRMiddleware\/user_projects\/domains\/base_domain\/servers\/AdminServer\/tmp\/_WL_user\/_auto_generated_ear_\/injyo5\/war\/WEB-INF\/lib\/struts2-core-2.3.14.jar!\/struts-default.xml:29:72\n    at com.opensymphony.xwork2.config.ConfigurationManager.getConfiguration(ConfigurationManager.java:70)\n    at org.apache.struts2.dispatcher.Dispatcher.init_PreloadConfiguration(Dispatcher.java:429)\n    at org.apache.struts2.dispatcher.Dispatcher.init(Dispatcher.java:473)\n    at org.apache.struts2.dispatcher.ng.InitOperations.initDispatcher(InitOperations.java:74)\n    at org.apache.struts2.dispatcher.ng.filter.StrutsPrepareAndExecuteFilter.init(StrutsPrepareAndExecuteFilter.java:51)\n    at weblogic.servlet.internal.FilterManager$FilterInitAction.run(FilterManager.java:332)\n    at weblogic.security.acl.internal.AuthenticatedSubject.doAs(AuthenticatedSubject.java:321)\n    at weblogic.security.service.SecurityManager.runAs(SecurityManager.java:120)\n    at weblogic.servlet.internal.FilterManager.loadFilter(FilterManager.java:98)\n    at weblogic.servlet.internal.FilterManager.preloadFilters(FilterManager.java:59)\n    at weblogic.servlet.internal.WebAppServletContext.preloadResources(WebAppServletContext.java:1876)\n    at weblogic.servlet.internal.WebAppServletContext.start(WebAppServletContext.java:3153)\n    at weblogic.servlet.internal.WebAppModule.startContexts(WebAppModule.java:1508)\n    at weblogic.servlet.internal.WebAppModule.start(WebAppModule.java:482)\n    at weblogic.application.internal.flow.ModuleStateDriver$3.next(ModuleStateDriver.java:425)\n    at weblogic.application.utils.StateMachineDriver.nextState(StateMachineDriver.java:52)\n    at weblogic.application.internal.flow.ModuleStateDriver.start(ModuleStateDriver.java:119)\n    at weblogic.application.internal.flow.ScopedModuleDriver.start(ScopedModuleDriver.java:200)\n    at weblogic.application.internal.flow.ModuleListenerInvoker.start(ModuleListenerInvoker.java:247)\n    at weblogic.application.internal.flow.ModuleStateDriver$3.next(ModuleStateDriver.java:425)\n    at weblogic.application.utils.StateMachineDriver.nextState(StateMachineDriver.java:52)\n    at weblogic.application.internal.flow.ModuleStateDriver.start(ModuleStateDriver.java:119)\n    at weblogic.application.internal.flow.StartModulesFlow.activate(StartModulesFlow.java:27)\n    at weblogic.application.internal.BaseDeployment$2.next(BaseDeployment.java:636)\n    at weblogic.application.utils.StateMachineDriver.nextState(StateMachineDriver.java:52)\n    at weblogic.application.internal.BaseDeployment.activate(BaseDeployment.java:205)\n    at weblogic.application.internal.EarDeployment.activate(EarDeployment.java:58)\n    at weblogic.application.internal.DeploymentStateChecker.activate(DeploymentStateChecker.java:161)\n    at weblogic.deploy.internal.targetserver.AppContainerInvoker.activate(AppContainerInvoker.java:79)\n    at weblogic.deploy.internal.targetserver.operations.AbstractOperation.activate(AbstractOperation.java:569)\n    at weblogic.deploy.internal.targetserver.operations.ActivateOperation.activateDeployment(ActivateOperation.java:150)\n    at weblogic.deploy.internal.targetserver.operations.ActivateOperation.doCommit(ActivateOperation.java:116)\n    at weblogic.deploy.internal.targetserver.operations.StartOperation.doCommit(StartOperation.java:140)\n    at weblogic.deploy.internal.targetserver.operations.AbstractOperation.commit(AbstractOperation.java:323)\n    at weblogic.deploy.internal.targetserver.DeploymentManager.handleDeploymentCommit(DeploymentManager.java:844)\n    at weblogic.deploy.internal.targetserver.DeploymentManager.activateDeploymentList(DeploymentManager.java:1253)\n    at weblogic.deploy.internal.targetserver.DeploymentManager.handleCommit(DeploymentManager.java:440)\n    at weblogic.deploy.internal.targetserver.DeploymentServiceDispatcher.commit(DeploymentServiceDispatcher.java:163)\n    at weblogic.deploy.service.internal.targetserver.DeploymentReceiverCallbackDeliverer.doCommitCallback(DeploymentReceiverCallbackDeliverer.java:195)\n    at weblogic.deploy.service.internal.targetserver.DeploymentReceiverCallbackDeliverer.access$100(DeploymentReceiverCallbackDeliverer.java:13)\n    at weblogic.deploy.service.internal.targetserver.DeploymentReceiverCallbackDeliverer$2.run(DeploymentReceiverCallbackDeliverer.java:68)\n    at weblogic.work.SelfTuningWorkManagerImpl$WorkAdapterImpl.run(SelfTuningWorkManagerImpl.java:528)\n    at weblogic.work.ExecuteThread.execute(ExecuteThread.java:209)\n    at weblogic.work.ExecuteThread.run(ExecuteThread.java:178)\nCaused By: Unable to load bean: type: class:com.opensymphony.xwork2.ObjectFactory - bean - zip:C:\/Oracle\/FRMiddleware\/user_projects\/domains\/base_domain\/servers\/AdminServer\/tmp\/_WL_user\/_auto_generated_ear_\/injyo5\/war\/WEB-INF\/lib\/struts2-core-2.3.14.jar!\/struts-default.xml:29:72\n    at com.opensymphony.xwork2.config.providers.XmlConfigurationProvider.register(XmlConfigurationProvider.java:245)\n    at org.apache.struts2.config.StrutsXmlConfigurationProvider.register(StrutsXmlConfigurationProvider.java:102)\n    at com.opensymphony.xwork2.config.impl.DefaultConfiguration.reloadContainer(DefaultConfiguration.java:226)\n    at com.opensymphony.xwork2.config.ConfigurationManager.getConfiguration(ConfigurationManager.java:67)\n    at org.apache.struts2.dispatcher.Dispatcher.init_PreloadConfiguration(Dispatcher.java:429)\n    at org.apache.struts2.dispatcher.Dispatcher.init(Dispatcher.java:473)\n    at org.apache.struts2.dispatcher.ng.InitOperations.initDispatcher(InitOperations.java:74)\n    at org.apache.struts2.dispatcher.ng.filter.StrutsPrepareAndExecuteFilter.init(StrutsPrepareAndExecuteFilter.java:51)\n    at weblogic.servlet.internal.FilterManager$FilterInitAction.run(FilterManager.java:332)\n    at weblogic.security.acl.internal.AuthenticatedSubject.doAs(AuthenticatedSubject.java:321)\n    at weblogic.security.service.SecurityManager.runAs(SecurityManager.java:120)\n    at weblogic.servlet.internal.FilterManager.loadFilter(FilterManager.java:98)\n    at weblogic.servlet.internal.FilterManager.preloadFilters(FilterManager.java:59)\n    at weblogic.servlet.internal.WebAppServletContext.preloadResources(WebAppServletContext.java:1876)\n    at weblogic.servlet.internal.WebAppServletContext.start(WebAppServletContext.java:3153)\n    at weblogic.servlet.internal.WebAppModule.startContexts(WebAppModule.java:1508)\n    at weblogic.servlet.internal.WebAppModule.start(WebAppModule.java:482)\n    at weblogic.application.internal.flow.ModuleStateDriver$3.next(ModuleStateDriver.java:425)\n    at weblogic.application.utils.StateMachineDriver.nextState(StateMachineDriver.java:52)\n    at weblogic.application.internal.flow.ModuleStateDriver.start(ModuleStateDriver.java:119)\n    at weblogic.application.internal.flow.ScopedModuleDriver.start(ScopedModuleDriver.java:200)\n    at weblogic.application.internal.flow.ModuleListenerInvoker.start(ModuleListenerInvoker.java:247)\n    at weblogic.application.internal.flow.ModuleStateDriver$3.next(ModuleStateDriver.java:425)\n    at weblogic.application.utils.StateMachineDriver.nextState(StateMachineDriver.java:52)\n    at weblogic.application.internal.flow.ModuleStateDriver.start(ModuleStateDriver.java:119)\n    at weblogic.application.internal.flow.StartModulesFlow.activate(StartModulesFlow.java:27)\n    at weblogic.application.internal.BaseDeployment$2.next(BaseDeployment.java:636)\n    at weblogic.application.utils.StateMachineDriver.nextState(StateMachineDriver.java:52)\n    at weblogic.application.internal.BaseDeployment.activate(BaseDeployment.java:205)\n    at weblogic.application.internal.EarDeployment.activate(EarDeployment.java:58)\n    at weblogic.application.internal.DeploymentStateChecker.activate(DeploymentStateChecker.java:161)\n    at weblogic.deploy.internal.targetserver.AppContainerInvoker.activate(AppContainerInvoker.java:79)\n    at weblogic.deploy.internal.targetserver.operations.AbstractOperation.activate(AbstractOperation.java:569)\n    at weblogic.deploy.internal.targetserver.operations.ActivateOperation.activateDeployment(ActivateOperation.java:150)\n    at weblogic.deploy.internal.targetserver.operations.ActivateOperation.doCommit(ActivateOperation.java:116)\n    at weblogic.deploy.internal.targetserver.operations.StartOperation.doCommit(StartOperation.java:140)\n    at weblogic.deploy.internal.targetserver.operations.AbstractOperation.commit(AbstractOperation.java:323)\n    at weblogic.deploy.internal.targetserver.DeploymentManager.handleDeploymentCommit(DeploymentManager.java:844)\n    at weblogic.deploy.internal.targetserver.DeploymentManager.activateDeploymentList(DeploymentManager.java:1253)\n    at weblogic.deploy.internal.targetserver.DeploymentManager.handleCommit(DeploymentManager.java:440)\n    at weblogic.deploy.internal.targetserver.DeploymentServiceDispatcher.commit(DeploymentServiceDispatcher.java:163)\n    at weblogic.deploy.service.internal.targetserver.DeploymentReceiverCallbackDeliverer.doCommitCallback(DeploymentReceiverCallbackDeliverer.java:195)\n    at weblogic.deploy.service.internal.targetserver.DeploymentReceiverCallbackDeliverer.access$100(DeploymentReceiverCallbackDeliverer.java:13)\n    at weblogic.deploy.service.internal.targetserver.DeploymentReceiverCallbackDeliverer$2.run(DeploymentReceiverCallbackDeliverer.java:68)\n    at weblogic.work.SelfTuningWorkManagerImpl$WorkAdapterImpl.run(SelfTuningWorkManagerImpl.java:528)\n    at weblogic.work.ExecuteThread.execute(ExecuteThread.java:209)\n    at weblogic.work.ExecuteThread.run(ExecuteThread.java:178)\nCaused By: Bean type class com.opensymphony.xwork2.ObjectFactory with the name xwork has already been loaded by bean - zip:C:\/Users\/brownb\/.m2\/repository\/org\/apache\/struts\/struts2-core\/2.3.14\/struts2-core-2.3.14.jar!\/struts-default.xml:29:72 - bean - zip:C:\/Oracle\/FRMiddleware\/user_projects\/domains\/base_domain\/servers\/AdminServer\/tmp\/_WL_user\/_auto_generated_ear_\/injyo5\/war\/WEB-INF\/lib\/struts2-core-2.3.14.jar!\/struts-default.xml:29:72\n    at com.opensymphony.xwork2.config.providers.XmlConfigurationProvider.register(XmlConfigurationProvider.java:229)\n    at org.apache.struts2.config.StrutsXmlConfigurationProvider.register(StrutsXmlConfigurationProvider.java:102)\n    at com.opensymphony.xwork2.config.impl.DefaultConfiguration.reloadContainer(DefaultConfiguration.java:226)\n    at com.opensymphony.xwork2.config.ConfigurationManager.getConfiguration(ConfigurationManager.java:67)\n    at org.apache.struts2.dispatcher.Dispatcher.init_PreloadConfiguration(Dispatcher.java:429)\n    at org.apache.struts2.dispatcher.Dispatcher.init(Dispatcher.java:473)\n    at org.apache.struts2.dispatcher.ng.InitOperations.initDispatcher(InitOperations.java:74)\n    at org.apache.struts2.dispatcher.ng.filter.StrutsPrepareAndExecuteFilter.init(StrutsPrepareAndExecuteFilter.java:51)\n    at weblogic.servlet.internal.FilterManager$FilterInitAction.run(FilterManager.java:332)\n    at weblogic.security.acl.internal.AuthenticatedSubject.doAs(AuthenticatedSubject.java:321)\n    at weblogic.security.service.SecurityManager.runAs(SecurityManager.java:120)\n    at weblogic.servlet.internal.FilterManager.loadFilter(FilterManager.java:98)\n    at weblogic.servlet.internal.FilterManager.preloadFilters(FilterManager.java:59)\n    at weblogic.servlet.internal.WebAppServletContext.preloadResources(WebAppServletContext.java:1876)\n    at weblogic.servlet.internal.WebAppServletContext.start(WebAppServletContext.java:3153)\n    at weblogic.servlet.internal.WebAppModule.startContexts(WebAppModule.java:1508)\n    at weblogic.servlet.internal.WebAppModule.start(WebAppModule.java:482)\n    at weblogic.application.internal.flow.ModuleStateDriver$3.next(ModuleStateDriver.java:425)\n    at weblogic.application.utils.StateMachineDriver.nextState(StateMachineDriver.java:52)\n    at weblogic.application.internal.flow.ModuleStateDriver.start(ModuleStateDriver.java:119)\n    at weblogic.application.internal.flow.ScopedModuleDriver.start(ScopedModuleDriver.java:200)\n    at weblogic.application.internal.flow.ModuleListenerInvoker.start(ModuleListenerInvoker.java:247)\n    at weblogic.application.internal.flow.ModuleStateDriver$3.next(ModuleStateDriver.java:425)\n    at weblogic.application.utils.StateMachineDriver.nextState(StateMachineDriver.java:52)\n    at weblogic.application.internal.flow.ModuleStateDriver.start(ModuleStateDriver.java:119)\n    at weblogic.application.internal.flow.StartModulesFlow.activate(StartModulesFlow.java:27)\n    at weblogic.application.internal.BaseDeployment$2.next(BaseDeployment.java:636)\n    at weblogic.application.utils.StateMachineDriver.nextState(StateMachineDriver.java:52)\n    at weblogic.application.internal.BaseDeployment.activate(BaseDeployment.java:205)\n    at weblogic.application.internal.EarDeployment.activate(EarDeployment.java:58)\n    at weblogic.application.internal.DeploymentStateChecker.activate(DeploymentStateChecker.java:161)\n    at weblogic.deploy.internal.targetserver.AppContainerInvoker.activate(AppContainerInvoker.java:79)\n    at weblogic.deploy.internal.targetserver.operations.AbstractOperation.activate(AbstractOperation.java:569)\n    at weblogic.deploy.internal.targetserver.operations.ActivateOperation.activateDeployment(ActivateOperation.java:150)\n    at weblogic.deploy.internal.targetserver.operations.ActivateOperation.doCommit(ActivateOperation.java:116)\n    at weblogic.deploy.internal.targetserver.operations.StartOperation.doCommit(StartOperation.java:140)\n    at weblogic.deploy.internal.targetserver.operations.AbstractOperation.commit(AbstractOperation.java:323)\n    at weblogic.deploy.internal.targetserver.DeploymentManager.handleDeploymentCommit(DeploymentManager.java:844)\n    at weblogic.deploy.internal.targetserver.DeploymentManager.activateDeploymentList(DeploymentManager.java:1253)\n    at weblogic.deploy.internal.targetserver.DeploymentManager.handleCommit(DeploymentManager.java:440)\n    at weblogic.deploy.internal.targetserver.DeploymentServiceDispatcher.commit(DeploymentServiceDispatcher.java:163)\n    at weblogic.deploy.service.internal.targetserver.DeploymentReceiverCallbackDeliverer.doCommitCallback(DeploymentReceiverCallbackDeliverer.java:195)\n    at weblogic.deploy.service.internal.targetserver.DeploymentReceiverCallbackDeliverer.access$100(DeploymentReceiverCallbackDeliverer.java:13)\n    at weblogic.deploy.service.internal.targetserver.DeploymentReceiverCallbackDeliverer$2.run(DeploymentReceiverCallbackDeliverer.java:68)\n    at weblogic.work.SelfTuningWorkManagerImpl$WorkAdapterImpl.run(SelfTuningWorkManagerImpl.java:528)\n    at weblogic.work.ExecuteThread.execute(ExecuteThread.java:209)\n    at weblogic.work.ExecuteThread.run(ExecuteThread.java:178)\n> \n<Jun 11, 2013 9:28:43 PM EDT> <Error> <HTTP> <BEA-101165> <Could not load user defined filter in web.xml: org.apache.struts2.dispatcher.ng.filter.StrutsPrepareAndExecuteFilter.\nUnable to load configuration. - bean - zip:C:\/Oracle\/FRMiddleware\/user_projects\/domains\/base_domain\/servers\/AdminServer\/tmp\/_WL_user\/_auto_generated_ear_\/injyo5\/war\/WEB-INF\/lib\/struts2-core-2.3.14.jar!\/struts-default.xml:29:72\n    at org.apache.struts2.dispatcher.Dispatcher.init(Dispatcher.java:485)\n    at org.apache.struts2.dispatcher.ng.InitOperations.initDispatcher(InitOperations.java:74)\n    at org.apache.struts2.dispatcher.ng.filter.StrutsPrepareAndExecuteFilter.init(StrutsPrepareAndExecuteFilter.java:51)\n    at weblogic.servlet.internal.FilterManager$FilterInitAction.run(FilterManager.java:332)\n    at weblogic.security.acl.internal.AuthenticatedSubject.doAs(AuthenticatedSubject.java:321)\n    Truncated. see log file for complete stacktrace\nCaused By: Unable to load configuration. - bean - zip:C:\/Oracle\/FRMiddleware\/user_projects\/domains\/base_domain\/servers\/AdminServer\/tmp\/_WL_user\/_auto_generated_ear_\/injyo5\/war\/WEB-INF\/lib\/struts2-core-2.3.14.jar!\/struts-default.xml:29:72\n    at com.opensymphony.xwork2.config.ConfigurationManager.getConfiguration(ConfigurationManager.java:70)\n    at org.apache.struts2.dispatcher.Dispatcher.init_PreloadConfiguration(Dispatcher.java:429)\n    at org.apache.struts2.dispatcher.Dispatcher.init(Dispatcher.java:473)\n    at org.apache.struts2.dispatcher.ng.InitOperations.initDispatcher(InitOperations.java:74)\n    at org.apache.struts2.dispatcher.ng.filter.StrutsPrepareAndExecuteFilter.init(StrutsPrepareAndExecuteFilter.java:51)\n    Truncated. see log file for complete stacktrace\nCaused By: Unable to load bean: type: class:com.opensymphony.xwork2.ObjectFactory - bean - zip:C:\/Oracle\/FRMiddleware\/user_projects\/domains\/base_domain\/servers\/AdminServer\/tmp\/_WL_user\/_auto_generated_ear_\/injyo5\/war\/WEB-INF\/lib\/struts2-core-2.3.14.jar!\/struts-default.xml:29:72\n    at com.opensymphony.xwork2.config.providers.XmlConfigurationProvider.register(XmlConfigurationProvider.java:245)\n    at org.apache.struts2.config.StrutsXmlConfigurationProvider.register(StrutsXmlConfigurationProvider.java:102)\n    at com.opensymphony.xwork2.config.impl.DefaultConfiguration.reloadContainer(DefaultConfiguration.java:226)\n    at com.opensymphony.xwork2.config.ConfigurationManager.getConfiguration(ConfigurationManager.java:67)\n    at org.apache.struts2.dispatcher.Dispatcher.init_PreloadConfiguration(Dispatcher.java:429)\n    Truncated. see log file for complete stacktrace\nCaused By: Bean type class com.opensymphony.xwork2.ObjectFactory with the name xwork has already been loaded by bean - zip:C:\/Users\/brownb\/.m2\/repository\/org\/apache\/struts\/struts2-core\/2.3.14\/struts2-core-2.3.14.jar!\/struts-default.xml:29:72 - bean - zip:C:\/Oracle\/FRMiddleware\/user_projects\/domains\/base_domain\/servers\/AdminServer\/tmp\/_WL_user\/_auto_generated_ear_\/injyo5\/war\/WEB-INF\/lib\/struts2-core-2.3.14.jar!\/struts-default.xml:29:72\n    at com.opensymphony.xwork2.config.providers.XmlConfigurationProvider.register(XmlConfigurationProvider.java:229)\n    at org.apache.struts2.config.StrutsXmlConfigurationProvider.register(StrutsXmlConfigurationProvider.java:102)\n    at com.opensymphony.xwork2.config.impl.DefaultConfiguration.reloadContainer(DefaultConfiguration.java:226)\n    at com.opensymphony.xwork2.config.ConfigurationManager.getConfiguration(ConfigurationManager.java:67)\n    at org.apache.struts2.dispatcher.Dispatcher.init_PreloadConfiguration(Dispatcher.java:429)\n    Truncated. see log file for complete stacktrace\n> \n\nHere is my pom.xml:\n<properties>\n    <project.build.sourceEncoding>UTF-8<\/project.build.sourceEncoding>\n    <spring.version>3.2.3.RELEASE<\/spring.version>\n    <struts2.version>2.3.14<\/struts2.version>\n<\/properties>\n<dependencies>\n\n    <!-- Java Servlet provided jars -->\n    <dependency>\n        <groupId>javax.servlet<\/groupId>\n        <artifactId>servlet-api<\/artifactId>\n        <version>2.5<\/version>\n        <scope>provided<\/scope>\n    <\/dependency>\n    <dependency>\n        <groupId>javax.servlet.jsp<\/groupId>\n        <artifactId>jsp-api<\/artifactId>\n        <version>2.2<\/version>\n        <scope>provided<\/scope>\n    <\/dependency>\n\n    <!-- Oracle -->\n    <dependency>\n        <groupId>oracle<\/groupId>\n        <artifactId>ojdbc6<\/artifactId>\n        <version>1.0<\/version>\n    <\/dependency>\n\n    <!-- Struts 2 -->\n    <dependency>\n        <groupId>org.apache.struts<\/groupId>\n        <artifactId>struts2-convention-plugin<\/artifactId>\n        <version>${struts2.version}<\/version>\n    <\/dependency>\n    <dependency>\n        <groupId>org.apache.struts<\/groupId>\n        <artifactId>struts2-core<\/artifactId>\n        <version>${struts2.version}<\/version>\n    <\/dependency>\n    <dependency>\n        <groupId>org.apache.struts<\/groupId>\n        <artifactId>struts2-spring-plugin<\/artifactId>\n        <version>${struts2.version}<\/version>\n        <exclusions>\n            <exclusion>\n                <groupId>org.springframework<\/groupId>\n                <artifactId>spring-beans<\/artifactId>\n            <\/exclusion>\n            <exclusion>\n                <groupId>org.springframework<\/groupId>\n                <artifactId>spring-context<\/artifactId>\n            <\/exclusion>\n            <exclusion>\n                <groupId>org.springframework<\/groupId>\n                <artifactId>spring-core<\/artifactId>\n            <\/exclusion>\n            <exclusion>\n                <groupId>org.springframework<\/groupId>\n                <artifactId>spring-web<\/artifactId>\n            <\/exclusion>\n        <\/exclusions>\n    <\/dependency>\n\n    <!-- Hibernate Entity Manager -->\n    <dependency>\n        <groupId>org.hibernate<\/groupId>\n        <artifactId>hibernate-entitymanager<\/artifactId>\n        <version>4.2.2.Final<\/version>\n    <\/dependency>\n\n    <!-- Spring 3 -->\n    <dependency>\n        <groupId>org.springframework<\/groupId>\n        <artifactId>spring-beans<\/artifactId>\n        <version>${spring.version}<\/version>\n    <\/dependency>\n    <dependency>\n        <groupId>org.springframework<\/groupId>\n        <artifactId>spring-context<\/artifactId>\n        <version>${spring.version}<\/version>\n    <\/dependency>\n    <dependency>\n        <groupId>org.springframework<\/groupId>\n        <artifactId>spring-core<\/artifactId>\n        <version>${spring.version}<\/version>\n    <\/dependency>\n    <dependency>\n        <groupId>org.springframework<\/groupId>\n        <artifactId>spring-expression<\/artifactId>\n        <version>${spring.version}<\/version>\n    <\/dependency>\n    <dependency>\n        <groupId>org.springframework<\/groupId>\n        <artifactId>spring-jdbc<\/artifactId>\n        <version>${spring.version}<\/version>\n    <\/dependency>\n    <dependency>\n        <groupId>org.springframework<\/groupId>\n        <artifactId>spring-orm<\/artifactId>\n        <version>${spring.version}<\/version>\n    <\/dependency>\n    <dependency>\n        <groupId>org.springframework<\/groupId>\n        <artifactId>spring-test<\/artifactId>\n        <version>${spring.version}<\/version>\n    <\/dependency>\n    <dependency>\n        <groupId>org.springframework<\/groupId>\n        <artifactId>spring-tx<\/artifactId>\n        <version>${spring.version}<\/version>\n    <\/dependency>\n    <dependency>\n        <groupId>org.springframework<\/groupId>\n        <artifactId>spring-web<\/artifactId>\n        <version>${spring.version}<\/version>\n    <\/dependency>\n<\/dependencies>\n\nHere is my struts.xml\n\n<!-- Struts Constants that are overridden -->\n<constant name=\"struts.devMode\" value=\"false\" \/>\n<constant name=\"struts.objectFactory\"\n    value=\"org.apache.struts2.spring.StrutsSpringObjectFactory\" \/>\n<constant name=\"struts.convention.default.parent.package\"\n    value=\"com.ceic\" \/>\n\n<!-- Needed for Weblogic configuration -->\n<constant name=\"struts.convention.action.includeJars\" value=\".*?\/eServices.*?jar(!\/)?\" \/>\n<constant name=\"struts.convention.action.fileProtocols\" value=\"jar,zip\" \/>\n<!-- End Weblogic configuration -->\n\n<!-- Package name should default to the module name -->\n<package name=\"user\" namespace=\"\/User\" extends=\"struts-default\">\n    <action name=\"Login\">\n        <result>pages\/login.jsp<\/result>\n    <\/action>\n    <action name=\"Welcome\" class=\"com.mkyong.user.action.WelcomeUserAction\">\n        <result name=\"SUCCESS\">pages\/welcome_user.jsp<\/result>\n    <\/action>\n<\/package>\n\n\nAny ideas on what I'm doing wrong here?  I can deploy it manually (without an error), but I still get an error when I try to run it.\nThanks in advance for the help! What I ending up doing was adding the struts2 jars to the lib folder instead of letting Maven manage them and modified the weblogic.xml to contain the following:\n<wls:container-descriptor>\n    <wls:prefer-web-inf-classes>true<\/wls:prefer-web-inf-classes>\n<\/wls:container-descriptor> \n\nOnce I made these changes, the program ran without any problems!\nI hope this helps someone!\n","meta":{"link":"https:\/\/stackoverflow.com\/questions\/17056514\/issue-when-trying-to-deploy-a-struts2-spring-3-web-application-to-weblogic-10-3"},"label":[[308,369,"problem"],[4936,4955,"problem"],[9914,10019,"problem"],[23620,23876,"resolution"]],"Comments":[]}
{"id":364,"text":"Get latest from Visual Studio Team Services using Command Line passing \/login credentials with TF.exe. Has anyone had success getting latest source code from the Visual Studio Team Services (formerly Visual Studio Online, Team Foundation Service) Version Control Server using the command line and passing in credentials programmatically? \n-I have discovered that you can't use the Windows ID credentials that you use to login to Team Explorer or the VSO website in the command line.  You need to create Alternate Credentials for the user profile in Team Services.\n-I have found out that if you omit the \/login in tf.exe, the Team Services login dialog appears and asks you to type in your Windows ID credentials (unless they are already cached in your Team Explorer or Visual Studio (or even possibly Browser and Windows Credential Caches) \n-I have found out that the alternate credential work Using the Java version of tf.exe - Team Explorer Everywhere Command Line Client (TEE CLC).  TEE CLC actually uses the \/login credentials that you pass in and lets you connect.  The same thing does NOT seem to be possible with the TF.EXE in C:\\Program Files (x86)\\Microsoft Visual Studio 12.0\\Common7\\IDE\\  BUT INSTALLING JAVA ON THIS BUILD ENVIRONMENT IS AGAINST POLICY. So the TEE CLC is NOT a viable option.\ntf get $\/MyProj \/collection:https:\/\/myaccount.visualstudio.com\/DefaultCollection \/login:user:pass \n\nthe above command simply ignores the \/login credentials if you have the Windows ID credentials cached or it returns the error message TF30063: You are not authorized to access myaccount.visualstudio.com (which is not true, because the credentials DO work with the Java client)\nAre there any other alternatives that do not require installing Java? I got an answer from Microsoft Support: AA Creds for VSO do not work with TF.EXE at this time. TEE CLC or using object model code are the only alternatives currently. We are looking at doing this in the future.\nObject Model Code refers to the Microsoft.TeamFoundation.VersionControl.Client Namespace in the dll by the same name.  I ended up writing a quick C# console app to download the latest code without installing Java. An added benefit of this approach is that it does not require creating a throwaway Workspace.\nif you use the code below to create an executable called tfsget.exe it can be called from the command line like this:\ntfsget https:\/\/myvso.visualstudio.com\/DefaultCollection $\/MyProj\/Folder c:\\Projects login password\n\nand I added a silent switch to suppress listing each file that can be used like:\ntfsget https:\/\/myvso.visualstudio.com\/DefaultCollection $\/MyProj\/Folder c:\\Projects login password silent\n\nhere's the code, hope this helps until MS updates TF.exe \nusing System;\nusing System.Collections.Generic;\nusing System.IO;\nusing System.Net;\nusing Microsoft.TeamFoundation.Client;\nusing Microsoft.TeamFoundation.VersionControl.Client;\n\nnamespace TfsGet\n{\n    class Program\n    {\n        static void Main(string[] args)\n        {\n            var tfsParams = TfsDownloadParams.Create(args);\n\n            var tpc = new TfsTeamProjectCollection(new Uri(tfsParams.ServerUrl), tfsParams.Credentials);\n\n            CheckAccess(tpc, tfsParams);\n\n            Download(tpc, tfsParams);\n\n        }\n\n        private static void CheckAccess(TfsTeamProjectCollection tpc, TfsDownloadParams tfsParams)\n        {\n            try\n            {\n                tpc.Authenticate();\n            }\n            catch\n            {\n                Console.WriteLine(\"TFS Authentication Failed\");\n                Console.WriteLine(\"Server Url:{0}\", tfsParams.ServerUrl);\n                Console.WriteLine(\"Project Path:{0}\", tfsParams.ServerProjectPath);\n                Console.WriteLine(\"Target Path:{0}\", tfsParams.TargetPath);\n                Environment.Exit(1);\n            }\n        }\n\n        static void Download(TfsTeamProjectCollection tpc, TfsDownloadParams tfsParams)\n        {   \n            var versionControl = tpc.GetService<VersionControlServer>();\n            \/\/ Listen for the Source Control events.\n            versionControl.NonFatalError += Program.OnNonFatalError;\n\n            var files = versionControl.GetItems(tfsParams.ServerProjectPath, VersionSpec.Latest, RecursionType.Full);\n            foreach (Item item in files.Items)\n            {\n                var localFilePath = GetLocalFilePath(tfsParams, item);\n\n                switch (item.ItemType)\n                {\n                    case ItemType.Any:\n                        throw new ArgumentOutOfRangeException(\"ItemType.Any - not sure what to do with this\");\n                    case ItemType.File:\n                        if (!tfsParams.Silent) Console.WriteLine(\"Getting: '{0}'\", localFilePath);\n                        item.DownloadFile(localFilePath);\n                        break;\n                    case ItemType.Folder:\n                        if (!tfsParams.Silent) Console.WriteLine(\"Creating Directory: {0}\", localFilePath);\n                        Directory.CreateDirectory(localFilePath);\n                        break;\n                }\n            }\n        }\n\n        private static string GetLocalFilePath(TfsDownloadParams tfsParams, Item item)\n        {\n            var projectPath = tfsParams.ServerProjectPath;\n            var pathExcludingLastFolder = projectPath.Substring(0, projectPath.LastIndexOf('\/')+1);\n            string relativePath = item.ServerItem.Replace(pathExcludingLastFolder, \"\");\n            var localFilePath = Path.Combine(tfsParams.TargetPath, relativePath);\n            return localFilePath;\n        }\n\n        internal static void OnNonFatalError(Object sender, ExceptionEventArgs e)\n        {\n            var message = e.Exception != null ? e.Exception.Message : e.Failure.Message;\n            Console.Error.WriteLine(\"Exception: \" + message);\n        }\n    }\n\n    public class TfsDownloadParams\n    {\n        public string ServerUrl { get; set; }\n        public string ServerProjectPath { get; set; }\n        public string TargetPath { get; set; }\n        public TfsClientCredentials Credentials { get; set; }\n        public bool Silent { get; set; }\n\n        public static TfsDownloadParams Create(IList<string> args)\n        {\n            if (args.Count < 5)\n            {\n                Console.WriteLine(\"Please supply 5 or 6 parameters: tfsServerUrl serverProjectPath targetPath userName password [silent]\");\n                Console.WriteLine(\"The optional 6th 'silent' parameter will suppress listing each file downloaded\");\n                Console.WriteLine(@\"Ex: tfsget \"\"https:\/\/myvso.visualstudio.com\/DefaultCollection\"\" \"\"$\/MyProject\/ProjectSubfolder\"\" \"\"c:\\Projects Folder\"\", user, password \");\n\n                Environment.Exit(1);\n            }\n\n            var tfsServerUrl = args[0]; \/\/\"https:\/\/myvso.visualstudio.com\/DefaultCollection\";\n            var serverProjectPath = args[1]; \/\/ \"$\/MyProject\/Folder Path\";\n            var targetPath = args[2]; \/\/ @\"c:\\Projects\\\";\n            var userName = args[3]; \/\/\"login\";\n            var password = args[4]; \/\/\"passsword\";\n            var silentFlag = args.Count >= 6 && (args[5].ToLower() == \"silent\"); \/\/\"silent\";\n            var tfsCredentials = GetTfsCredentials(userName, password);\n\n            var tfsParams = new TfsDownloadParams\n            {\n                ServerUrl = tfsServerUrl,\n                ServerProjectPath = serverProjectPath,\n                TargetPath = targetPath,\n                Credentials = tfsCredentials,\n                Silent = silentFlag,\n            };\n            return tfsParams;\n        }\n\n        private static TfsClientCredentials GetTfsCredentials(string userName, string password)\n        {\n            var networkCreds= new NetworkCredential(userName, password);\n            var basicCreds = new BasicAuthCredential(networkCreds);\n            var tfsCreds = new TfsClientCredentials(basicCreds)\n            {\n                AllowInteractive = false\n            };\n            return tfsCreds;\n        }\n    }\n}\n\n","meta":{"link":"https:\/\/stackoverflow.com\/questions\/21005274\/get-latest-from-visual-studio-team-services-using-command-line-passing-login-cr"},"label":[[1404,1606,"problem"],[1791,1831,"problem"],[2092,2174,"resolution"]],"Comments":[]}
{"id":365,"text":"How do I trigger release pipeline with continuous deployment?. The title is not the best, I agree, please read more details to understand what I mean...\nI have a project\/repository which has following properties:\n\nCommit messages are following Conventional Commits\nResult of above: there is not single place in the repository which has version number. Versions are calculated automatically based on commits history.\nHistory of past \"releases\" is basically git tags history (when the release is done and 1.2.3 version appears, corresponding commit is being tagged)\nCI is \"simple\"\/integrated with Github, every merge\/push to master triggers full pipeline including deploy\/release part (which produces new version, pushes a tag, \"makes release\" in short)\nI have no ways (or, I think I have no ways, or I do not want) to trigger the CI manually for release from CI itself. Every trigger should come from git\/github.\n\nWhile this works quite fine in general, my problem is that usually the repo is constant and receives no real new commits, however in some active days I might have 2-3-4-10 new pull requests to be merged.\nWith the current approach, 10 merged pull requests would result in 10 releases with 10 version increases. But I would prefer to first merge all 10 and then have one release and one version increase.\nAny advice, how can I achieve that, what would be the state-of-the-art recommendation here?\nOne of my thoughts was to have some pre-release or develop branch where I can merge the PRs first and only when it's time to release, merge develop to master. But it looks a bit cumbersome and not really making life easier for contributors (they need to target develop, then I need to create PR from develop to master etc...)\nUpdate:\n\nI believe, the answer should be CI-independent. It does not matter which exact CI system is here, what's important is that I don't want to go to CI to trigger the jobs, all interactions should happen via repository. Webhooks\/connection between repo and CI is of course available and working.\nOf course, having said \"simple\" CI I didn't mean that its configuration is fixed. Of course I can update it in any way I want, no problems with that (trigger on different events from repository, on different branches etc) There are multiple ways to accomplish what you desire.\nOne idea you already mentioned yourself:\n\nOne of my thoughts was to have some pre-release or develop branch where I can merge the PRs first and only when it's time to release, merge develop to master.\n\nThis is the approach that by far takes the least effort.\nEverything else would require you to change the pipeline.\nIf you are okay with that here are some ideas that are not hard to implement given your current structure:\n\nChange the pipeline to not release on every change in the master branch but on every change in the release branch. By that all developers will still be able to target master and you can then squash all the commits and introduce them to release. This approach is simple but depending on the project can lead to issues and\/or confusion regarding the git structure.\nChange the pipeline to not release on every change in the master branch but on every new tag that is pushed. By that you can gather all the commits in the master branch and once you decide it is ready to be release simply push a git tag manually. If you want to go another step further configure the pipeline to automatically create a git tag only if there is a signed commit message by you (or other trusted maintaners) that includes something like version bump to vX.Y.Z. This approach is a more modern one and offers way more flexibility but requires more change in the pipeline than the previous one (keep in mind that changing the pipeline is a one-time effort).\n\n","meta":{"link":"https:\/\/stackoverflow.com\/questions\/69583694\/how-do-i-trigger-release-pipeline-with-continuous-deployment"},"label":[[1144,1221,"problem"],[1434,1566,"resolution"],[2737,2851,"resolution"],[3100,3208,"resolution"]],"Comments":[]}
{"id":366,"text":"PHPUnit not continuing with tests after fatal error when using --process-isolation. I have a PHPUnit test suite that is currently causing a fatal error due to a class definition that's not being found. This, ultimately, is a failure of the testing code itself and a failure by the developer to vindicate the test itself before committing code. \nHowever, things like this do happen from time to time, and it would be wonderful if, when a fatal error occurs (regardless of who's ultimately responsible), the test simply be marked as a failure, and the remainder of the test suite still be executed.\nI have read about the --process-isolation switch and as far as I can tell, it should take care of this. Since each test runs in a separate process, if the child dies due to a fatal error, the parent can still continue to run. In fact, this is stated explicitly in this answer to a similar question: https:\/\/stackoverflow.com\/a\/5340151\/84762 which shows the exact type of output I would like to see myself.\nHowever, I seem to get the exact same output regardless of whether or not I use the --process-isolation flag:\nWITHOUT process isolation\n[kogi@phagocyte ~]$ \/usr\/bin\/phpunit --colors --verbose --coverage-html \"target\/coverage\" ~\/app\/zend\/tests\/application\/\n\nPHP Fatal error:  Class 'Rmd_Database_OldObject' not found in \/home\/kogi\/app\/zend\/private\/models\/translate\/Poll.php on line 9\nPHP Stack trace:\nPHP   1. {main}() \/usr\/bin\/phpunit:0\nPHP   2. PHPUnit_TextUI_Command::main() \/usr\/bin\/phpunit:46\nPHP   3. PHPUnit_TextUI_Command->run() \/usr\/share\/pear\/PHPUnit\/TextUI\/Command.php:130\nPHP   4. PHPUnit_Runner_BaseTestRunner->getTest() \/usr\/share\/pear\/PHPUnit\/TextUI\/Command.php:150\nPHP   5. PHPUnit_Framework_TestSuite->addTestFiles() \/usr\/share\/pear\/PHPUnit\/Runner\/BaseTestRunner.php:96\nPHP   6. PHPUnit_Framework_TestSuite->addTestFile() \/usr\/share\/pear\/PHPUnit\/Framework\/TestSuite.php:419\nPHP   7. PHPUnit_Util_Fileloader::checkAndLoad() \/usr\/share\/pear\/PHPUnit\/Framework\/TestSuite.php:358\nPHP   8. PHPUnit_Util_Fileloader::load() \/usr\/share\/pear\/PHPUnit\/Util\/Fileloader.php:79\nPHP   9. include_once() \/usr\/share\/pear\/PHPUnit\/Util\/Fileloader.php:95\nPHP  10. require_once() \/home\/kogi\/app\/zend\/tests\/application\/translate\/PollTest.php:11\n\nFatal error: Class 'Rmd_Database_OldObject' not found in \/home\/kogi\/app\/zend\/private\/models\/translate\/Poll.php on line 9\n\nCall Stack:\n    0.0003      91584   1. {main}() \/usr\/bin\/phpunit:0\n    0.0076     612672   2. PHPUnit_TextUI_Command::main() \/usr\/bin\/phpunit:46\n    0.0076     613744   3. PHPUnit_TextUI_Command->run() \/usr\/share\/pear\/PHPUnit\/TextUI\/Command.php:130\n    0.0246    1249464   4. PHPUnit_Runner_BaseTestRunner->getTest() \/usr\/share\/pear\/PHPUnit\/TextUI\/Command.php:150\n    0.0706    1626680   5. PHPUnit_Framework_TestSuite->addTestFiles() \/usr\/share\/pear\/PHPUnit\/Runner\/BaseTestRunner.php:96\n    0.1691    8053584   6. PHPUnit_Framework_TestSuite->addTestFile() \/usr\/share\/pear\/PHPUnit\/Framework\/TestSuite.php:419\n    0.1693    8057320   7. PHPUnit_Util_Fileloader::checkAndLoad() \/usr\/share\/pear\/PHPUnit\/Framework\/TestSuite.php:358\n    0.1694    8057664   8. PHPUnit_Util_Fileloader::load() \/usr\/share\/pear\/PHPUnit\/Util\/Fileloader.php:79\n    0.1711    8240600   9. include_once('\/home\/kogi\/app\/zend\/tests\/application\/translate\/PollTest.php') \/usr\/share\/pear\/PHPUnit\/Util\/Fileloader.php:95\n    0.1805    9187768  10. require_once('\/home\/kogi\/app\/zend\/private\/models\/translate\/Poll.php') \/home\/kogi\/app\/zend\/tests\/application\/translate\/PollTest.php:11\n\nWITH process isolation\n[kogi@phagocyte ~]$ \/usr\/bin\/phpunit --colors --verbose --coverage-html \"target\/coverage\" --process-isolation ~\/app\/zend\/tests\/application\/\nPHP Fatal error:  Class 'Rmd_Database_OldObject' not found in \/home\/kogi\/app\/zend\/private\/models\/translate\/Poll.php on line 9\nPHP Stack trace:\nPHP   1. {main}() \/usr\/bin\/phpunit:0\nPHP   2. PHPUnit_TextUI_Command::main() \/usr\/bin\/phpunit:46\nPHP   3. PHPUnit_TextUI_Command->run() \/usr\/share\/pear\/PHPUnit\/TextUI\/Command.php:130\nPHP   4. PHPUnit_Runner_BaseTestRunner->getTest() \/usr\/share\/pear\/PHPUnit\/TextUI\/Command.php:150\nPHP   5. PHPUnit_Framework_TestSuite->addTestFiles() \/usr\/share\/pear\/PHPUnit\/Runner\/BaseTestRunner.php:96\nPHP   6. PHPUnit_Framework_TestSuite->addTestFile() \/usr\/share\/pear\/PHPUnit\/Framework\/TestSuite.php:419\nPHP   7. PHPUnit_Util_Fileloader::checkAndLoad() \/usr\/share\/pear\/PHPUnit\/Framework\/TestSuite.php:358\nPHP   8. PHPUnit_Util_Fileloader::load() \/usr\/share\/pear\/PHPUnit\/Util\/Fileloader.php:79\nPHP   9. include_once() \/usr\/share\/pear\/PHPUnit\/Util\/Fileloader.php:95\nPHP  10. require_once() \/home\/kogi\/app\/zend\/tests\/application\/translate\/PollTest.php:11\n\nFatal error: Class 'Rmd_Database_OldObject' not found in \/home\/kogi\/app\/zend\/private\/models\/translate\/Poll.php on line 9\n\nCall Stack:\n    0.0003      91752   1. {main}() \/usr\/bin\/phpunit:0\n    0.0076     612824   2. PHPUnit_TextUI_Command::main() \/usr\/bin\/phpunit:46\n    0.0076     613896   3. PHPUnit_TextUI_Command->run() \/usr\/share\/pear\/PHPUnit\/TextUI\/Command.php:130\n    0.0246    1250360   4. PHPUnit_Runner_BaseTestRunner->getTest() \/usr\/share\/pear\/PHPUnit\/TextUI\/Command.php:150\n    0.0708    1627528   5. PHPUnit_Framework_TestSuite->addTestFiles() \/usr\/share\/pear\/PHPUnit\/Runner\/BaseTestRunner.php:96\n    0.1688    8054296   6. PHPUnit_Framework_TestSuite->addTestFile() \/usr\/share\/pear\/PHPUnit\/Framework\/TestSuite.php:419\n    0.1690    8057992   7. PHPUnit_Util_Fileloader::checkAndLoad() \/usr\/share\/pear\/PHPUnit\/Framework\/TestSuite.php:358\n    0.1691    8058336   8. PHPUnit_Util_Fileloader::load() \/usr\/share\/pear\/PHPUnit\/Util\/Fileloader.php:79\n    0.1707    8241296   9. include_once('\/home\/kogi\/app\/zend\/tests\/application\/translate\/PollTest.php') \/usr\/share\/pear\/PHPUnit\/Util\/Fileloader.php:95\n    0.1801    9188464  10. require_once('\/home\/kogi\/app\/zend\/private\/models\/translate\/Poll.php') \/home\/kogi\/app\/zend\/tests\/application\/translate\/PollTest.php:11\n\nFor those of us that can't effectively diff in our heads, the two outputs are literally identical (other than the execution time and memory usage which is negligibly different).\nIn both cases, the fatal error kills the entire test suite. In this particular case, this happens in the 3rd test and the remaining 150 tests (in several other files\/suites) are never executed.\nWhat am I doing wrong here? Is there some other way to survive a fatal error (marking the test as failed) in one test and still execute remaining tests?\n\nEDIT\nI am using PHPUnit 3.6.10\nEDIT\nComments on the answer to this question have inspired a new ticket on PHPUnit's GitHub page: https:\/\/github.com\/sebastianbergmann\/phpunit\/issues\/545 PHPUnit loads each test file that will be run before running any tests. This causes PHP to parse these files and execute their top-level code. If any class is loaded that, for example, extends a class that doesn't exist, you'll get a fatal error.\nI don't see any way around this without enhancing PHPUnit to parse the files without executing their code during the scanning process.\n","meta":{"link":"https:\/\/stackoverflow.com\/questions\/10077400\/phpunit-not-continuing-with-tests-after-fatal-error-when-using-process-isolati"},"label":[[0,82,"problem"],[6638,6884,"problem"]],"Comments":[]}
{"id":367,"text":"Chrome Extension and Jenkins URL's. I'm currently trying to develop a chrome extension which is supposed to display data from different Jenkins servers. The url to the jenkins server is being entered by the user.\nSo basically what I need is being able to access any kind of jenkins url.\nMy problem is that Chrome's Content Security Policy\nonly allows you to access domains which you've registered in the manifest.json like so:\n\"content_security_policy\": \"script-src 'self' http:\/\/localhost:8080\/; object-src 'self'\".\nBut since different users are going to enter different urls, I'd need to be able to change this policy dynamically, and I don't really know how to do that. You seem have a misconception as to what is needed to send a request to a random domain.\nCSP directive script-src dictates where you can execute code from specifically by <script src=\"https:\/\/some.domain\/script.js\">. That, indeed, is something you can't whitelist.\nTo send GET\/POST cross-site requests, you need a host permission, not a CSP modification.\nThere are 2 possible approaches:\n\nGive yourself wide permissions, e.g. \"*:\/\/*\/*\" or \"<all_urls>\". This will allow you to query anything at the cost of install-time warning \"can read and modify data on all websites\".\nUse the optional permissions API. That way, you can avoid the install-time warning and prompt for elevated permissions as the extension is configured. This would make sense if this configuration needs to be done infrequently.\nExtra note: as of 2016-10-04, permissions API is not implemented in Firefox WebExtensions \/ Edge Extensions, so if portability is a concern, it's best avoided for new projects for now.\n\n","meta":{"link":"https:\/\/stackoverflow.com\/questions\/28816374\/chrome-extension-and-jenkins-urls"},"label":[[306,418,"problem"],[1062,1126,"resolution"],[1244,1277,"resolution"]],"Comments":[]}
{"id":368,"text":"Error code H10 while Deploying Django code to Heroku. I'm having a lot of trouble in deploying my Django app to Heroku. I'm new to both Django and Heroku, and I'm not able to understand what the traceback is telling me.\nI keep getting the error code \"h10\" in heroku logs traceback. I believe gunicorn isn't starting properly, but I don't know the solution. I've also searched this issue for hours, and rewriting the Procfile, reinstalling gunicorn, virtualenv, django-heroku, etc hasn't worked for me.\nI'm not able to see what my error is. When I run heroku logs --tail i see:\n(--sliced to view gunicorn issue--)\n2020-08-11T15:30:05.149176+00:00 app[web.1]: [2020-08-11 15:30:05 +0000] [4] [INFO] Starting gunicorn 20.0.4\n2020-08-11T15:30:05.149958+00:00 app[web.1]: [2020-08-11 15:30:05 +0000] [4] [DEBUG] Arbiter booted\n2020-08-11T15:30:05.150076+00:00 app[web.1]: [2020-08-11 15:30:05 +0000] [4] [INFO] Listening at: http:\/\/0.0.0.0:46142 (4)\n2020-08-11T15:30:05.150185+00:00 app[web.1]: [2020-08-11 15:30:05 +0000] [4] [INFO] Using worker: sync\n2020-08-11T15:30:05.155860+00:00 app[web.1]: [2020-08-11 15:30:05 +0000] [10] [INFO] Booting worker with pid: 10\n2020-08-11T15:30:05.164169+00:00 app[web.1]: [2020-08-11 15:30:05 +0000] [10] [ERROR] Exception in worker process\n2020-08-11T15:30:05.164171+00:00 app[web.1]: Traceback (most recent call last):\n2020-08-11T15:30:05.164172+00:00 app[web.1]: File \"\/app\/.heroku\/python\/lib\/python3.6\/site-packages\/gunicorn\/arbiter.py\", line 583, in spawn_worker\n2020-08-11T15:30:05.164172+00:00 app[web.1]: worker.init_process()\n2020-08-11T15:30:05.164173+00:00 app[web.1]: File \"\/app\/.heroku\/python\/lib\/python3.6\/site-packages\/gunicorn\/workers\/base.py\", line 119, in init_process\n2020-08-11T15:30:05.164174+00:00 app[web.1]: self.load_wsgi()\n2020-08-11T15:30:05.164174+00:00 app[web.1]: File \"\/app\/.heroku\/python\/lib\/python3.6\/site-packages\/gunicorn\/workers\/base.py\", line 144, in load_wsgi\n2020-08-11T15:30:05.164174+00:00 app[web.1]: self.wsgi = self.app.wsgi()\n2020-08-11T15:30:05.164175+00:00 app[web.1]: File \"\/app\/.heroku\/python\/lib\/python3.6\/site-packages\/gunicorn\/app\/base.py\", line 67, in wsgi\n2020-08-11T15:30:05.164175+00:00 app[web.1]: self.callable = self.load()\n2020-08-11T15:30:05.164176+00:00 app[web.1]: File \"\/app\/.heroku\/python\/lib\/python3.6\/site-packages\/gunicorn\/app\/wsgiapp.py\", line 49, in load\n2020-08-11T15:30:05.164176+00:00 app[web.1]: return self.load_wsgiapp()\n2020-08-11T15:30:05.164177+00:00 app[web.1]: File \"\/app\/.heroku\/python\/lib\/python3.6\/site-packages\/gunicorn\/app\/wsgiapp.py\", line 39, in load_wsgiapp\n2020-08-11T15:30:05.164177+00:00 app[web.1]: return util.import_app(self.app_uri)\n2020-08-11T15:30:05.164178+00:00 app[web.1]: File \"\/app\/.heroku\/python\/lib\/python3.6\/site-packages\/gunicorn\/util.py\", line 358, in import_app\n2020-08-11T15:30:05.164178+00:00 app[web.1]: mod = importlib.import_module(module)\n2020-08-11T15:30:05.164179+00:00 app[web.1]: File \"\/app\/.heroku\/python\/lib\/python3.6\/importlib\/__init__.py\", line 126, in import_module\n2020-08-11T15:30:05.164179+00:00 app[web.1]: return _bootstrap._gcd_import(name[level:], package, level)\n2020-08-11T15:30:05.164180+00:00 app[web.1]: File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\n2020-08-11T15:30:05.164180+00:00 app[web.1]: File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\n2020-08-11T15:30:05.164180+00:00 app[web.1]: File \"<frozen importlib._bootstrap>\", line 953, in _find_and_load_unlocked\n2020-08-11T15:30:05.164186+00:00 app[web.1]: ModuleNotFoundError: No module named 'scholarstarterapp.wsgi'\n2020-08-11T15:30:05.164438+00:00 app[web.1]: [2020-08-11 15:30:05 +0000] [10] [INFO] Worker exiting (pid: 10)\n2020-08-11T15:30:05.212254+00:00 app[web.1]: [2020-08-11 15:30:05 +0000] [4] [INFO] Shutting down: Master\n2020-08-11T15:30:05.212403+00:00 app[web.1]: [2020-08-11 15:30:05 +0000] [4] [INFO] Reason: Worker failed to boot.\n2020-08-11T15:30:05.325066+00:00 heroku[web.1]: Process exited with status 3\n2020-08-11T15:30:05.382253+00:00 heroku[web.1]: State changed from starting to crashed\n2020-08-11T15:32:16.258158+00:00 heroku[router]: at=error code=H10 desc=\"App crashed\" method=GET path=\"\/\" host=scholarstarterapp.herokuapp.com request_id=585ca89d-e578-49d1-ad4c-3626ed289e76 fwd=\"68.227.124.46\" dyno= connect= service= status=503 bytes= protocol=https\n2020-08-11T15:32:16.392639+00:00 heroku[router]: at=error code=H10 desc=\"App crashed\" method=GET path=\"\/favicon.ico\" host=scholarstarterapp.herokuapp.com request_id=6409d9cb-3263-450a-a458-700fb6cb012b fwd=\"68.227.124.46\" dyno= connect= service= status=503 bytes= protocol=https\n\nmy settings.py looks like:\nimport logging\nimport os\nimport django_heroku\n\nBASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n\n\n# Quick-start development settings - unsuitable for production\n# See https:\/\/docs.djangoproject.com\/en\/2.2\/howto\/deployment\/checklist\/\n\nSECRET_KEY = os.environ.get('SECRET_KEY')\n\n# SECURITY WARNING: don't run with debug turned on in production!\nDEBUG = True\n\nSITE_ID=1\nif DEBUG:\n    # will output to your console\n    logging.basicConfig(\n        level = logging.DEBUG,\n        format = '%(asctime)s %(levelname)s %(message)s',\n    )\nelse:\n    # will output to logging file\n    logging.basicConfig(\n        level = logging.DEBUG,\n        format = '%(asctime)s %(levelname)s %(message)s',\n        filename = '\/logfile.log',\n        filemode = 'a'\n    )\n\nALLOWED_HOSTS = [ 'https:\/\/scholarstarterapp.herokuapp.com\/' ]\n\nif DEBUG:\n    EMAIL_BACKEND = 'django.core.mail.backends.console.EmailBackend' #during development - update on deployment\n\n# Application definition\n\nINSTALLED_APPS = [\n    'django.contrib.admin',\n    'django.contrib.auth',\n    'django.contrib.contenttypes',\n    'django.contrib.sessions',\n    'django.contrib.messages',\n    'django.contrib.staticfiles',\n    'django.contrib.sites',\n    'django.contrib.flatpages',\n   # 'regwall',\n\n    # 3rd party\n    'allauth',\n    'allauth.account',\n    'allauth.socialaccount',\n    'debreach',\n    'ckeditor',\n    'ckeditor_uploader',\n\n    # authentication\n    'allauth.socialaccount.providers.google',\n    'allauth.socialaccount.providers.facebook',\n    \n     #----------(I've cut out my local app configs here)---\n    'formtools',\n    'send',\n    'crispy_forms',\n    'bootstrap4',\n    'djstripe',\n    'bootstrap_modal_forms',\n]\n\nMIDDLEWARE = [\n    \n\n    'whitenoise.middleware.WhiteNoiseMiddleware',\n    'django.contrib.flatpages.middleware.FlatpageFallbackMiddleware',\n    'django.middleware.clickjacking.XFrameOptionsMiddleware',\n    'django.middleware.gzip.GZipMiddleware',\n    'debreach.middleware.RandomCommentMiddleware',\n    'django.middleware.common.CommonMiddleware',\n    'django.middleware.csrf.CsrfViewMiddleware',\n    'django.middleware.security.SecurityMiddleware',\n    'django.contrib.sessions.middleware.SessionMiddleware',\n    'django.contrib.auth.middleware.AuthenticationMiddleware',\n    'django.contrib.messages.middleware.MessageMiddleware',\n]\n\nROOT_URLCONF = 'scholarstarterapp.urls'\n\nTEMPLATES = [\n    {\n        'BACKEND': 'django.template.backends.django.DjangoTemplates',\n        'DIRS': [os.path.join(BASE_DIR, 'templates')],\n        'APP_DIRS': True,\n        'OPTIONS': {\n            'context_processors': [\n                'django.template.context_processors.debug',\n                'django.template.context_processors.request',\n                'django.contrib.auth.context_processors.auth',\n                'django.contrib.messages.context_processors.messages'\n            ],\n        },\n    },\n]\n\nWSGI_APPLICATION = 'scholarstarterapp.wsgi.application'\n\n\n# Database\n# https:\/\/docs.djangoproject.com\/en\/2.2\/ref\/settings\/#databases\n\n#DATABASES = {\n#    'default': {\n#        'ENGINE': 'django.db.backends.postgresql',\n#        'NAME': 'buoeirnx',\n#        'USER': 'buoeirnx',\n#        'PASSWORD': 'lWdGUjsieyc5LocCEiZispUMIoLk5yzV',\n#        'HOST':'ruby.db.elephantsql.com',\n#        'PORT': '5432'\n#    }\n#}\n\nDATABASES = {\n    'default': {\n        'ENGINE': 'django.db.backends.sqlite3',\n        'NAME': os.path.join(BASE_DIR, 'db.sqlite3'),\n    }\n}\n\nPROJECT_DIR = os.path.dirname(__file__)\n\nMEDIA_ROOT = os.path.join(PROJECT_DIR, 'media')\n\nMEDIA_URL = '\/media\/'\nCKEDITOR_UPLOAD_PATH = \"uploads\/\"\nCKEDITOR_IMAGE_BACKEND=\"pillow\"\n\nCKEDITOR_CONFIGS = {\n    'article_config': {\n        'toolbar': 'Special',\n        'toolbar_Special': [\n            {'name': 'styles', 'items': ['Styles', 'Format', 'Font', 'FontSize', '-', 'Maximize', 'ShowBlocks']},\n            '\/',\n            {'name': 'basicstyles',\n             'items': ['Bold', 'Italic', 'Underline', 'Strike', 'Subscript', 'Superscript', '-', 'RemoveFormat']},\n            {'name': 'clipboard', 'items': ['Cut', 'Copy', 'Paste', 'PasteText', 'PasteFromWord', '-', 'Undo', 'Redo']},\n            {'name': 'paragraph',\n             'items': ['NumberedList', 'BulletedList', '-', 'Outdent', 'Indent', '-', 'Blockquote', 'CreateDiv', '-',\n                       'JustifyLeft', 'JustifyCenter', 'JustifyRight', 'JustifyBlock']},\n            {'name': 'links', 'items': ['Link', 'Unlink']},\n            {'name': 'insert',\n             'items': ['Image', 'Table', 'HorizontalRule', 'Iframe', 'Youtube']},\n        ],\n        'extraPlugins': ','.join(['youtube'])\n    }\n}\n\n# Static files (CSS, JavaScript, Images)\n# https:\/\/docs.djangoproject.com\/en\/1.9\/howto\/static-files\/\nSTATIC_ROOT = os.path.join(BASE_DIR, 'staticfiles')\nSTATIC_URL = '\/static\/'\nMEDIA_URL = '\/images\/'\n\n# Extra places for collectstatic to find static files.\nSTATICFILES_DIRS = (\n    os.path.join(BASE_DIR, 'static'),\n)\nMEDIA_ROOT = os.path.join(BASE_DIR, 'static\/images')\n\nif os.getcwd() == '\/app':\n    DEBUG=True\n    SECURE_PROXY_SSL_HEADER = ('HTTP_X_FORWARDED_PROTO', 'https')\n    SECURE_SSL_REDIRECT = True\n\n\nSTATICFILES_FINDERS = (\n    'django.contrib.staticfiles.finders.FileSystemFinder',\n    'django.contrib.staticfiles.finders.AppDirectoriesFinder',\n#    'django.contrib.staticfiles.finders.DefaultStorageFinder',\n)\n\n# Password validation\n# https:\/\/docs.djangoproject.com\/en\/2.2\/ref\/settings\/#auth-password-validators\n\nAUTH_PASSWORD_VALIDATORS = [\n    {\n        'NAME': 'django.contrib.auth.password_validation.UserAttributeSimilarityValidator',\n    },\n    {\n        'NAME': 'django.contrib.auth.password_validation.MinimumLengthValidator',\n    },\n    {\n        'NAME': 'django.contrib.auth.password_validation.CommonPasswordValidator',\n    },\n    {\n        'NAME': 'django.contrib.auth.password_validation.NumericPasswordValidator',\n    },\n]\n\n\n# Internationalization\n# https:\/\/docs.djangoproject.com\/en\/2.2\/topics\/i18n\/\n\nLANGUAGE_CODE = 'en-us'\n\nTIME_ZONE = 'America\/Chicago'\n\nUSE_I18N = True\n\nUSE_L10N = True\n\nUSE_TZ = True\n\nAUTH_USER_MODEL = 'users.CustomUser'\n\nLOGIN_REDIRECT_URL = 'dashboard'\nLOGOUT_REDIRECT_URL = 'home'\n\n#AUTHENTICATION_BACKENDS = ['users.forms.EmailBackend']\n\nEMAIL_BACKEND = 'django.core.mail.backends.console.EmailBackend'\n\nAUTHENTICATION_BACKENDS = (\n    # Needed to login by username in Django admin, regardless of `allauth`\n    \"django.contrib.auth.backends.ModelBackend\",\n\n    # `allauth` specific authentication methods, such as login by e-mail\n    \"allauth.account.auth_backends.AuthenticationBackend\",\n)\n\nSITE_ID = 1\n\nACCOUNT_EMAIL_REQUIRED = True\nACCOUNT_USERNAME_REQUIRED = False\nACCOUNT_SIGNUP_PASSWORD_ENTER_TWICE = False\nACCOUNT_AUTHENTICATION_METHOD = 'email'\nACCOUNT_UNIQUE_EMAIL = True\n\nSOCIALACCOUNT_QUERY_EMAIL=ACCOUNT_EMAIL_REQUIRED\nSOCIALACCOUNT_EMAIL_REQUIRED=ACCOUNT_EMAIL_REQUIRED\nSOCIALACCOUNT_STORE_TOKENS=False\n\nSOCIALACCOUNT_PROVIDERS = {\n    'facebook': {\n        'METHOD': 'oauth2',\n        'SCOPE': ['email', 'public_profile', 'user_friends'],\n        'AUTH_PARAMS': {'auth_type': 'reauthenticate'},\n        'INIT_PARAMS': {'cookie': True},\n        'FIELDS': [\n            'id',\n            'email',\n            'name',\n            'first_name',\n            'last_name',\n            'verified',\n            'locale',\n            'timezone',\n            'link',\n            'gender',\n            'updated_time',\n        ],\n        'EXCHANGE_TOKEN': True,\n        'LOCALE_FUNC': 'path.to.callable',\n        'VERIFIED_EMAIL': False,\n        'VERSION': 'v2.12',\n    },\n     'google': {\n        'SCOPE': [\n            'profile',\n            'email',\n        ],\n        'AUTH_PARAMS': {\n            'access_type': 'online',\n        }\n    }\n}\n\nACCOUNT_FORMS = {'signup': 'users.forms.CustomSignupForm'}\n\n# Configure django app for heroku.\ndjango_heroku.settings(locals())\n\nand my procfile looks like:\nweb: gunicorn scholarstarterapp.wsgi:application --log-file - --log-level debug\npython manage.py collectstatic --noinput\nmanage.py migrate\n\nPlease let me know if you know what my error means! Putting this here for future reference and because I solved my problem.\nMy \"settings.py\" \"wsgi.py\" etc. were not stored in a file named \"scholarstarterapp\". They were in \"scholarstarterappmaster\"\nI edited my wsgi.py file, procfile, and settings.py file to reflect the \"scholarstarterappmaster\" folder name and the deployment was clean.\nMake sure you know what your file names are referenced in!!\n","meta":{"link":"https:\/\/stackoverflow.com\/questions\/63361904\/error-code-h10-while-deploying-django-code-to-heroku"},"label":[[0,52,"problem"],[12788,12908,"problem"],[12911,13018,"resolution"]],"Comments":[]}
{"id":369,"text":"How can I use the legacy build system with Xcode 10's `xcodebuild`?. I'd like to use the new build system of Xcode 10 for development, but our build in our continuous integration system fails since the xcarchive produced has an issue: the Info.plist in the xcarchive is missing the ApplicationProperties key and the information therein!\nIt turns out switching back to the legacy build system fixes this. This can be done in the workspace settings (File > Workspace Settings… > Build System). But I would prefer to keep the new build system for development and only use the legacy build system for CI builds.\nIs there a way to make xcodebuild use the legacy build system without modifying the workspace? There is an (as of yet undocumented) flag in xcodebuild: -UseModernBuildSystem=<value>. The value can be either 0 or NO to use the legacy (\"original\") build system, or 1 or YES to use the new build system.\nFor example:\nxcodebuild -workspace Foo.xcworkspace -scheme Bar -configuration Release -archivePath \/path\/to\/Foo.xcarchive clean archive -UseModernBuildSystem=NO\n\n(-UseNewBuildSystem=<value> seems to work as well; this flags was introduced in Xcode 9 but I suspect UseModernBuildSystem is going to be the \"official\" flag for this.)\n","meta":{"link":"https:\/\/stackoverflow.com\/questions\/51205221\/how-can-i-use-the-legacy-build-system-with-xcode-10s-xcodebuild"},"label":[[10,66,"problem"],[740,907,"resolution"]],"Comments":[]}
{"id":370,"text":"installing cygwin: setup.ini missing from http:\/\/mirrors.kernel.org. I'm trying to install cygwin on a windows 2008 server. I managed to get a hold of the cygwin setup.exe version 2.721.\nSince cygwin.com is down at the moment, i tried several mirrors found through the google cache of the cygwin mirrors. I ran into the same problem like this guy: Help needed installing cygwin: may be ini file problem \nLike they suggested, I tried http:\/\/mirrors.kernel.org as mirror. It downloads some setup.bz files fine. Then I get the error \"Unable to get setup.ini from http:\/\/mirrors.kernel.org\/\". Any suggestions how to install cygwin now? I got this error when I tried to use http:\/\/cygwin.com\/setup.exe and not http:\/\/cygwin.com\/setup-x86.exe or http:\/\/cygwin.com\/setup-x86_64.exe. It seems the mirrors have just recently removed support for setup.exe. The setup.exe is still available to download but none of the mirrors work.\nThe solution is to use either the setup-x86.exe or the setup-x86_64.exe.\n","meta":{"link":"https:\/\/stackoverflow.com\/questions\/15496333\/installing-cygwin-setup-ini-missing-from-http-mirrors-kernel-org"},"label":[[1,67,"problem"],[785,845,"problem"],[941,993,"resolution"]],"Comments":[]}
{"id":371,"text":"No code coverage generated from ReportGenerator in Azure DevOps. I am using the ReportGenerator task in my Azure DevOps build to generate a code coverage report. The problem is that the generated report is empty. \nIn my VsTest task that runs the unit tests I have checked the option Code coverage enabled. The .coverage file is correctly generated and the unit tests are all successfully run. However, the generated code coverage report shows that no code was covered by any of the unit tests. \nHere is a screenshot of my code coverage Report Generator task.\n\nAnd here is the output from the code coverage report.\n\nHere is a screenshot of the Azure DevOps log file for the task showing that it executed correctly without any errors.\n\nInterestingly the log does show the following\nAnalyzing 0 classes\n\nIs this because ReportGenerator cannnot find the assemblies \/ classes? I've tried specififying a value for Source directories but this has made no difference. \nWhy is the code coverage report showing no code coverage? ReportGenerator can not handle .coverage files.\nYou can find the supported formats here: https:\/\/github.com\/danielpalme\/ReportGenerator#supported-input-and-output-file-formats\nYou can use a tool like OpenCover or coverlet to generate a coverage report in a format that ReportGenerator is able to parse.\nOr you can convert the .coverage file format. Here you find instructions how this can be done: https:\/\/github.com\/danielpalme\/ReportGenerator\/wiki\/Visual-Studio-Coverage-Tools\n","meta":{"link":"https:\/\/stackoverflow.com\/questions\/58374797\/no-code-coverage-generated-from-reportgenerator-in-azure-devops"},"label":[[0,64,"problem"],[1019,1066,"problem"],[1203,1367,"resolution"]],"Comments":[]}
{"id":372,"text":"passing pipeline variable as argument into Powershell build step not working in Azure DevOps. I want to be able to pass a pipeline variable into a Powershell build step in Azure DevOps Server. I'm trying to accomplish this by passing the variable as an argument.\nHere is my pipeline YAML file:\n- task: PowerShell@2\n  displayName: 'Detect Subfolder Changes'\n  name: setvarStep\n  inputs:\n    targetType: 'filePath'\n    filePath: $(System.DefaultWorkingDirectory)\\detectchanges.ps1\n    failOnStderr: true\n  \n- task: PowerShell@2\n  displayName: 'Get Version Number'\n  name: getVersion\n  inputs:\n    arguments: >\n      - packagepath $(setvarStep.changedPackage)\n    targetType: 'filePath'\n    filePath: $(System.DefaultWorkingDirectory)\\getversion.ps1\n    failOnStderr: true\n\nHere is the Powershell script (getversion.ps1) where I pass the 'packagepath' argument:\nparam($packagepath)\n\nWrite-Host \"packagepath is: $packagepath\"\n\n$xml = [Xml] (Get-Content $packagepath)\n$version = [Version] $xml.Project.PropertyGroup.Version\n\n\"##vso[task.setvariable variable=packageVersion;isOutput=true]$version\"\n\nThe Writ-Host command prints out 'packagepath is: -'. I turned on debugging and the argument value does show up in the log as: ##[debug]INPUT_ARGUMENTS: '- packagepath dotnet\/TestPackage\/*.csproj'\nInstead of passing the variable as an argument I also tried inserting the variable into the script like this: $packagepath = $($env:SETVARSTEP.CHANGEDPACKAGE) but it didn't work.\nI did try adding double quotes to the argument parameter in the build step like this: - packagepath \"$(setvarStep.changedPackage)\". It didn't work either.\nThe argument value is getting to the Powershell script, getversion.ps1 but for some reason the value is not displaying. @g.sulman Thanks for the suggestion. I changed - packagepath to -packagepath and it worked. I figured it was something small like the syntax.\n","meta":{"link":"https:\/\/stackoverflow.com\/questions\/65906620\/passing-pipeline-variable-as-argument-into-powershell-build-step-not-working-in"},"label":[[0,93,"problem"],[1093,1145,"problem"],[1177,1215,"problem"],[1783,1820,"resolution"]],"Comments":[]}
{"id":373,"text":"How to resolve StandardWrapperValve error on Catalina?. I am creating a signup application with validation and Captcha (using SimpleCaptcha). I am able to deploy my application on localhost and everything works perfectly inclusive of the captcha.\nBut when deployed the war file to a test server, my captcha is not working, and here's an output from the log file:\norg.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:111) \nServlet SimpleCaptchaServlet is currently unavailable\n\nI did register my SimpleCaptchaServlet on the web.xml, and I can locate the class file for that Servlet, but still unable to locate the problem. May I know how can I fix this issue?\nHere's the web.xml:\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<web-app version=\"2.5\" xmlns=\"http:\/\/java.sun.com\/xml\/ns\/javaee\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/java.sun.com\/xml\/ns\/javaee http:\/\/java.sun.com\/xml\/ns\/javaee\/web-app_2_5.xsd\">\n<servlet>\n    <servlet-name>SignupProcessServlet<\/servlet-name>\n    <servlet-class>com.example.controller.SignupProcessServlet<\/servlet-class>\n<\/servlet>\n<servlet>\n    <servlet-name>SignupSuccessServlet<\/servlet-name>\n    <servlet-class>com.example.view.SignupSuccessServlet<\/servlet-class>\n<\/servlet>\n<servlet>\n    <servlet-name>SignupServlet<\/servlet-name>\n    <servlet-class>com.example.view.SignupServlet<\/servlet-class>\n<\/servlet>\n<servlet>\n    <servlet-name>SimpleCaptchaServlet<\/servlet-name>\n    <servlet-class>com.example.controller.SimpleCaptchaServlet<\/servlet-class>\n<\/servlet>\n<servlet-mapping>\n    <servlet-name>SignupProcessServlet<\/servlet-name>\n    <url-pattern>\/signup_process.do<\/url-pattern>\n<\/servlet-mapping>\n<servlet-mapping>\n    <servlet-name>SignupSuccessServlet<\/servlet-name>\n    <url-pattern>\/signup_success.view<\/url-pattern>\n<\/servlet-mapping>\n<servlet-mapping>\n    <servlet-name>SignupServlet<\/servlet-name>\n    <url-pattern>\/signup.view<\/url-pattern>\n<\/servlet-mapping>\n<servlet-mapping>\n    <servlet-name>SimpleCaptchaServlet<\/servlet-name>\n    <url-pattern>\/captchaImg<\/url-pattern>\n<\/servlet-mapping>\n<session-config>\n    <session-timeout>\n        30\n    <\/session-timeout>\n<\/session-config>\n<welcome-file-list>\n    <welcome-file>signup.html<\/welcome-file>\n<\/welcome-file-list>\n\n\nThank you very much. Thanks for helping me out. I managed to get my issue fixed. It is an issue with the existing java 2 sdk on the server, not running java ee 5. It's outdated, and using tomcat 5.5. Upon creating a new java and tomcat instance (as there are still some applications running on the old version), everything works well.\nFor anyone who have happened to meet with this similar issue as mine, it's either you upgrade the latest jave 2 sdk and tomcat, or downgrade your script to the old version compatible. \n","meta":{"link":"https:\/\/stackoverflow.com\/questions\/2085114\/how-to-resolve-standardwrappervalve-error-on-catalina"},"label":[[251,321,"problem"],[448,501,"problem"],[2389,2483,"problem"],[2489,2528,"resolution"],[2705,2802,"resolution"]],"Comments":[]}
{"id":374,"text":"Gitolite Authentication Issue from Remote Server. I am having trouble running a simple command (which is part of a larger deploy process problem) on one sever from another. \nuser gitolite:\nI can ssh using that user onto the box as it returns\nX@repo2:~$ ssh gitolite@repo1\nPTY allocation request failed on channel 0\nhello repo2, this is gitolite v2.1-19-g36dfb85 running on git 1.7.0.4\nthe gitolite config gives you the following access:\n     R   W      proto_projectname_rails\n    @R_ @W_     testing\nConnection to repo1.x closed.\n\nHowever when I try run git ls-remote ssh:\/\/gitolite@repo1.x:2011\/proto_projectname_rails it prompts for a password.\nmy gitolite.conf has the lines\nrepo    proto_projectname_rails\n    RW+     =   MBP\n    RW+     =   repo2\n\nand the public key is in my keydir with the same name e.g. repo2.pub. \nI also ran \ngl-setup repo2.pub \n\nwhich updated the authorised_keys to include repo2 in there. \nI am all out of ideas as to what else I need to do. If i run the same command from my local machine. i.e from MBP it completes successfully. \nAny ideas on this would be more than welcome.\nThank you. If ssh gitolite@repo1 works, then that is the ssh address you must use for gitolite.\nGitolite itself will call git, and the ssh settings on the server might use internally gitolite@repo1.x:2011 for that git call.\nBut for the end-user, client of gitolite, git ls-remote ssh:\/\/gitolite@repo1\/proto_projectname_rails will work.\n","meta":{"link":"https:\/\/stackoverflow.com\/questions\/13570074\/gitolite-authentication-issue-from-remote-server"},"label":[[540,647,"problem"],[1122,1140,"resolution"],[1374,1432,"resolution"]],"Comments":[]}
{"id":375,"text":"Multiple queries in code igniter - semicolon breaks query. I am working on a project where I need to create a pivot table with dynamic columns. To this end I am using the tutorial as described here: http:\/\/stratosprovatopoulos.com\/web-development\/mysql\/pivot-table-with-dynamic-columns\/\nUsing that tutorial, I was able to make a proper query to make a pivot table.\nWhen I execute it via phpMyAdmin SQL dialog, the query runs fine and the results come up as expected. The problem I ran into is that when I try to run the query through Code Igniter via $this->db->query($query), it fails at the first semicolons that mark the end of the first SQL statement in the overall query. The problem is that the statement is built up of several queries that I need to run at the same time. \nI read about transactions on CI, but could not figure how (if possible) to obtain the result of the query. \nWill greatly appreciate any advice on a proper way to run that query on CI and avoid the semicolon problem.\nThanks! In case this is helpful to anyone in the future, I was able to get over the multi-statement issue by placing the whole query inside a stored procedure. The procedure along with its parameters is then called via CodeIgniter.\n","meta":{"link":"https:\/\/stackoverflow.com\/questions\/24336309\/multiple-queries-in-code-igniter-semicolon-breaks-query"},"label":[[0,57,"problem"],[1105,1154,"resolution"]],"Comments":[]}
{"id":376,"text":"Web Project is not deploying. When I try to deploy a web project, Wildfly always returns me this error:\n12:08:58,929 ERROR [org.jboss.msc.service.fail] (ServerService Thread Pool -- 81) MSC000001: Failed to start service jboss.deployment.unit.\"focusoc-web-0.0.1-SNAPSHOT.war\".undertow-deployment: org.jboss.msc.service.StartException in service jboss.deployment.unit.\"focusoc-web-0.0.1-SNAPSHOT.war\".undertow-deployment: java.lang.RuntimeException: java.lang.RuntimeException: com.sun.faces.config.ConfigurationException: CONFIGURATION FAILED! null\nat org.wildfly.extension.undertow.deployment.UndertowDeploymentService$1.run(UndertowDeploymentService.java:81)\nat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\nat java.util.concurrent.FutureTask.run(FutureTask.java:266)\nat org.jboss.threads.ContextClassLoaderSavingRunnable.run(ContextClassLoaderSavingRunnable.java:35)\nat org.jboss.threads.EnhancedQueueExecutor.safeRun(EnhancedQueueExecutor.java:1982)\nat org.jboss.threads.EnhancedQueueExecutor$ThreadBody.doRunTask(EnhancedQueueExecutor.java:1486)\nat org.jboss.threads.EnhancedQueueExecutor$ThreadBody.run(EnhancedQueueExecutor.java:1377)\nat java.lang.Thread.run(Thread.java:748)\nat org.jboss.threads.JBossThread.run(JBossThread.java:485)\nCaused by: java.lang.RuntimeException: java.lang.RuntimeException: com.sun.faces.config.ConfigurationException: CONFIGURATION FAILED! null\nat io.undertow.servlet.core.DeploymentManagerImpl.deploy(DeploymentManagerImpl.java:252)\nat org.wildfly.extension.undertow.deployment.UndertowDeploymentService.startContext(UndertowDeploymentService.java:96)\nat org.wildfly.extension.undertow.deployment.UndertowDeploymentService$1.run(UndertowDeploymentService.java:78)\n... 8 more\nCaused by: java.lang.RuntimeException: com.sun.faces.config.ConfigurationException: CONFIGURATION FAILED! null\nat com.sun.faces.config.ConfigureListener.contextInitialized(ConfigureListener.java:283)\nat io.undertow.servlet.core.ApplicationListeners.contextInitialized(ApplicationListeners.java:187)\nat io.undertow.servlet.core.DeploymentManagerImpl$1.call(DeploymentManagerImpl.java:216)\nat io.undertow.servlet.core.DeploymentManagerImpl$1.call(DeploymentManagerImpl.java:185)\nat io.undertow.servlet.core.ServletRequestContextThreadSetupAction$1.call(ServletRequestContextThreadSetupAction.java:42)\nat io.undertow.servlet.core.ContextClassLoaderSetupAction$1.call(ContextClassLoaderSetupAction.java:43)\nat org.wildfly.extension.undertow.deployment.UndertowDeploymentInfoService$UndertowThreadSetupAction.lambda$create$0(UndertowDeploymentInfoService.java:1502)\nat org.wildfly.extension.undertow.deployment.UndertowDeploymentInfoService$UndertowThreadSetupAction.lambda$create$0(UndertowDeploymentInfoService.java:1502)\nat org.wildfly.extension.undertow.deployment.UndertowDeploymentInfoService$UndertowThreadSetupAction.lambda$create$0(UndertowDeploymentInfoService.java:1502)\nat org.wildfly.extension.undertow.deployment.UndertowDeploymentInfoService$UndertowThreadSetupAction.lambda$create$0(UndertowDeploymentInfoService.java:1502)\nat io.undertow.servlet.core.DeploymentManagerImpl.deploy(DeploymentManagerImpl.java:250)\n... 10 more\nCaused by: com.sun.faces.config.ConfigurationException: CONFIGURATION FAILED! null\nat com.sun.faces.config.ConfigManager.initialize(ConfigManager.java:357)\nat com.sun.faces.config.ConfigureListener.contextInitialized(ConfigureListener.java:205)\n... 20 more\nCaused by: java.lang.NullPointerException\nat com.sun.faces.facelets.impl.DefaultResourceResolver.resolveUrl(DefaultResourceResolver.java:40)\nat com.sun.faces.facelets.impl.DefaultFaceletFactory.init(DefaultFaceletFactory.java:129)\nat com.sun.faces.application.ApplicationAssociate.createFaceletFactory(ApplicationAssociate.java:849)\nat com.sun.faces.application.ApplicationAssociate.initializeFacelets(ApplicationAssociate.java:342)\nat com.sun.faces.application.ApplicationAssociate.getCompiler(ApplicationAssociate.java:420)\nat com.sun.faces.config.processor.FaceletTaglibConfigProcessor.process(FaceletTaglibConfigProcessor.java:217)\nat com.sun.faces.config.ConfigManager.initialize(ConfigManager.java:341)\n... 21 more\n\nI don't know what it is missing in the project, and I don't know what to do. In localhost I also have a keycloak server, but it doesn't have bother this one, don't it? Finally I solved it. The main problem that I had it is what I configured it as Maven Project, and I didn't compile it as Maven.\n","meta":{"link":"https:\/\/stackoverflow.com\/questions\/58234411\/web-project-is-not-deploying"},"label":[[196,220,"problem"],[522,542,"problem"],[3442,3472,"problem"],[4382,4447,"problem"]],"Comments":[]}
{"id":377,"text":"Curl into API cluster secured by open VPN from another cluster's pod's container. I have created 2 kubernetes clusters on AWS within a VPC.\n1) Cluster dedicated to micro services (MI)\n2) Cluster dedicated to Consul\/Vault (Vault)\nSo basically both of those clusters can be reached through distinct classic public load balancers which expose k8s APIs\nMI: https:\/\/api.k8s.domain.com\nVault: https:\/\/api.vault.domain.com\nI also set up openvpn on both clusters, so you need to be logged in vpn to \"curl\" or \"kubectl\" into the clusters.\nTo do that I just added a new rule in the ELBs's security groups with the VPN's IP on port 443:\nHTTPS   443   VPN's IP\/32\nAt this point all works correctly, which means I'm able to successfully \"kubectl\" in both clusters.\nNext thing I need to do, is to be able to do a curl from Vault's cluster within pod's container within  into the MI cluster. Basically:\nVault Cluster --------> curl https:\/\/api.k8s.domain.com --header \"Authorization: Bearer $TOKEN\"--------> MI cluster\nThe problem is that at the moment clusters only allow traffic from VPN's IP.\nTo solve that, I've added new rules in the security group of MI cluster's load balancer.\nThose new rules allow traffic from each vault's node private and master instances IPs.\nBut for some reason it does not work!\nPlease note that before adding restrictions in the ELB's security group I've made sure the communication works with both clusters allowing all traffic (0.0.0.0\/0)\nSo the question is when I execute a command curl in pod's container into another cluster api within the same VPC, what is the IP of the container to add to the security group ? NAT gateway's EIP for the Vault VPC had to be added to the ELB's security group to allow traffic.\n","meta":{"link":"https:\/\/stackoverflow.com\/questions\/56534190\/curl-into-api-cluster-secured-by-open-vpn-from-another-clusters-pods-container"},"label":[[1037,1080,"problem"],[1635,1732,"resolution"]],"Comments":[]}
{"id":378,"text":"GitLab Runner not using Registry Mirror. Since docker introduced the pull rate limit i'm running a pull-through registry.\nI have done the setup with the documentation on dockers site (https:\/\/docs.docker.com\/registry\/recipes\/mirror\/#configure-the-cache).\nIn my runners config.toml two volumes will be mounted, one for the docker socket and another one for the daemon.json.\nBut everytime i want to build something i get the following error message that i have reached my pull rate limit:\nRunning with gitlab-runner 13.11.0 (7f7a4bb0)\n\n\non srv-gitlab-ba5 vXWs_kze\nPreparing the \"docker\" executor\n00:25\nUsing Docker executor with image alpine:3.12 ...\nWARNING: Pulling GitLab Runner helper image from Docker Hub. Helper image is migrating to registry.gitlab.com, for more information see https:\/\/docs.gitlab.com\/runner\/configuration\/advanced-configuration.html#migrate-helper-image-to-registrygitlabcom\nPulling docker image gitlab\/gitlab-runner-helper:x86_64-7f7a4bb0 ...\nUsing docker image sha256:25f27e06750273451a8211bbe80dc14f5d2bb389ff912ae0adffb477fa26058f for gitlab\/gitlab-runner-helper:x86_64-7f7a4bb0 with digest gitlab\/gitlab-runner-helper@sha256:20ff486a25738d42d2bca770092d2c4b1ae28881de3b599877ab954d3654a08a ...\nPulling docker image alpine:3.12 ...\nWARNING: Failed to pull image with policy \"always\": toomanyrequests: You have reached your pull rate limit. You may increase the limit by authenticating and upgrading: https:\/\/www.docker.com\/increase-rate-limit (manager.go:205:2s)\nERROR: Preparation failed: failed to pull image \"alpine:3.12\" with specified policies [always]: toomanyrequests: You have reached your pull rate limit. You may increase the limit by authenticating and upgrading: https:\/\/www.docker.com\/increase-rate-limit (manager.go:205:2s)\nWill be retried in 3s ...\n\nMy config.toml for the runner looks like:\nconcurrent = 5\ncheck_interval = 0\n\n[session_server]\n  session_timeout = 1800\n\n[[runners]]\n  name = \"srv-gitlab-ba5\"\n  url = \"***\"\n  token = \"***\"\n  executor = \"docker\"\n  [runners.cache]\n    Type = \"s3\"\n    Shared = true\n    [runners.cache.s3]\n      ServerAddress = \"***\"\n      AccessKey = \"***\"\n      SecretKey = \"***\"\n      BucketName = \"gitlab-cache\"\n      Insecure = false\n  [runners.docker]\n    tls_verify = false\n    image = \"docker:19.03.8\"\n    privileged = false\n    disable_entrypoint_overwrite = false\n    oom_kill_disable = false\n    disable_cache = false\n    volumes = [\"\/var\/run\/docker.sock:\/var\/run\/docker.sock\", \"\/etc\/docker:\/etc\/docker:ro\", \"\/cache\"]\n    shm_size = 0\n\ndocker-compose.yml for the runner:\nversion: \"3.7\"\nservices:\n  gitlab_runner:\n    image: {self build image with config.toml above}\n    restart: always\n    container_name: gitlab-runner\n    privileged: true\n    volumes:\n      - \/var\/run\/docker.sock:\/var\/run\/docker.sock\n      - \/etc\/docker:\/etc\/docker:ro\n\nCan someone tell me if my configuration is correct or is it possible that i forgot something?\nI have googled a lot but didn't find the correct solution for my problem. If you do not switch off instance runner very often, try to avoid using option \"always\"\nWARNING: Failed to pull image with policy \"always\"\n\nTo switch this value, set up gitlab-runner registry with --docker-pull-policy \"if-not-present\"\nthen docker will keep layers of images and will not pull it every time from docker registry, no clue then to create additional registry\n","meta":{"link":"https:\/\/stackoverflow.com\/questions\/68181263\/gitlab-runner-not-using-registry-mirror"},"label":[[1270,1368,"problem"],[3052,3079,"resolution"],[3154,3226,"resolution"]],"Comments":[]}
{"id":379,"text":"Automated Deployment and Upgrade Strategy for ASP.Net MVC Application. I am working on a ASP.net MVC4 project where a same project needs to be deployed to many clients on daily basis, each client will have its own domain \/ sub domain and a separate app pool and db (MSSSQL). \nDoing each deployment manually could take at least 1-2 hours if everything goes well. Is there anyway using which I can do this in some automated way?\nMoreover, we also need to update all of the apps when a new version is released.. may be one by one or all of them at same time. However, doing this manually could take weeks and once we have more clients then it will not possible doing this update manually. \nThe update involves, suspending app for some time, taking a full backup of files and db, update application code\/ files in app folder, upgrade db with a script and then start app, doing some diagnosis script to check if update was successful or not, if not we need to check what went wrong?\nHow can we automate this updates? Any idea would be great on how to approach this issue. As a developer for BuildMaster, I can say that this scenario, known as the \"Core Version\" pattern, is a common one. If you're OK with a paid solution, you can setup your deployment plans within the tool that do exactly what you described.\nAs a more concrete example, we experience this exact situation in a slightly different way. BuildMaster has a set of 60+ extensions that rely on a specific SDK version. In our recent 4.0 release, we had to re-deploy every extension because of breaking API changes within the SDK. This is essentially equivalent to having a bunch of customers and deploying to them all at once. We have set up our deployment plans such that any time we create a new release of the SDK application, we have the option to set a variable that says to build every extension that relies on the SDK:\n\nIn BuildMaster, the idea is to promote a build (i.e. an immutable object that travels through various environments like Dev, Test, Staging, Prod) to its final environment (where it becomes the deployed build for the release). In your case, this would be pushing your MVC application to its final environment, and that would then trigger the deployments of all dependent applications (i.e. your customers' instances of your application). For our SDK, the plan looks like this: \n\nFor your scenario, you would only need the single action, \"Promote Build\". As I mentioned before, any dependents would then be promoted to their final environments, so all your customer deployments would kick off once that action is run during deployment. As an example, our Azure extension's deployment plan for its final environment looks like this (internal URLs redacted):\n\nYou may have noticed that these plans are marked \"Shared\", which means every extension we have has the exact same deployment plan, but utilizes different variables to handle the minor differences like names, paths, etc. \nSince this is such an enormous topic I could go on for ages, but I think that should be sufficient for your use-case if you wanted to try it out.\n","meta":{"link":"https:\/\/stackoverflow.com\/questions\/21255820\/automated-deployment-and-upgrade-strategy-for-asp-net-mvc-application"},"label":[[276,336,"problem"],[565,685,"problem"],[1086,1098,"resolution"],[1226,1269,"resolution"],[2380,2434,"resolution"]],"Comments":[]}
{"id":380,"text":"Error while distributing app on air using dropbox. I am using Enterprise certificate to distribute on air using dropbox following this link: http:\/\/aaronparecki.com\/articles\/2011\/01\/21\/1\/how-to-distribute-your-ios-apps-over-the-air\n\nWhile creating plist I used a dummy application URL and title i.e.\nhttps.....ipa, AppName resp.\nBoth ipa and plist Uploaded on dropbox\nUsed shared link of plist from dropbox and mention on HTML document\nI used HTML webpage\/document shared link (replacing www.dropbox.com by dl.dropboxusercontent.com).\n\nBut unable to download app.\nWhile downloading I am getting the error: Unable to download app. \"AppName\" can not be downloaded at this time.\nConsole message: \nBG Application: Not Present, BG Daemon: Present. Daemons: networkd apsd itunesstored \n <Error>: FAILURE: Failed to open property list at file:\/\/\/private\/var\/db\/launchd.db\/com.apple.launchd\/overrides.plist for reading. (The operation couldn’t be completed. No such file or directory)\n <Notice>: WiFi:[428491987.594512]: Client itunesstored set type to normal application\n\nCan anyone please help? The url given during the \"Save for Enterprise Distribution\"  process is the url that gets saved in the .plist. That url should be the location of the .ipa file. \nThen the link to download your app is like this itms-services:\/\/?action=download-manifest&amp;url=<LINK_TO_PLIST>  Then the .plist directs the user to the .ipa url saved in the .plist.  The link example was taken from a working implementation of OTA distribution that I did for iOS 7.1 devices a couple of months ago.\n","meta":{"link":"https:\/\/stackoverflow.com\/questions\/25056585\/error-while-distributing-app-on-air-using-dropbox"},"label":[[606,675,"problem"],[790,828,"problem"],[913,975,"problem"],[1089,1199,"problem"],[1200,1364,"resolution"]],"Comments":[]}
{"id":381,"text":"React route gives unauthorized on heroku while working in development. I am working on React app using express in backend, and I deployed it on Heroku.\nNow its working well except 1 route which gives me unauthorized message\nMy Heroku logs\nAnd this route is working on development mode.\nAlso I checked the token of the user in the localStorage and its already there.\nAnyone has any idea about this issue? I figured out what was the problem after asking a friend of mine.\nThe problem was there was a conflict in routes between frontend and backend, so I added \/api\/ to backend routes and to the frontend baseUrl.\nEx:\nin backend I was using the routes like app.use(\"\/topic\", topicRoutes);\nin frontend my baseUrl was localhost:3000\nTo fix my problem, I changed the routes:\napp.use(\"\/api\/topic\", topicRoutes);\nlocalhost:3000\/api\/\nHope it helps someone :))\n","meta":{"link":"https:\/\/stackoverflow.com\/questions\/60129285\/react-route-gives-unauthorized-on-heroku-while-working-in-development"},"label":[[0,70,"problem"],[498,545,"problem"],[552,609,"resolution"]],"Comments":[]}
{"id":383,"text":"Deployed application using InstallShield does not work. If you see below image, there are two applications deployed to IIS.\n\nQManualDeployment - This is deployed using Visual studio Publish feature.\nInstallShieldPOCWebApplication - This is deployed using InstallShield installation tool.\n\nOption 1 is working without any issues, but as you see InstallShieldPOCWebApplication it looks like foder rather than web site. Also it does not work.\nHow can I deploy application as website using INSTALLSHIELD\n\nHere is my IIS Settings from InstallShield Tool Default Web Site is a website.  QManualDeployment is a virtual directory \/ application off of Default Web Site.\nIn InstallShield, it isn't enough to just author all of your directories\/files into components.  You also have to define the IIS configuration. \nI'm not sure if you are using InstallShield  Express, Professional\/Premier  of Limited Edition so I can't give you more direction then that.\n","meta":{"link":"https:\/\/stackoverflow.com\/questions\/48803933\/deployed-application-using-installshield-does-not-work"},"label":[[0,55,"problem"],[344,415,"problem"],[661,756,"problem"],[775,805,"resolution"]],"Comments":[]}
{"id":384,"text":"Error in setting JAVA_HOME. I have recently downloaded Maven and followed the instructions given on this this page. I already have ant installed on my machine.\nNow, if I want to verify that Maven is installed perfectly or not it is giving me error that JAVA_HOME is not set correctly, but same works perfectly fine for ANT.\nFor Maven I tried :\n1. open cmd\n2. type mvn -version\n3. Error appeared :\nC:\\Users\\Admin>mvn -version\n\nERROR: JAVA_HOME is set to an invalid directory.\nJAVA_HOME = \"C:\\Program Files\\Java\\jre7\\bin\"\nPlease set the JAVA_HOME variable in your environment to match the\nlocation of your Java installation\n\nFor ANT I tried and worked :\n1. open cmd\n2. type mvn -version\n3. Apache Ant(TM) version 1.9.1 compiled on May 15 2013\n\nI went to the directory to check that java.exe is actually there in that directory or not and it was there. I checked the environment variables they set fine. I restarted the system and checked again but same problem. Please let me know what am I missing. JAVA_HOME should point to jdk directory and not to jre directory. Also JAVA_HOME should point to the home jdk directory and not to jdk\/bin directory.\nAssuming that you have JDK installed in your program files directory then you need to set the JAVA_HOME like this:\nJAVA_HOME=\"C:\\Program Files\\Java\\jdkxxx\"\n\nxxx is the jdk version\nFollow this link to learn more about setting JAVA_HOME:\nhttp:\/\/docs.oracle.com\/cd\/E19182-01\/820-7851\/inst_cli_jdk_javahome_t\/index.html\n","meta":{"link":"https:\/\/stackoverflow.com\/questions\/17315425\/error-in-setting-java-home"},"label":[[433,474,"problem"],[998,1063,"problem"],[1234,1303,"resolution"]],"Comments":[]}
{"id":385,"text":"Required. Should have privileged access, such as root or administrator in manageiq. I try to monitor Hawkular with manageiq on Centos 7. These are my monitoring implementation process which are run on root privilege.\nFirst cassandra nosql db is executed.\n# systemctl start cassandra\n\nAnd next I execute the hawkular server like below as root\n# unzip hawkular-services-dist-0.23.0.Final.zip\n# .\/add-user.sh -a -u username -p password -g read-write,read-only\n# .\/standalone.sh -Dhawkular.rest.user=username -Dhawkular.rest.password=password -Dhawkular.agent.enabled=true\n\nAnd then, I run Docker and related manageiq docker-image.\n# systemctl start docker\n# docker pull manageiq\/manageiq:euwe-1\n# docker run --privileged -d -p 8443:443 manageiq\/manageiq:euwe-1\n\nI run manageiq on Firefox with the following address and it works successfully with user admin and password smartvm.\nhttps:\/\/localhost:8443\n\nAnd I try to add Hawkular middleware on manageiq web UI with the menu \"Add New Middleware Provider\" However error message is shown, \n\"Required. Should have privileged access, such as root or administrator.\"\n\nI cannot understand this error message. Hawkular server and manageiq are run on root privilege. How can I add Hawkular service into manageiq monitoring service? I solved it. My OS is windows 10 pro, but my hyper-v service was deactivated. I activated hyper-v and it worked well. Thanks any way\n","meta":{"link":"https:\/\/stackoverflow.com\/questions\/41775329\/required-should-have-privileged-access-such-as-root-or-administrator-in-manage"},"label":[[0,83,"problem"],[1314,1346,"problem"],[1349,1367,"resolution"]],"Comments":[]}
{"id":386,"text":"node npm install -g bower FATAL ERROR: CALL_AND_RETRY_2 on Win7. Windows 7 Enterprise, Git and Node installed.\nWhen I try to \n    npm install -g bower\n\nin command prompt (as administrator or user) on fresh node js install (x86 and x64), it sits there for 20 minutes and its memory usage slowly climbs up to 2GB of RAM. After 20 minutes it crashes with \n    FATAL ERROR: CALL_AND_RETRY_2 Allocation failed - process out of memory.\n\nAny ideas about how to install bower globally?\nUPDATE: It looks like npm fails to install any package globally (yeoman, grunt). Clearing cache, installing and reinstalling different node js versions or package version doesn't help.\nI get a bunch of ENOENT and EPERM errors if I don't run \n    npm rm -g bower\n    npm cache clean\n\nfirst. I think the part of the problem could be that my Windows profile is stored on unix server and the path length to my AppData\/Roaming is about 100 characters long. The solution: admins changed the windows policy group for my profile, so that AppData is kept on local machine instead of remote server location and it fixed my problem. I can install any package globally now without any issues.\n","meta":{"link":"https:\/\/stackoverflow.com\/questions\/25514839\/node-npm-install-g-bower-fatal-error-call-and-retry-2-on-win7"},"label":[[0,64,"problem"],[500,541,"problem"],[680,703,"problem"],[951,1074,"resolution"]],"Comments":[]}
{"id":387,"text":"Can't send email in production. I am developing online shop and everything worked fine on local machine.Client add item to cart, enter his credentials click 'Make Order' and than order confirmation should be sent to his email. And it was working fine, even with redis + celery, but since when I've deployed project to server (linode.com), order confirmation doesn't work. Worker took tasks but never execute this. I thought it was due to celery, and I've made decision to make sending email without task manager and queue. But it didn't help.\nI use UFW firewall in the server:\nTo                         Action      From\n--                         ------      ----\n22\/tcp                     ALLOW       Anywhere                  \n80\/tcp                     ALLOW       Anywhere                  \nAnywhere                   ALLOW       96.126.119.66             \n443                        ALLOW       Anywhere                  \n22\/tcp (v6)                ALLOW       Anywhere (v6)             \n80\/tcp (v6)                ALLOW       Anywhere (v6)             \n443 (v6)                   ALLOW       Anywhere (v6)  \n\nI get error like this:\n\n[Errno 110] Connection timed out\n\nHope you will help me to understand the issue\nTraceback:\nEnvironment:\n\n\nRequest Method: POST\nRequest URL: https:\/\/bauerdress.ru\/orders\/create\/\n\nDjango Version: 3.1\nPython Version: 3.8.5\nInstalled Applications:\n['django.contrib.admin',\n 'django.contrib.auth',\n 'django.contrib.contenttypes',\n 'django.contrib.sessions',\n 'django.contrib.messages',\n 'django.contrib.sites',\n 'django.contrib.staticfiles',\n 'shop.apps.ShopConfig',\n 'cart.apps.CartConfig',\n 'orders.apps.OrdersConfig',\n 'wishlist.apps.WishlistConfig',\n 'widget_tweaks',\n 'crispy_forms',\n 'information.apps.InformationConfig',\n 'promotion.apps.PromotionConfig',\n 'django_filters',\n 'email_sub.apps.EmailSubConfig',\n 'django_simple_coupons',\n 'ckeditor',\n 'flower',\n 'allauth',\n 'allauth.account',\n 'allauth.socialaccount']\nInstalled Middleware:\n['django.middleware.security.SecurityMiddleware',\n 'django.contrib.sessions.middleware.SessionMiddleware',\n 'django.middleware.common.CommonMiddleware',\n 'django.middleware.csrf.CsrfViewMiddleware',\n 'django.contrib.auth.middleware.AuthenticationMiddleware',\n 'django.contrib.messages.middleware.MessageMiddleware',\n 'django.middleware.clickjacking.XFrameOptionsMiddleware']\n\n\n\nTraceback (most recent call last):\n  File \"\/home\/kirill\/venv\/lib\/python3.8\/site-packages\/django\/core\/handlers\/exception.py\", line 47, in inner\n    response = get_response(request)\n  File \"\/home\/kirill\/venv\/lib\/python3.8\/site-packages\/django\/core\/handlers\/base.py\", line 179, in _get_response\n    response = wrapped_callback(request, *callback_args, **callback_kwargs)\n  File \"\/home\/kirill\/bauer_dress\/orders\/views.py\", line 46, in order_create\n    order_created(order.id)\n  File \"\/home\/kirill\/bauer_dress\/orders\/tasks.py\", line 22, in order_created\n    mail_sent = send_mail(subject, None, settings.EMAIL_HOST_USER, [order.email, settings.MAIL], html_message=html)\n  File \"\/home\/kirill\/venv\/lib\/python3.8\/site-packages\/django\/core\/mail\/__init__.py\", line 61, in send_mail\n    return mail.send()\n  File \"\/home\/kirill\/venv\/lib\/python3.8\/site-packages\/django\/core\/mail\/message.py\", line 284, in send\n    return self.get_connection(fail_silently).send_messages([self])\n  File \"\/home\/kirill\/venv\/lib\/python3.8\/site-packages\/django\/core\/mail\/backends\/smtp.py\", line 102, in send_messages\n    new_conn_created = self.open()\n  File \"\/home\/kirill\/venv\/lib\/python3.8\/site-packages\/django\/core\/mail\/backends\/smtp.py\", line 62, in open\n    self.connection = self.connection_class(self.host, self.port, **connection_params)\n  File \"\/usr\/lib\/python3.8\/smtplib.py\", line 253, in __init__\n    (code, msg) = self.connect(host, port)\n  File \"\/usr\/lib\/python3.8\/smtplib.py\", line 339, in connect\n    self.sock = self._get_socket(host, port, self.timeout)\n  File \"\/usr\/lib\/python3.8\/smtplib.py\", line 308, in _get_socket\n    return socket.create_connection((host, port), timeout,\n  File \"\/usr\/lib\/python3.8\/socket.py\", line 808, in create_connection\n    raise err\n  File \"\/usr\/lib\/python3.8\/socket.py\", line 796, in create_connection\n    sock.connect(sa)\n\nException Type: TimeoutError at \/orders\/create\/\nException Value: [Errno 110] Connection timed out\n\nviews.py\nfrom django.conf import settings\nfrom django.shortcuts import render, get_object_or_404\nfrom django.http import HttpResponse\nfrom cart.cart import Cart\nfrom .models import OrderItem, Order\nfrom .forms import OrderCreateForm\nfrom shop.models import Product\nfrom django.contrib.admin.views.decorators import staff_member_required\nfrom django_simple_coupons.forms import CouponApplyForm\nimport weasyprint\nfrom django.template import loader\nfrom django.core.mail import send_mail\nfrom email_sub.models import Subscription\nfrom django.core.exceptions import ObjectDoesNotExist\nfrom .tasks import order_created\n\n\n\n\n def order_create(request):\n    cart = Cart(request)\n    coupon_apply_form = CouponApplyForm()\n    if request.method == 'POST':\n        form = OrderCreateForm(request.POST)\n        if form.is_valid():\n            order = form.save(commit=False)\n            if cart.coupon:\n                order.coupon = cart.coupon\n                order.discount = cart.coupon.discount.value\n            order.save()\n            for item in cart:\n                OrderItem.objects.create(\n                    order=order,\n                    product=item['product'],\n                    product_set=item['product_set'],\n                    price=item['product_set'].price,\n                    quantity=item['quantity'],\n                    size = item['product_set'].size_set,\n                    color = item['color']\n                )\n            try:\n                Subscription.objects.get(email=order.email)\n            except ObjectDoesNotExist:\n                Subscription.objects.create(name=order.first_name, surname=order.last_name, email=order.email)\n            \n            order_created(order.id) # <--- order confirmation email \n            cart.clear()\n            if cart.coupon:\n                cart.clear_coupon()\n            return render(request, 'orders\/order\/created.html', {'order': order})\n    else:\n        form = OrderCreateForm()\n    return render(request, 'orders\/order\/create_v2.html', {\n        'cart': cart,\n        'form': form,\n        'coupon_apply_form': coupon_apply_form})\n\ntasks.py\ndef order_created(order_id):\n    \"\"\"\n    Task for sending mail if order created\n    \"\"\"\n    order = Order.objects.get(id=order_id)\n    subject = 'Заказ #{}'.format(order.id)\n    html = loader.render_to_string('orders\/order\/mail2.html', context={'order': order})\n    send_mail = send_mail(subject, None, settings.EMAIL_HOST_USER, [order.email, settings.MAIL], html_message=html)\n    return send_mail\n\nsettings.py (part of smtp settings)\n#smtp\nEMAIL_USE_TLS = True\nEMAIL_HOST = 'smtp.gmail.com'\nEMAIL_HOST_USER = 'myuser@gmail.com' #real data used in production\nEMAIL_HOST_PASSWORD = '1234'\nEMAIL_PORT = 587 The problem was that I use Linode and linode blocks ports, found out it accidentally\nAs I've read GoDaddy does the same thing\n\nNeed to send mail from your Linode? New Linode accounts have ports 25,\n465, and 587 blocked by default. To have these restrictions removed,\nplease review this guide, then open a Support Ticket after creating\nyour Linode.\nIn an effort to fight spam, Linode restricts outbound connections on\nports 25, 465, and 587 on all Linodes for new accounts created after\nNovember 5th, 2019.\n\n","meta":{"link":"https:\/\/stackoverflow.com\/questions\/65141577\/cant-send-email-in-production"},"label":[[0,30,"problem"],[1141,1173,"problem"],[7064,7083,"problem"],[7293,7373,"resolution"]],"Comments":[]}
{"id":388,"text":"How to load local JavaScript libraries in Github Pages?. I recently deployed my website to GitHub Pages - https:\/\/max-stevenson.github.io\/my-year-in-books\/\nI have a two local JavaScript libraries (jQuery and swiped-events) downloaded and within the following directory: src\/public\/js\/lib.\nIn my index.html file at the root directory, I am linking to the two libraries like so:\n<script type=\"text\/javascript\" src=\"\/src\/public\/js\/lib\/jquery-3.4.1.min.js\"><\/script>\n<script src=\"\/src\/public\/js\/lib\/swiped-events.js\"><\/script>\n\nBut when I access the page on GitHub Pages I get the following errors:\nGET https:\/\/max-stevenson.github.io\/src\/public\/js\/lib\/jquery-3.4.1.min.js net::ERR_ABORTED 404 (Not Found)\nGET https:\/\/max-stevenson.github.io\/src\/public\/js\/lib\/swiped-events.js net::ERR_ABORTED 404 (Not Found)\n\nWhen I run a local instance on my machine via a node express server and visit the site at localhost:3000, everything works great.\nCan anyone please advise me where I'm going wrong and how to correctly reference my scripts so that they are loaded in GitHub Pages? Try:\n<script type=\"text\/javascript\" src=\"\/my-year-in-books\/scr\/js\/lib\/jquery-3.4.1.min.js\"><\/script\n\nOr:\n<script type=\"text\/javascript\" src=\".\/scr\/public\/js\/lib\/jquery-3.4.1.min.js\"><\/script\n\n","meta":{"link":"https:\/\/stackoverflow.com\/questions\/60017177\/how-to-load-local-javascript-libraries-in-github-pages"},"label":[[595,805,"problem"],[1075,1169,"resolution"],[1175,1260,"resolution"]],"Comments":[]}
{"id":389,"text":"Problems in configuration\/installation with reticulate [R]. I'm trying to work with reticulate for integrating Python modules inside R and, despite following the tutorial (miniconda is already installed), nothing seems to work properly. I tried with the default way and specifying conda and virtual environments:\n> library(reticulate)\n> py_config()\n# python:         C:\/Users\/juanj\/AppData\/Local\/r-miniconda\/envs\/r-reticulate\/python.exe\n# libpython:      C:\/Users\/juanj\/AppData\/Local\/r-miniconda\/envs\/r-reticulate\/python36.dll\n# pythonhome:     C:\/Users\/juanj\/AppData\/Local\/r-miniconda\/envs\/r-reticulate\n# version:        3.6.12 (default, Dec  9 2020, 00:11:44) [MSC v.1916 64 bit (AMD64)]\n# Architecture:   64bit\n# numpy:          C:\/Users\/juanj\/AppData\/Local\/r-miniconda\/envs\/r-reticulate\/Lib\/site-packages\/numpy\n# numpy_version:  1.19.2\n\n> py_install(\"python-igraph\")\n# Collecting package metadata (current_repodata.json): ...working... done\n# Solving environment: ...working... failed with initial frozen solve. Retrying with flexible solve.\n# Solving environment: ...working... failed with repodata from current_repodata.json, will retry with next repodata source.\n# Collecting package metadata (repodata.json): ...working... done\n# Solving environment: ...working... done\n# \n# ## Package Plan ##\n# \n#   environment location: C:\\Users\\juanj\\AppData\\Local\\r-miniconda\\envs\\r-reticulate\n# \n#   added \/ updated specs:\n#     - python\n#     - python-igraph\n# [...]\n# python-igraph-0.8.3  | 1.4 MB    | ########## | 100% \n# Preparing transaction: ...working... done\n# Verifying transaction: ...working... done\n# Executing transaction: ...working... done\n\n> py_module_available(\"python-igraph\")\n# [1] FALSE\n\n## virtual environment\n> virtualenv_create(\"r-reticulate\")\n# Using Python: C:\/Users\/juanj\/AppData\/Local\/r-miniconda\/envs\/r-reticulate\/python.exe\n# Creating virtual environment \"r-reticulate\" ... Done!\n# Installing packages: \"pip\", \"wheel\", \"setuptools\", \"numpy\"\n# Collecting pip\n# [...]\n# Successfully installed numpy-1.19.5 pip-21.0.1 setuptools-53.0.0 wheel-0.36.2\n# Virtual environment 'r-reticulate' successfully created.\n\n> virtualenv_install(\"r-reticulate\", \"python-igraph\")\n# Using virtual environment \"r-reticulate\" ...\n# Collecting python-igraph\n#   Downloading python_igraph-0.8.3-cp36-cp36m-win_amd64.whl (1.4 MB)\n# Collecting texttable>=1.6.2\n#   Downloading texttable-1.6.3-py2.py3-none-any.whl (10 kB)\n# Installing collected packages: texttable, python-igraph\n# Successfully installed python-igraph-0.8.3 texttable-1.6.3\n\n> py_module_available(\"python-igraph\")\n# [1] FALSE\n\n## conda\n> conda_create(\"r-reticulate.conda\")\n# Collecting package metadata (current_repodata.json): ...working... done\n# Solving environment: ...working... done\n# \n# ## Package Plan ##\n# \n#   environment location: C:\\Users\\juanj\\AppData\\Local\\R-MINI~1\\envs\\r-reticulate.conda\n# [...]\n# Preparing transaction: ...working... done\n# Verifying transaction: ...working... done\n# Executing transaction: ...working... done\n# #\n# # To activate this environment, use\n# #\n# #     $ conda activate r-reticulate.conda\n# #\n# # To deactivate an active environment, use\n# #\n# #     $ conda deactivate\n# \n# [1] \"C:\\\\Users\\\\juanj\\\\AppData\\\\Local\\\\r-miniconda\\\\envs\\\\r-reticulate.conda\\\\python.exe\"\n\n> conda_install(\"r-reticulate.conda\", \"python-igraph\")\n# Collecting package metadata (current_repodata.json): ...working... done\n# Solving environment: ...working... failed with initial frozen solve. Retrying with flexible solve.\n# Solving environment: ...working... failed with repodata from current_repodata.json, will retry with next repodata source.\n# Collecting package metadata (repodata.json): ...working... done\n# Solving environment: ...working... done\n# \n# ## Package Plan ##\n# \n#   environment location: C:\\Users\\juanj\\AppData\\Local\\R-MINI~1\\envs\\r-reticulate.conda\n# [...]\n# python-igraph-0.8.3  | 1.4 MB    | ########## | 100% \n# Preparing transaction: ...working... done\n# Verifying transaction: ...working... done\n# Executing transaction: ...working... done\n\n> py_module_available(\"python-igraph\")\n# [1] FALSE\n\nAny idea what I'm missing, please? Thanks. I forgot to mention that I was working on Windows SO, but I guess this information did not add nothing relevant.\nFinally (and taking into consideration @reverse_engineer's answer) I solved it simply adding pip = T, nothing about configuring a virtual environment:\n> library(reticulate)\n> py_install(\"python-igraph\", pip = T)\n> import(\"igraph\")\n# Module(igraph)\n\n","meta":{"link":"https:\/\/stackoverflow.com\/questions\/66013900\/problems-in-configuration-installation-with-reticulate-r"},"label":[[1656,1704,"problem"],[4434,4470,"resolution"]],"Comments":[]}
{"id":390,"text":"Creating AWS_ALB From Terraform. I am trying to create a simple AWS_ALB via terraform, I have created a separate VPC with public and private subnets but the ALB creating is giving me invalidSubnet error\nresource \"aws_alb\" \"test\" {\n  name            = \"test-alb-tf\"\n  internal        = false\n  security_groups = [\"sg-0385b126e18d3ca67\"]\n  subnets         = [\"192.168.11.0\/24\"]\n}\n\nPublic subnets are created as below\n        {\n            \"MapPublicIpOnLaunch\": true,\n            \"AvailabilityZoneId\": \"use1-az2\",\n            \"Tags\": [\n                {\n                    \"Value\": \"staging\",\n                    \"Key\": \"Environment\"\n                },\n                {\n                    \"Value\": \"secure-public-us-east-1a\",\n                    \"Key\": \"Name\"\n                },\n                {\n                    \"Value\": \"true\",\n                    \"Key\": \"Terraform\"\n                }\n            ],\n            \"AvailableIpAddressCount\": 250,\n            \"DefaultForAz\": false,\n            \"SubnetArn\": \"arn:aws:ec2:us-east-1:041840987519:subnet\/subnet-03757baf0df052c07\",\n            \"Ipv6CidrBlockAssociationSet\": [],\n            \"VpcId\": \"vpc-00689d5ff034a0c99\",\n            \"State\": \"available\",\n            \"AvailabilityZone\": \"us-east-1a\",\n            \"SubnetId\": \"subnet-03757baf0df052c07\",\n            \"OwnerId\": \"041840987519\",\n            \"CidrBlock\": \"192.168.11.0\/24\",\n            \"AssignIpv6AddressOnCreation\": false\n        },\n\nThis is what terraform plan show\n  # aws_alb.test will be created\n  + resource \"aws_alb\" \"test\" {\n      + arn                        = (known after apply)\n      + arn_suffix                 = (known after apply)\n      + dns_name                   = (known after apply)\n      + enable_deletion_protection = false\n      + enable_http2               = true\n      + id                         = (known after apply)\n      + idle_timeout               = 60\n      + internal                   = false\n      + ip_address_type            = (known after apply)\n      + load_balancer_type         = \"application\"\n      + name                       = \"test-alb-tf\"\n      + security_groups            = [\n          + \"sg-0385b126e18d3ca67\",\n        ]\n      + subnets                    = [\n          + \"192.168.11.0\/24\",\n        ]\n      + vpc_id                     = (known after apply)\n      + zone_id                    = (known after apply)\n\n      + subnet_mapping {\n          + allocation_id = (known after apply)\n          + subnet_id     = (known after apply)\n        }\n    }\n\nand this is the error that I am getting\nError: Error creating application Load Balancer: InvalidSubnet: The subnet ID '192.168.11.0\/24' is not valid\n    status code: 400, request id: 5521ae30-dc0b-4f4c-ae89-47eb729126c7\n\nAnyone got an idea as to what i am missing So the error clearly shows the problem. \nThe subnet ID '192.168.11.0\/24' is not valid\n\nDid you login the aws console and check what subnet you want to assign to this ALB?\nit has the id as subnet-xxxxxxxx\n","meta":{"link":"https:\/\/stackoverflow.com\/questions\/59858401\/creating-aws-alb-from-terraform"},"label":[[2568,2623,"problem"],[2880,2988,"resolution"]],"Comments":[]}
{"id":391,"text":"Azure Devops pipeline call separate e2e repo between ci and dev deployment stages. I have a yaml build pipeline for a project that has multiple stages, it runs build_and_test then if successful it runs ci_deployment, then dev_deployment etc. I want to add a stage between CI and DEV that runs a seperate repos pipeline, in this case its a ReadyApi repo that will run api tests against the CI environment so if it fails we block the build from proceeding to DEV.\nI have got the readyApi pipeline to run the tests against the environment when I run that pipeline but I don't know how to go about tying it into a middle stage of my other pipeline.\nSo my question is how do I write the stage to run a seperate pipeline, very new to working with yaml so any help with this or resources that could help me to understand would be greatly appreciated.\nReadyApi pipeline:\n---\ntrigger:\n  batch: false\n  branches:\n    include:\n    - trunk\npool:\n  name: OnPrem TestAgents\n  demands: TestRunner -equals ReadyAPI\n\nsteps:\n- script: |\n   powershell 1 | \"C:\\Program Files (x86)\\Java\\jre1.8.0_241\\bin\\java.exe\" -jar \"C:\\ready-api-license-manager\\ready-api-license-manager-1.3.2.jar\" -s licenseServerUrl\n   echo alma\n  displayName: 'Command Line Script'\n\n\n- task: SoapUIProForAzureDevOpsTask@1\n  displayName: 'SoapUI Pro for Azure DevOps'\n  inputs:\n    project: 'ReadyAPI'\n    testSuite: API\n    projectPassword: 'unencryptkey'\n\n\n- task: PublishTestResults@2\n  displayName: 'Publish Test Results **\/*.xml'\n  inputs:\n    testResultsFiles: '**\/*.xml'\n    searchFolder: '$(Common.TestResultsDirectory)'\n    mergeTestResults: true\n    failTaskOnFailedTests: true\n\n\nMain build pipeline I want to update:\n---\ntrigger:\n  batch: false\n  branches:\n    include:\n    - trunk\npool: \"poolName\"\n\nvariables:\n  buildMajor: 1\n  buildMinor: 3\nstages:\n- stage: build_and_test\n  displayName: Build and Test\n  variables:\n    azureResourceGroup: \n    azureInfrastructureStateStorageAccount: \n    environmentName: ci\n  jobs:\n  - job: build_image\n  - job: run_unit_test\n  - job: run_component_test\n  - job: build_and_push_container\n   \n- stage: ci_deployment\n  variables:\n    deployment_name: $(serviceName)\n    namespace: ci\n    environmentName: ci\n    buildid: $(buildVersion).$(Build.BuildNumber).$(Build.SourceVersion)\n    minReplicas: 1\n    maxReplicas: 5\n  dependsOn: build_and_test\n  jobs:\n  - deployment: deploy_to_ci\n    environment: ci\n    displayName: Deploy to ci\n    strategy:\n      runOnce:\n        deploy:\n          steps:\n          - template: pipelines\/azure-pipelines-deploy.yaml\n            parameters:\n              environmentName: $(environmentName)\n              aadPodIdentityName: $(aadPodIdentityName)\n              appConfigName: $(appConfigName)\n              keyVaultName: $(keyVaultName)\n- stage: dev_deployment\n  variables:\n    deployment_name: $(serviceName)\n    namespace: dev\n    environmentName: dev\n    buildid: $(buildVersion).$(Build.BuildNumber).$(Build.SourceVersion)\n    minReplicas: 3\n    maxReplicas: 10\n  dependsOn: ci_deployment\n  jobs:\n  - deployment: deploy_to_dev\n    environment: dev\n    displayName: Deploy to dev\n    strategy:\n      runOnce:\n        deploy:\n          steps:\n          - template: pipelines\/azure-pipelines-deploy.yaml\n            parameters:\n              environmentName: $(environmentName)\n              aadPodIdentityName: $(aadPodIdentityName)\n              appConfigName: $(appConfigName)\n              keyVaultName: $(keyVaultName) If you want to block proceeding original pipeline I would recommend you to use this extension Trigger Build Task\nYou may define it here trigger in a stage between ci_deployment and dev_deployment so it will be waiting for sucessfull run of your tests:\n- stage: ci_tests\n  dependsOn: ci_deployment\n  jobs:\n  - job:\n    steps:\n    - task: TriggerBuild@3\n      displayName: 'Trigger a new build of Validate-BuildVariable Update'\n      inputs:\n        buildDefinition: 'Your build name'\n        useSameBranch: false\n        branchToUse: master\n        waitForQueuedBuildsToFinish: true\n        authenticationMethod: 'OAuth Token'\n        password: $(System.AccessToken)\n- stage: dev_deployment\n  .....\n  dependsOn: ci_tests\n\n","meta":{"link":"https:\/\/stackoverflow.com\/questions\/68193498\/azure-devops-pipeline-call-separate-e2e-repo-between-ci-and-dev-deployment-stage"},"label":[[252,318,"problem"],[3538,3575,"resolution"],[3584,3659,"resolution"]],"Comments":[]}
{"id":392,"text":"Move code INTO Azure DevOps Repo FROM TFS WITHOUT losing TFS HISTORY. In our whole company we are the first one trying this.\nWe are trying to get the code from TFS into Azure DevOps.\nBelow is TFS details\nTFS Server: companyTFS\n\nTFS Project Collection: TechTeam. \n\nTFS Project name: Main.\n\nTFS Branch : Dev\n\nNow I am trying to get code from TFS into Azure DevOps. We could have just copied the code from TFS folder into Azure DevOps folder but we do not want to lose the TFS history.\nAs described in below I installed Chocolatey and also installed gittfs.\nhttps:\/\/blog.rsuter.com\/migrate-a-tfs-repository-to-a-vsts-git-repository\nWhen I give below command\ngit tfs clone http:\/\/companyTFS:8080\/TechTeam\/Main $\/Main\/Dev\/Registration\/FeeDetails . –ignore-branches –debug\n\nI am getting below exception.\nTF31002: Unable to connect to this Team Foundation Server: http:\/\/companyTFS:8080\/TechTeam\/Main.\nTeam Foundation Server Url: http:\/\/companyTFS:8080\/TechTeam\/Main.\n\nPossible reasons for failure include:\n- The name, port number, or protocol for the Team Foundation Server is incorrect.\n- The Team Foundation Server is offline.\n- The password has expired or is incorrect.\n\nTechnical information (for administrator):\nThe remote server returned an error: (404) Not Found.\nThe remote server returned an error: (404) Not Found.\n\nI also tried this but did not work.\nhttps:\/\/github.com\/microsoft\/azure-repos-vscode\/issues\/320#issuecomment-335573266\nI am trying to find out whether my TFS server can do handshaking with external components.\nI would be glad if someone can tell me what I am doing wrong. At last after spending almost 4 days, I am able to move the code from TFS into Azure DevOps along with history. Please see below all the steps I did.\nInstall Chocolatey:\nFirst we need to install Chocolatey. To make matters simple I created below two files in same folder.\nFileName : installChocolatey.cmd\nContent:\n@echo off\nSET DIR=%~dp0%\n\n%systemroot%\\System32\\WindowsPowerShell\\v1.0\\powershell.exe -NoProfile -ExecutionPolicy Bypass -Command \"& '%DIR%install.ps1' %*\"\nSET PATH=%PATH%;%ALLUSERSPROFILE%\\chocolatey\\bin\n\nFileName: install.ps1\nContent: Copy paste the content from here https:\/\/chocolatey.org\/install.ps1\nOpen command prompt in admin mode and ran installChocolatey.cmd to install Chocolatey.\nIn command prompt give choco -v to see whether it is installed correctly.\nInstall git tfs:\nNext, we need to install git tfs tool in order to do that give below command in command prompt and follow the instructions.\nchoco install gittfs\n\nIn my case it installed to C:\\Tools\\gittfs.\nOpen Environment Variables and make sure you add below (in my case )to PATH variable.\nC:\\Tools\\gittfs\nIn command prompt give git tfs -version to see what version it installed.\nMove code from TFS to Azure DevOps:\nAzure DevOps Project Name: Experiment\nAzure DevOps Repo Name: MyRepo\nAzure DevOps Repo Name: Master and DEV\n\nFirst, I clone this repo to my local. Made sure that I have all remoted branches onto my local.\nIn my local created a new branch name called TestBranch and pushed it to remote. So now MyRepo has 3 branches and TestBranch is my local working branch.\nIn command prompt, I went to folder where I mapped Azure DevOps repo. I gave below command to get TFS code with history.\ngit tfs clone http:\/\/companyTFS:8080\/TechTeam $\/Main\/Dev\/Registration\/FeeDetails . –debug\n\nThis will take sometime and after that you can see that TFS code is downloaded to that folder.\nIn same command prompt,\ngive below command to make sure what is your working branch\ngit branch \n\nBelow command will create a branch\ngit checkout -b TempBranch\n\nFor below command get url of your target Azure DevOps repo.\nBelow command will set your remote Azure DevOps Repo as Repo where you want to target\ngit remote add origin https:\/\/xxx@dev.azure.com\/xxx\/Experiment\/_git\/MyRepo\n\nBelow will create new branch called TempBranch in your target repo in ADO. Here in Experiment project and MyRepo repo\ngit push --set-upstream origin TempBranch\n\nNow all your code and history from TFS made its way into Azure DevOps into your Repo. You can use PULL Request feature to merge into other branches and so on.\nNow lets say you want your Azure DevOps code structure to be different.\nGo to folder where you downloaded code\/history from TFS and create folder structure or changes the way you want\nGiive below command one by one.\ngit add .\n\nTo see all your changes give below command\ngit status\n\nTo commit all your changes in local\ngit commit -m \"[commit message goes here]\"\n\nCommand to push the changes from local to remote\ngit push\nThe code will be in Experiment project MyRepo repo TempBranch branch\n","meta":{"link":"https:\/\/stackoverflow.com\/questions\/62764145\/move-code-into-azure-devops-repo-from-tfs-without-losing-tfs-history"},"label":[[807,855,"problem"],[1741,1759,"resolution"],[2371,2386,"resolution"],[2578,2679,"resolution"],[2754,2789,"resolution"],[2908,3074,"resolution"],[3149,3216,"resolution"],[3269,3358,"resolution"],[3455,3478,"resolution"],[3539,3550,"resolution"],[3587,3613,"resolution"],[3633,3673,"resolution"],[3761,3835,"resolution"],[3955,3996,"resolution"]],"Comments":[]}
{"id":393,"text":"Azure DevOps - Publish pipeline task throwing Http timeout exception. I am getting Http timeout exception while using Azure DevOps - PublishPipelineArtifact task.\nThis task was working perfect till yesterday morning but suddenly getting error all night after trying for multiple attempts also.\nCan someone help please ?\nTask used below:\nversion:1.2.3\nPublish File size: Few bytes only\n- publish: $(System.DefaultWorkingDirectory)\/bin\/WebApp\n  artifact: WebApp\n\nException:\nUploading pipeline artifact from \/home\/vsts\/work\/1\/s\/coverage for build #1773\n\nInformation, ApplicationInsightsTelemetrySender will correlate events with X-TFS-Session 71dd2ca5-fba3-42dc-bcae-5d90c107c066\n\nInformation, DedupManifestArtifactClient will correlate http requests with X-TFS-Session 71dd2ca5-fba3-42dc-bcae-5d90c107c066\n\nInformation, 1 files to be processed in 1 groups.\n\nInformation, 1 out of 1 files processed (Group: 1\/1)\n\nInformation, Processed 1 files from \/home\/vsts\/work\/1\/s\/coverage successfully.\n\nInformation, Uploading 1 files from: \/home\/vsts\/work\/1\/s\/coverage\n\nInformation, Uploaded 0.0 MB out of 0.3 MB.\n\nInformation, Uploaded 0.0 MB out of 0.3 MB.\n\nInformation, Uploaded 0.0 MB out of 0.3 MB.\n\n\nInformation, Uploaded 0.0 MB out of 0.3 MB.\n\nInformation, Uploaded 0.0 MB out of 0.3 MB.\n\nInformation, ArtifactHttpRetryMessageHandler.SendAsync: https:\/\/vsblobprodsu6weu.vsblob.visualstudio.com\/A1b625a1a-423a-4874-bccf-3e37eef5432a\/_apis\/dedup\/nodes\/A21A6CD7D48BAA32CF50EB0E9E180F56C7472AC5F075DFD53C4483EA4BB8A88A02 attempt 1\/6 failed with TimeoutException: 'The HTTP request timed out after 00:00:50.'\n\nInformation, Uploaded 0.0 MB out of 0.3 MB.\n\nInformation, Uploaded 0.0 MB out of 0.3 MB. The issue was caused by this accident in UK South region. Now is fine.\n","meta":{"link":"https:\/\/stackoverflow.com\/questions\/63897433\/azure-devops-publish-pipeline-task-throwing-http-timeout-exception"},"label":[[0,68,"problem"],[1702,1744,"problem"]],"Comments":[]}
{"id":394,"text":"GHC incompatibility installing haskell-src-exts via cabal. I'm running into a compatibility problem trying to cabal install agda using GHC 7.8.3 and Cabal 1.16.0.2, on Ubuntu 14.04. \nThe problem appears to be with haskell-src-exts-1.15.0.1, which Agda requires. Compiling that library runs into the following well-known problem caused (I believe) by Happy-generated code being incompatible with a change to GHC in version 7.8.\ntemplates\/GenericTemplate.hs:104:22:\n    Couldn't match expected type ‘Bool’\n                with actual type ‘Happy_GHC_Exts.Int#’\n    In the expression:\n      (n Happy_GHC_Exts.<# (0# :: Happy_GHC_Exts.Int#))\n    In a stmt of a pattern guard for\n                   a case alternative:\n      (n Happy_GHC_Exts.<# (0# :: Happy_GHC_Exts.Int#))\n    In a case alternative:\n        n | (n Happy_GHC_Exts.<# (0# :: Happy_GHC_Exts.Int#))\n          -> (happyReduceArr Happy_Data_Array.! rule) i tk st\n          where\n              rule\n                = (Happy_GHC_Exts.I#\n                     ((Happy_GHC_Exts.negateInt#\n                         ((n Happy_GHC_Exts.+# (1# :: Happy_GHC_Exts.Int#))))))\n\nThe article mentioned above describes how to clean and rebuild a library that contains incompatible Happy-generated code. However, I couldn't make this work for haskell-src-exts-1.15.0.1. In particular, trying\ncabal unpack haskell-src-exts-1.15.0.1 \ncd haskell-src-exts-1.15.0.1\ncabal clean\ncabal install\n\nruns into the same problem.\nAny suggestions? Short answer: Cabal 1.16.* is incompatible with GHC 7.8.3, so install a recent version of cabal-install (e.g. version 1.20.0.3), delete the directory associated to GHC 7.8.3 from ~\/.ghc (e.g. x86_64-linux-7.8.3) and install everything again.\nLong answer: See GHC issue 9060.\n","meta":{"link":"https:\/\/stackoverflow.com\/questions\/25104367\/ghc-incompatibility-installing-haskell-src-exts-via-cabal"},"label":[[78,182,"problem"],[1488,1531,"problem"],[1536,1715,"resolution"]],"Comments":[]}
{"id":395,"text":"Why would a special ansible variable be undefined?. I have been working recently on an anible script for CD and have began to run into an issue. When I run the most recent YAML file I get an error \nfatal: [Windows-AWS]: FAILED! => {\"msg\": \"The task includes an option with an undefined variable. The error was: 'ansible_play_name' is undefined\\n\\nThe error appears to have been in 'x.yml': line 32, column 3, but may\\nbe elsewhere in the file depending on the exact syntax problem.\\n\\nThe offending line appears to be:\\n\\n\\n- name: Add RabbitMQ user\\n ^ here\\n\"}\n\nThe variable in question in contained in the code like this:\n- name: Add RabbitMQ user\n  win_shell: |\n    Set-Location \"C:\\Program Files\\RabbitMQ Server\\rabbitmq_server-{{rabbitmq_version}}\\sbin\"\n    $users = .\\rabbitmqctl.bat list_users\n\n    if($users -Like \"*{{ansible_play_name}}*\")\n    {\n      Write-Host \"Skipping: '{{ansible_play_name}}' already exists.\"\n    }\n    else\n    {\n      .\\rabbitmqctl.bat add_user {{ansible_play_name}} \"{{rabbitmq_password}}\"\n      .\\rabbitmqctl.bat set_permissions {{ansible_play_name}} \".*\" \".*\" \".*\"\n      .\\rabbitmqctl.bat set_user_tags {{ansible_play_name}} administrator\n    }\n  register: rabbit_add_user_out\n  changed_when: '\"Adding user\" in rabbit_add_user_out.stdout'\n  failed_when: '\"Error\" in rabbit_add_user_out.stderr'\n\nAnyone have any suggestions as to why it wouldn't be able to pick up this value? The playbook it is running from has fact gathering set to true as well so it should be assigned ansible_play_name - (The name of the currently executed play) is Added in 2.8. May be you have a lower version.\n","meta":{"link":"https:\/\/stackoverflow.com\/questions\/58877276\/why-would-a-special-ansible-variable-be-undefined"},"label":[[310,343,"problem"],[1509,1620,"problem"]],"Comments":[]}
{"id":396,"text":"Azure DevOps Build Pipeline - Unable to access $(Build.BuildNumber) in Script. In my build pipeline I am doing the following:\n\nRestore from Git\nPowershell Script - to retrieve the Build\nnumber and write that to a json file BEFORE.... \nBuild solution \nArchive Files\nPublish Artifact.\n\nIn step 2 the Powershell script is pretty simple:\nDEFINED ENV VARIABLES:\nName: buildNumber  Value: $(Build.BuildNumber)\nName: rootPath Value:$(Build.ArtifactStagingDirectory)\n\nCODE:\n$theFile = Get-ChildItem -Path $rootPath -Recurse -Filter \"host.json\" | Select-Object -First 1\n$propertyName = \"BuildNumber\"\n\nif($theFile)\n{\n    $json = Get-Content \"$theFile\" | Out-String | ConvertFrom-Json\n    if($json.$propertyName)\n    { \n        $json.$propertyName = $buildNumber\n    }else{    \n        Add-Member -InputObject $json -MemberType NoteProperty -Name $propertyName -Value $buildNumber\n    }\n    $json | ConvertTo-Json -depth 100 | Out-File \"$theFile\"\n\n}\nelse\n{\n    Write-Warning \"Found no files.\"\n}\n\nFor some reason my $buildNumber is coming back null.  The $rootPath is working.\nAm I not able to access the $(Build.BuildNumber) outside the build step?  The build number format is defined in the Options for the Pipeline and it works fine when stamping the build, but I am unable to access it in my powershell script.\nAny Thoughts? Use $env:BUILD_BUILDNUMBER instead of the $(...) notation.\nSee the different notations for different script types in the docs.\n","meta":{"link":"https:\/\/stackoverflow.com\/questions\/52899564\/azure-devops-build-pipeline-unable-to-access-build-buildnumber-in-script"},"label":[[0,77,"problem"],[1317,1375,"resolution"]],"Comments":[]}
{"id":595,"text":"unable to check in or see files in source control explorer. I am using Visual Studio 2015 professional, our source control is on Visual Studio Team Services. I can make the initial connection to our server, but when I'm trying to check in changes I get \n\"TF400324: Team Foundation services are not available from server\"\n\nAlso, when I'm trying to view files in the Source Control Explorer after a minute or so it writes \"Working...\" it writes at the tab its disconnected. When my colleague tried to connect from her computer everything was fine, I was also able to connect with my own credentials from a remote machine. Things I've tried:\n\ndeleting files from the TFS cache\nupdating Visual Studio from update 2 to update 3\ninstalling windows updates\ndisabling my antivirus\nlogging in with a different account\nrestarting my computer and renewing my ip\n\nI'm both lost and losing it. Any help would be highly appreciated. Thanks Well, restarting my router helped fixed it. that is very strange because the internet was working fine (I could post a question for SO for example) except I had some issues with Skype. I guess something in that pleased the god of version controls or something.\n","meta":{"link":"https:\/\/stackoverflow.com\/questions\/40470182\/unable-to-check-in-or-see-files-in-source-control-explorer"},"label":[[211,320,"problem"],[328,470,"problem"],[932,952,"resolution"]],"Comments":[]}
{"id":596,"text":"How do I set an environment variable for Ansible itself not using shell?. I would like to keep tmp directory on the VM in my test region. There is the following solution for the problem: setting ANSIBLE_KEEP_REMOTE_FILES to 1 on the Ansible machine. \nThe issue is that the ansible machine is a local docker container so I need to ensure that this variable is always set. Otherwise I'm loosing some documents. When I reboot my system and start this docker container with Ansible I'm loosing this variable. \nIs there a way to set this environment variable somewhere in Ansible configuration = or in a playbook configuration somewhere? I need a permanent solution in order not to forget this variable.\nThank you! \nQ: \"Is there a way to set this environment variable somewhere in Ansible configuration?\"\n\nA: Yes. It is. For example\n$ cat ansible.cfg \n[defaults]\nkeep_remote_files = true\n\nSee DEFAULT_KEEP_REMOTE_FILES.\n","meta":{"link":"https:\/\/stackoverflow.com\/questions\/60035275\/how-do-i-set-an-environment-variable-for-ansible-itself-not-using-shell"},"label":[[380,505,"problem"],[858,882,"resolution"]],"Comments":[]}
{"id":597,"text":"The service connection does not exist or has not been authorized for use. I'm trying to create my first release pipeline, however I keep getting this error:\n\nException Message: The pipeline is not valid. Job Phase_1: Step AzureResourceGroupDeployment input ConnectedServiceName references service connection \n  which could not be found. The service connection does not exist or has not been authorized for use. For authorization details, \n  refer to https:\/\/aka.ms\/yamlauthz. (type PipelineValidationException)\n\nI've tried to follow the instructions in the link, however the \"Authorize Resources\" button does not exist.\n\"Allow all pipelines to use this service connection\" is already enabled and I have recreated the deployment task after enabling this.\nHow do I authorise the resource? My \"Service connection\" which defined the service principal connection had been created separately to the task in my release pipeline.\nIn order for \"Authorize Resources\" to occur, you must create a new connection from the task itself (you may need to use the advanced options to add an existing service principal).\n\nunder \"Azure subscription\" click the name of the subscription you wish to use\nClick the drop down next to \"Authorize\" and open advanced options\nClick \" use the full version of the service connection dialog.\"\nEnter all your credentials and hit save\n\n","meta":{"link":"https:\/\/stackoverflow.com\/questions\/58285661\/the-service-connection-does-not-exist-or-has-not-been-authorized-for-use"},"label":[[177,409,"problem"],[790,921,"problem"],[976,1021,"resolution"],[1038,1099,"resolution"]],"Comments":[]}
{"id":599,"text":"Rails installing mysql - Error installing mysql2: ERROR: Failed to build gem native extension. I have a Rails 3.2.13 app that I would like deployed, but the hosting service would require MySQL, as opposed to sqlite which I have been using.\nIn the process of converting from sqlite to MySQL, I had to install the ruby gem mysql2, which is giving me the following error upon installing:\n\nError installing mysql2: ERROR: Failed to build gem native extension.\n\nI've tried both bundle install, as well as gem install mysql2, but the same error message appeared.\nI am aware that the solution that has worked for many is sudo apt-get install libmysql-ruby libmysqlclient-dev, but I'm using Git Bash on Windows, so I found a Windows equivalent (@francois's answer on this question). I installed MySQL Server 5.6 with the installer, and ran the following command:\ngem install mysql2 -- '--with-mysql-lib=\"c:\\Program Files\\MySQL\\MySQL Server 5.6\\lib\" --with-mysql-include=\"c:\\Program Files\\MySQL\\MySQL Server 5.6\\include\"' \n\nFrom what I have read, this is supposed to successfully install the 'mysql2' ruby gem.  But strangely enough, I still had the following error:\n\n    Temporarily enhancing PATH to include DevKit... Building native extensions with: '--with-mysql-lib=\"c:\\Program\n\nFiles\\MySQL\\MySQL  Server 5.6\\lib\" --with-mysql-include=\"c:\\Program\n  Files\\MySQL\\MySQL Server 5.6\\i nclude\"' This could take a while...\n  ERROR:  Error installing mysql2:\n                  ERROR: Failed to build gem native extension.\n        c:\/RailsInstaller\/Ruby1.9.3\/bin\/ruby.exe extconf.rb --with-mysql-lib=\"c:\\Pro gram Files\\MySQL\\MySQL Server 5.6\\lib\" --with-mysql-include=\"c:\\Program Files\\My SQL\\MySQL Server 5.6\\include\" checking for rb_thread_blocking_region()... yes checking for rb_wait_for_single_fd()... yes checking for rb_hash_dup()... yes\n\nchecking for rb_intern3()... yes checking for main() in -llibmysql...\n  yes checking for mysql.h... yes checking for errmsg.h... yes checking\n  for mysqld_error.h... yes creating Makefile\n    make generating mysql2-i386-mingw32.def compiling client.c client.c: In function 'rb_raise_mysql2_error': client.c:139:3:\n\nwarning: ISO C90 forbids mixed declarations and code client.c: In\n  function 'finish_and_mark_inactive': client.c:508:3: warning: ISO C90\n  forbids mixed declarations and code client.c: In function\n  'rb_mysql_client_abandon_results': client.c:535:3: warning: ISO C90\n  forbids mixed declarations and code client.c: In function\n  'rb_mysql_client_next_result': client.c:938:5: warning: ISO C90\n  forbids mixed declarations and code compiling mysql2_ext.c compiling\n  result.c result.c: In function 'rb_mysql_result_fetch_fields':\n  result.c:407:35: warning: comparison between signed and unsigned\n  integer express ions linking shared-object mysql2\/mysql2.so client.o:\n  In function nogvl_connect':\n  c:\\RailsInstaller\\Ruby1.9.3\\lib\\ruby\\gems\\1.9.1\\gems\\mysql2-0.3.13\\ext\\mysql2\/cl\n  ient.c:158: undefined reference tomysql_real_connect@32' client.o:\n  In function nogvl_init':\n  c:\\RailsInstaller\\Ruby1.9.3\\lib\\ruby\\gems\\1.9.1\\gems\\mysql2-0.3.13\\ext\\mysql2\/cl\n  ient.c:150: undefined reference tomysql_init@4' client.o: In\n  function set_ssl_options':\n  c:\\RailsInstaller\\Ruby1.9.3\\lib\\ruby\\gems\\1.9.1\\gems\\mysql2-0.3.13\\ext\\mysql2\/cl\n  ient.c:1078: undefined reference tomysql_ssl_set@24' client.o: In\n  function mysql_client_options':\n  c:\\RailsInstaller\\Ruby1.9.3\\lib\\ruby\\gems\\1.9.1\\gems\\mysql2-0.3.13\\ext\\mysql2\/cl\n  ient.c:723: undefined reference tomysql_options@12'\n  c:\\RailsInstaller\\Ruby1.9.3\\lib\\ruby\\gems\\1.9.1\\gems\\mysql2-0.3.13\\ext\\mysql2\/cl\n  ient.c:727: undefined reference to mysql_error@4' client.o: In\n  functionrb_mysql_info':\n  c:\\RailsInstaller\\Ruby1.9.3\\lib\\ruby\\gems\\1.9.1\\gems\\mysql2-0.3.13\\ext\\mysql2\/cl\n  ient.c:267: undefined reference to mysql_info@4' client.o: In\n  functionrb_mysql_client_warning_count':\n  c:\\RailsInstaller\\Ruby1.9.3\\lib\\ruby\\gems\\1.9.1\\gems\\mysql2-0.3.13\\ext\\mysql2\/cl\n  ient.c:257: undefined reference to mysql_warning_count@4' client.o:\n  In functionnogvl_do_result':\n  c:\\RailsInstaller\\Ruby1.9.3\\lib\\ruby\\gems\\1.9.1\\gems\\mysql2-0.3.13\\ext\\mysql2\/cl\n  ient.c:370: undefined reference to mysql_store_result@4' client.o: In\n  functionrb_mysql_client_more_results':\n  c:\\RailsInstaller\\Ruby1.9.3\\lib\\ruby\\gems\\1.9.1\\gems\\mysql2-0.3.13\\ext\\mysql2\/cl\n  ient.c:923: undefined reference to mysql_more_results@4' client.o: In\n  functionnogvl_select_db':\n  c:\\RailsInstaller\\Ruby1.9.3\\lib\\ruby\\gems\\1.9.1\\gems\\mysql2-0.3.13\\ext\\mysql2\/cl\n  ient.c:863: undefined reference to mysql_select_db@8' client.o: In\n  functionnogvl_ping':\n  c:\\RailsInstaller\\Ruby1.9.3\\lib\\ruby\\gems\\1.9.1\\gems\\mysql2-0.3.13\\ext\\mysql2\/cl\n  ient.c:894: undefined reference to mysql_ping@4' client.o: In\n  functionrb_mysql_client_thread_id':\n  c:\\RailsInstaller\\Ruby1.9.3\\lib\\ruby\\gems\\1.9.1\\gems\\mysql2-0.3.13\\ext\\mysql2\/cl\n  ient.c:856: undefined reference to mysql_thread_id@4' client.o: In\n  functionrb_mysql_client_last_id':\n  c:\\RailsInstaller\\Ruby1.9.3\\lib\\ruby\\gems\\1.9.1\\gems\\mysql2-0.3.13\\ext\\mysql2\/cl\n  ient.c:825: undefined reference to mysql_insert_id@4' client.o: In\n  functionnogvl_read_query_result':\n  c:\\RailsInstaller\\Ruby1.9.3\\lib\\ruby\\gems\\1.9.1\\gems\\mysql2-0.3.13\\ext\\mysql2\/cl\n  ient.c:357: undefined reference to mysql_read_query_result@4'\n  client.o: In functionrb_mysql_client_server_info':\n  c:\\RailsInstaller\\Ruby1.9.3\\lib\\ruby\\gems\\1.9.1\\gems\\mysql2-0.3.13\\ext\\mysql2\/cl\n  ient.c:788: undefined reference to mysql_get_server_version@4'\n  c:\\RailsInstaller\\Ruby1.9.3\\lib\\ruby\\gems\\1.9.1\\gems\\mysql2-0.3.13\\ext\\mysql2\/cl\n  ient.c:789: undefined reference tomysql_get_server_info@4' client.o:\n  In function rb_mysql_client_info':\n  c:\\RailsInstaller\\Ruby1.9.3\\lib\\ruby\\gems\\1.9.1\\gems\\mysql2-0.3.13\\ext\\mysql2\/cl\n  ient.c:756: undefined reference tomysql_get_client_version@0'\n  c:\\RailsInstaller\\Ruby1.9.3\\lib\\ruby\\gems\\1.9.1\\gems\\mysql2-0.3.13\\ext\\mysql2\/cl\n  ient.c:757: undefined reference to mysql_get_client_info@0' client.o:\n  In functionrb_mysql_client_real_escape':\n  c:\\RailsInstaller\\Ruby1.9.3\\lib\\ruby\\gems\\1.9.1\\gems\\mysql2-0.3.13\\ext\\mysql2\/cl\n  ient.c:662: undefined reference to mysql_real_escape_string@16'\n  client.o: In functionfinish_and_mark_inactive':\n  c:\\RailsInstaller\\Ruby1.9.3\\lib\\ruby\\gems\\1.9.1\\gems\\mysql2-0.3.13\\ext\\mysql2\/cl\n  ient.c:515: undefined reference to mysql_free_result@4' client.o: In\n  functionnogvl_send_query':\n  c:\\RailsInstaller\\Ruby1.9.3\\lib\\ruby\\gems\\1.9.1\\gems\\mysql2-0.3.13\\ext\\mysql2\/cl\n  ient.c:334: undefined reference to mysql_send_query@12' client.o: In\n  functionnogvl_close':\n  c:\\RailsInstaller\\Ruby1.9.3\\lib\\ruby\\gems\\1.9.1\\gems\\mysql2-0.3.13\\ext\\mysql2\/cl\n  ient.c:190: undefined reference to mysql_close@4' client.o: In\n  functionrb_mysql_client_escape':\n  c:\\RailsInstaller\\Ruby1.9.3\\lib\\ruby\\gems\\1.9.1\\gems\\mysql2-0.3.13\\ext\\mysql2\/cl\n  ient.c:238: undefined reference to mysql_escape_string@12' client.o:\n  In functionrb_raise_mysql2_error':\n  c:\\RailsInstaller\\Ruby1.9.3\\lib\\ruby\\gems\\1.9.1\\gems\\mysql2-0.3.13\\ext\\mysql2\/cl\n  ient.c:125: undefined reference to mysql_error@4'\n  c:\\RailsInstaller\\Ruby1.9.3\\lib\\ruby\\gems\\1.9.1\\gems\\mysql2-0.3.13\\ext\\mysql2\/cl\n  ient.c:126: undefined reference tomysql_sqlstate@4'\n  c:\\RailsInstaller\\Ruby1.9.3\\lib\\ruby\\gems\\1.9.1\\gems\\mysql2-0.3.13\\ext\\mysql2\/cl\n  ient.c:140: undefined reference to mysql_errno@4' client.o: In\n  functionrb_connect':\n  c:\\RailsInstaller\\Ruby1.9.3\\lib\\ruby\\gems\\1.9.1\\gems\\mysql2-0.3.13\\ext\\mysql2\/cl\n  ient.c:297: undefined reference to mysql_errno@4' client.o: In\n  functionrb_mysql_client_store_result':\n  c:\\RailsInstaller\\Ruby1.9.3\\lib\\ruby\\gems\\1.9.1\\gems\\mysql2-0.3.13\\ext\\mysql2\/cl\n  ient.c:966: undefined reference to mysql_errno@4' client.o: In\n  functionrb_mysql_client_next_result':\n  c:\\RailsInstaller\\Ruby1.9.3\\lib\\ruby\\gems\\1.9.1\\gems\\mysql2-0.3.13\\ext\\mysql2\/cl\n  ient.c:939: undefined reference to mysql_next_result@4' client.o: In\n  functionrb_mysql_client_affected_rows':\n  c:\\RailsInstaller\\Ruby1.9.3\\lib\\ruby\\gems\\1.9.1\\gems\\mysql2-0.3.13\\ext\\mysql2\/cl\n  ient.c:839: undefined reference to mysql_affected_rows@4' client.o:\n  In functionrb_mysql_client_async_result':\n  c:\\RailsInstaller\\Ruby1.9.3\\lib\\ruby\\gems\\1.9.1\\gems\\mysql2-0.3.13\\ext\\mysql2\/cl\n  ient.c:419: undefined reference to mysql_errno@4' client.o: In\n  functionrb_mysql_client_abandon_results':\n  c:\\RailsInstaller\\Ruby1.9.3\\lib\\ruby\\gems\\1.9.1\\gems\\mysql2-0.3.13\\ext\\mysql2\/cl\n  ient.c:539: undefined reference to mysql_next_result@4'\n  c:\\RailsInstaller\\Ruby1.9.3\\lib\\ruby\\gems\\1.9.1\\gems\\mysql2-0.3.13\\ext\\mysql2\/cl\n  ient.c:538: undefined reference tomysql_more_results@4'\n  c:\\RailsInstaller\\Ruby1.9.3\\lib\\ruby\\gems\\1.9.1\\gems\\mysql2-0.3.13\\ext\\mysql2\/cl\n  ient.c:547: undefined reference to mysql_free_result@4' client.o: In\n  functionnogvl_do_result':\n  c:\\RailsInstaller\\Ruby1.9.3\\lib\\ruby\\gems\\1.9.1\\gems\\mysql2-0.3.13\\ext\\mysql2\/cl\n  ient.c:368: undefined reference to mysql_use_result@4' client.o: In\n  functionnogvl_close':\n  c:\\RailsInstaller\\Ruby1.9.3\\lib\\ruby\\gems\\1.9.1\\gems\\mysql2-0.3.13\\ext\\mysql2\/cl\n  ient.c:190: undefined reference to mysql_close@4' client.o: In\n  functionset_charset_name':\n  c:\\RailsInstaller\\Ruby1.9.3\\lib\\ruby\\gems\\1.9.1\\gems\\mysql2-0.3.13\\ext\\mysql2\/cl\n  ient.c:1067: undefined reference to mysql_options@12'\n  c:\\RailsInstaller\\Ruby1.9.3\\lib\\ruby\\gems\\1.9.1\\gems\\mysql2-0.3.13\\ext\\mysql2\/cl\n  ient.c:1069: undefined reference tomysql_error@4' client.o: In\n  function init_mysql2_client':\n  c:\\RailsInstaller\\Ruby1.9.3\\lib\\ruby\\gems\\1.9.1\\gems\\mysql2-0.3.13\\ext\\mysql2\/cl\n  ient.c:1105: undefined reference tomysql_get_client_info@0'\n  result.o: In function rb_mysql_result_count':\n  c:\\RailsInstaller\\Ruby1.9.3\\lib\\ruby\\gems\\1.9.1\\gems\\mysql2-0.3.13\\ext\\mysql2\/re\n  sult.c:576: undefined reference tomysql_num_rows@4' result.o: In\n  function rb_mysql_result_fetch_field':\n  c:\\RailsInstaller\\Ruby1.9.3\\lib\\ruby\\gems\\1.9.1\\gems\\mysql2-0.3.13\\ext\\mysql2\/re\n  sult.c:126: undefined reference tomysql_fetch_field_direct@8'\n  c:\\RailsInstaller\\Ruby1.9.3\\lib\\ruby\\gems\\1.9.1\\gems\\mysql2-0.3.13\\ext\\mysql2\/re\n  sult.c:114: undefined reference to mysql_num_fields@4' result.o: In\n  functionrb_mysql_result_fetch_fields':\n  c:\\RailsInstaller\\Ruby1.9.3\\lib\\ruby\\gems\\1.9.1\\gems\\mysql2-0.3.13\\ext\\mysql2\/re\n  sult.c:403: undefined reference to mysql_num_fields@4' result.o: In\n  functionnogvl_fetch_row':\n  c:\\RailsInstaller\\Ruby1.9.3\\lib\\ruby\\gems\\1.9.1\\gems\\mysql2-0.3.13\\ext\\mysql2\/re\n  sult.c:105: undefined reference to mysql_fetch_row@4' result.o: In\n  functionrb_mysql_result_free_result':\n  c:\\RailsInstaller\\Ruby1.9.3\\lib\\ruby\\gems\\1.9.1\\gems\\mysql2-0.3.13\\ext\\mysql2\/re\n  sult.c:75: undefined reference to mysql_free_result@4' result.o: In\n  functionrb_mysql_result_fetch_row':\n  c:\\RailsInstaller\\Ruby1.9.3\\lib\\ruby\\gems\\1.9.1\\gems\\mysql2-0.3.13\\ext\\mysql2\/re\n  sult.c:213: undefined reference to mysql_fetch_lengths@4'\n  c:\\RailsInstaller\\Ruby1.9.3\\lib\\ruby\\gems\\1.9.1\\gems\\mysql2-0.3.13\\ext\\mysql2\/re\n  sult.c:215: undefined reference tomysql_num_fields@4' result.o: In\n  function rb_mysql_result_each':\n  c:\\RailsInstaller\\Ruby1.9.3\\lib\\ruby\\gems\\1.9.1\\gems\\mysql2-0.3.13\\ext\\mysql2\/re\n  sult.c:503: undefined reference tomysql_fetch_fields@4' result.o: In\n  function rb_mysql_result_free_result':\n  c:\\RailsInstaller\\Ruby1.9.3\\lib\\ruby\\gems\\1.9.1\\gems\\mysql2-0.3.13\\ext\\mysql2\/re\n  sult.c:75: undefined reference tomysql_free_result@4' result.o: In\n  function rb_mysql_result_each':\n  c:\\RailsInstaller\\Ruby1.9.3\\lib\\ruby\\gems\\1.9.1\\gems\\mysql2-0.3.13\\ext\\mysql2\/re\n  sult.c:531: undefined reference tomysql_fetch_fields@4' result.o: In\n  function rb_mysql_result_free_result':\n  c:\\RailsInstaller\\Ruby1.9.3\\lib\\ruby\\gems\\1.9.1\\gems\\mysql2-0.3.13\\ext\\mysql2\/re\n  sult.c:75: undefined reference tomysql_free_result@4' result.o: In\n  function rb_mysql_result_each':\n  c:\\RailsInstaller\\Ruby1.9.3\\lib\\ruby\\gems\\1.9.1\\gems\\mysql2-0.3.13\\ext\\mysql2\/re\n  sult.c:490: undefined reference tomysql_num_rows@4' result.o: In\n  function rb_mysql_result_free_result':\n  c:\\RailsInstaller\\Ruby1.9.3\\lib\\ruby\\gems\\1.9.1\\gems\\mysql2-0.3.13\\ext\\mysql2\/re\n  sult.c:75: undefined reference tomysql_free_result@4' collect2: ld\n  returned 1 exit status make: *** [mysql2.so] Error 1\n    Gem files will remain installed in c:\/RailsInstaller\/Ruby1.9.3\/lib\/ruby\/gems\/1.9 .1\/gems\/mysql2-0.3.13\n\nfor inspection. Results logged to\n  c:\/RailsInstaller\/Ruby1.9.3\/lib\/ruby\/gems\/1.9.1\/gems\/mysql2-0.\n          3.13\/ext\/mysql2\/gem_make.out\nCan anyone please help?  I am having much trouble comprehending what's wrong with the installation. I had this same error on Win64 environment.\nI tested a lot of solutions but the only that worked for me was:\n\ngem install mysql2 -v '0.3.11'\n\n","meta":{"link":"https:\/\/stackoverflow.com\/questions\/17866003\/rails-installing-mysql-error-installing-mysql2-error-failed-to-build-gem-nat"},"label":[[25,93,"problem"],[12779,12809,"resolution"]],"Comments":[]}
{"id":600,"text":"deployment problem - files point to dev version. I just published a website in vs2008 and FTP it to the live site and there is an error where some of the files are pointing to the dev server??\nException Details: System.NullReferenceException: Object reference not set to an instance of an object.\n\nSource Error:\nAn unhandled exception was generated during the execution of the current web request. Information regarding the origin and location of the exception can be identified using the exception stack trace below.\nStack Trace:\n[NullReferenceException: Object reference not set to an instance of an object.]\n   Support.Models.ConsumerDataContext..ctor() in C:_work\\Models\\Consumer.designer.cs:41\n   Support.Models.Consumers..ctor() in C:_work\\Models\\Consumers.cs:17\n   Support.Controllers.HomeController..ctor() in C:_work\\Controllers\\HomeController.cs:22\n[TargetInvocationException: Exception has been thrown by the target of an invocation.]\n   System.RuntimeTypeHandle.CreateInstance(RuntimeType type, Boolean publicOnly, Boolean noCheck, Boolean& canBeCached, RuntimeMethodHandle& ctor, Boolean& bNeedSecurityCheck) +0\n   System.RuntimeType.CreateInstanceSlow(Boolean publicOnly, Boolean fillCache) +146\n   System.RuntimeType.CreateInstanceImpl(Boolean publicOnly, Boolean skipVisibilityChecks, Boolean fillCache) +298\n   System.Activator.CreateInstance(Type type, Boolean nonPublic) +79\n   System.Web.Mvc.DefaultControllerFactory.GetControllerInstance(RequestContext requestContext, Type controllerType) +121\n[InvalidOperationException: An error occurred when trying to create a controller of type 'Support.Controllers.HomeController'. Make sure that the controller has a parameterless public constructor.]\n   System.Web.Mvc.DefaultControllerFactory.GetControllerInstance(RequestContext requestContext, Type controllerType) +839\n   System.Web.Mvc.DefaultControllerFactory.CreateController(RequestContext requestContext, String controllerName) +66\n   System.Web.Mvc.MvcHandler.ProcessRequestInit(HttpContextBase httpContext, IController& controller, IControllerFactory& factory) +194\n   System.Web.Mvc.MvcHandler.BeginProcessRequest(HttpContextBase httpContext, AsyncCallback callback, Object state) +86\n   System.Web.CallHandlerExecutionStep.System.Web.HttpApplication.IExecutionStep.Execute() +392\n   System.Web.HttpApplication.ExecuteStep(IExecutionStep step, Boolean& completedSynchronously) +263\nany idea why?\nbtw \"Consumer\" is a dbml file\nthanks The file paths in the stack trace are coming from the pdb files and are based on the file paths at compilation time. They do not necessarilty mean that the running assemblies are looking there for code. \nConsider these paths as aids for debugging on your development machine.\nI suspect that a null reference in your model designer is actually due to a problem with differing schemas on the live and dev databases. Or from missing data that the code assumes will always be there.\n","meta":{"link":"https:\/\/stackoverflow.com\/questions\/3658070\/deployment-problem-files-point-to-dev-version"},"label":[[212,296,"problem"],[860,944,"problem"],[1517,1711,"problem"],[2751,2935,"problem"]],"Comments":[]}
{"id":601,"text":"Retrieve \"Integrated in build \" link type from workitems in VSTS. We are in process of retrieving specific fields from work items in VSTS following the instructions from Fetch work items with queries programatically in VSTS \nBut unable to fetch \"Integrated in build\" Development link type using \"Microsoft.VSTS.Build.IntegrationBuild\" reference field, all other fields are retrievable.\nHow can we retrieve the Development links associated to a work item? You can get the Development links via the REST API to get work item relations:\nGET https:\/\/account.visualstudio.com\/DefaultCollection\/_apis\/wit\/workitems\/{workitem ID}?$expand=relations&api-version=1.0\n\nThen you can get the Development links in the ArtifactLinks through the output.\nAnd the Microsoft.VSTS.Build.IntegrationBuild field can not be used to get the build links for VSTS, since the Microsoft.VSTS.Build.IntegrationBuild field is designed for the XAML build (not vNext build). \nAnd you can also find relation information in the blog Automatic linking work items to builds:\n\nThe build fields are populated with build numbers only for XML builds.\n  This will continue to work as is for XML builds. The new build system\n  does not populate these fields.\n\n","meta":{"link":"https:\/\/stackoverflow.com\/questions\/47888050\/retrieve-integrated-in-build-link-type-from-workitems-in-vsts"},"label":[[229,350,"problem"],[463,656,"resolution"],[658,737,"resolution"],[746,942,"problem"]],"Comments":[]}
{"id":602,"text":"The SonarQube code analysis task in our Azure Devops build pipeline yaml getting failed with storage issue in default location. Issue:- The SonarQube code analysis task in our Azure Devops build pipeline yaml getting failed with below error.\nWhat we tried: We have SonarQube analysis task yaml in our AzureDevops pipeline and there is a maven task which using \"clean verify\" goal followed by the sonarqube task. when we execute this the maven task getyting failed with the below error.\n\nFailed to execute goal\norg.sonarsource.scanner.maven:sonar-maven-plugin:3.8.0.2131:sonar on\nproject: Unable to load component interface\norg.sonar.api.utils.TempFolder: Failed to create temporary folder in\n\/home\/vowne\/.sonar: \/home\/vowne\/.sonar\/.sonartmp_xxxxxxx: No space\nleft on device.\n\nNot sure from where the sonarqube is taking the location from . its taking the users home directory now. \/home\/vowne. We have enough space in other location and would need to change the sonarqube temp location to there.\nIs there something we can add to the maven task or sonar task to change the location of this temp ?\n- task: SonarQubePrepare@4\n    inputs:\n      SonarQube: 'Sonarqube'\n      ScannerMode: 'Other'\n      extraProperties: |\n          sonar.projectName=${{ parameters.myName }}\n          sonar.coverage.xxxxxx.xmlReportPaths=$(System.DefaultWorkingDirectory)\/xxxxxxx\/xxxxxx\/site\/xxxxx\/xxxxx.xml\n\nHere can we add an extra property to the sonar task like sonar.userHome=$(System.DefaultWorkingDirectory)\nor maven task need to change \"\"goal\"  from \"clean verify\" to some customised values. According to source code this path is derived from the following properties:\n\nsonar.globalWorking.directory system property (resolved relative to settings below if not absolute),\nsonar.userHome system property,\nSONAR_USER_HOME environment variable,\n.sonar directory under user.home system property.\n\nIt seems that the last case matches your scenario. Try to set one of the above settings to select different location instead of ~\/.sonar.\n","meta":{"link":"https:\/\/stackoverflow.com\/questions\/67972841\/the-sonarqube-code-analysis-task-in-our-azure-devops-build-pipeline-yaml-getting"},"label":[[0,128,"problem"],[655,688,"problem"],[749,774,"problem"],[1936,2015,"resolution"]],"Comments":[]}
{"id":606,"text":"Setting up scikit-learn - numpy errors on import. I'm trying to set up scikit-learn, but after installing numpy and scikit-learn (using pip) that's what I get when trying to imort sklearn:\nimport sklearn\nRuntimeError: module compiled against API version 8 but this version of numpy is 7\nTraceback (most recent call last):\nFile \"<stdin>\", line 1, in <module>\nFile \"\/usr\/local\/lib\/python2.7\/site-packages\/sklearn\/__init__.py\", line 32, in <module>\n  from .base import clone\nFile \"\/usr\/local\/lib\/python2.7\/site-packages\/sklearn\/base.py\", line 10, in <module>\n  from scipy import sparse\nFile \"\/Library\/Python\/2.7\/site-packages\/scipy-0.13.0.dev_c31f167_20130304-py2.7-macosx-10.8-intel.egg\/scipy\/sparse\/__init__.py\", line 182, in <module>\nfrom .csr import *\nFile \"\/Library\/Python\/2.7\/site-packages\/scipy-0.13.0.dev_c31f167_20130304-py2.7-macosx-10.8-intel.egg\/scipy\/sparse\/csr.py\", line 15, in <module>\nfrom .sparsetools import csr_tocsc, csr_tobsr, csr_count_blocks, \\\nFile \"\/Library\/Python\/2.7\/site-packages\/scipy-0.13.0.dev_c31f167_20130304-py2.7-macosx-10.8-intel.egg\/scipy\/sparse\/sparsetools\/__init__.py\", line 5, in <module>\nfrom .csr import *\nFile \"\/Library\/Python\/2.7\/site-packages\/scipy-0.13.0.dev_c31f167_20130304-py2.7-macosx-10.8-intel.egg\/scipy\/sparse\/sparsetools\/csr.py\", line 26, in <module>\n_csr = swig_import_helper()\nFile \"\/Library\/Python\/2.7\/site-packages\/scipy-0.13.0.dev_c31f167_20130304-py2.7-macosx-10.8-intel.egg\/scipy\/sparse\/sparsetools\/csr.py\", line 22, in swig_import_helper\n_mod = imp.load_module('_csr', fp, pathname, description)\nImportError: numpy.core.multiarray failed to import\n\nTried to reinstall things without success Looking at this issue on github, it sounds like it might require a specific version of numpy - they used 1.7.0b2.\n","meta":{"link":"https:\/\/stackoverflow.com\/questions\/15282046\/setting-up-scikit-learn-numpy-errors-on-import"},"label":[[204,286,"problem"],[1555,1606,"problem"],[1707,1742,"resolution"]],"Comments":[]}
{"id":607,"text":"Install Tensorflow 2.x only for CPU using PIP. how do you install only a CPU version of Tensorflow 2.x using pip ?\nIn the past, it was possible to install this 2 different versions.\nSince I am running the scripts in a nonen GPU device ( without envidia card, intel card available without cuda support), I am getting following error:\n2020-04-14 23:28:14.632879: W tensorflow\/stream_executor\/platform\/default\/dso_loader.cc:55] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n2020-04-14 23:28:14.632902: E tensorflow\/stream_executor\/cuda\/cuda_driver.cc:313] failed call to cuInit: UNKNOWN ERROR (303)\nIn the past my workaround was to use a CPU only version.\nThanks for the hints in advance Issue solved after  installing a CPU only version.\nI used pin tensorflow-cpu and the version of the release. Somehow the fallback solution for CPU did not work in my setup.\n","meta":{"link":"https:\/\/stackoverflow.com\/questions\/61191925\/install-tensorflow-2-x-only-for-cpu-using-pip"},"label":[[425,552,"problem"],[635,677,"problem"],[786,816,"resolution"]],"Comments":[]}
{"id":608,"text":"Mistakenly worked in someone else's branch from my team in Azure Devops. Confused regarding pushing my changes to the server. I worked in someone else's branch instead of mine and when I try to push it to the server, I am unable to do so. When I try to commit and push I get the following error. Please help. According to the error message: Update your branch by pulling before push, we need update your local branch via git pull and then push the change to Azure DevOps Repo. Also, we could do this via the button Pull and Push\nUpdate1\nThanks for Philippe‘s sharing.\nWe should know that a pull is doing a merge by default and that most of the times, when you work in your own branch, you prefer to sync with a rebase. So this popup won't help.\n","meta":{"link":"https:\/\/stackoverflow.com\/questions\/65283009\/mistakenly-worked-in-someone-elses-branch-from-my-team-in-azure-devops-confuse"},"label":[[239,294,"problem"],[421,476,"resolution"],[492,528,"resolution"]],"Comments":[]}
{"id":609,"text":"Python 3.5 64bit on Windows 8.1 64bit, The only way to install TensorFlow binaries on Windows does not work. Is it possible to install Python 3.5.x on Windows 8.1?\nThe primary goal is to install TensorFlow directly on my Windows.\nIt is not working. More specifically when using a 64 bit windows, and naturally trying the 64bits Python. The AMD name in the installation file is confusing, since my laptop is an Intel. But that is the only 64 bit option so I select that.\nI try to set up via binaries. In the middle of installation a GUI tells me installation failed:\n\nBTW, I have done my best to remove any remaining Python 32 bits from my system. After running windows update, I finally managed to install python 3.5 64 bits and then tensorflow on my windows 8.1. \n","meta":{"link":"https:\/\/stackoverflow.com\/questions\/43333699\/python-3-5-64bit-on-windows-8-1-64bit-the-only-way-to-install-tensorflow-binari"},"label":[[500,565,"problem"],[647,744,"resolution"]],"Comments":[]}
